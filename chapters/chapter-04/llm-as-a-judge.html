<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM-as-a-Judge | Chapter 4 | DSPy: The Comprehensive Guide</title>
    <meta name="description"
        content="Learn to use LLM-as-a-Judge for evaluating nuanced, context-sensitive tasks where traditional metrics fail.">
    <link rel="stylesheet" href="../../assets/css/style.css?v=2">
    <link rel="stylesheet" href="../../assets/css/chapter.css?v=8">
    <link rel="stylesheet" href="../../assets/css/content.css?v=2">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>

<body class="chapter-page">
    <!-- Skip Link for Accessibility -->
    <a href="#main-content" class="skip-link">Skip to main content</a>
    <header>
        <div class="logo">
            <a href="../../index.html">
                <img src="../../src/assets/logos/logo.png" alt="DSPy Ebook Logo">
                <span>DSPy Ebook</span>
            </a>
        </div>
        <nav>
            <ul>
                <li><a href="../../index.html">Home</a></li>
                <li><a href="../../index.html#chapters">Chapters</a></li>
                <li><a href="https://github.com/dustinober1/Ebook_DSPy" target="_blank">GitHub</a></li>
            </ul>
        </nav>
    </header>

    <div class="chapter-layout">
        <aside class="sidebar" id="sidebar">
            <div class="sidebar-header">
                <h2>Chapter 4</h2>
                <span class="sidebar-subtitle">Evaluation</span>
            </div>
            <nav class="sidebar-nav">
                <ul class="section-list" id="section-list">
                    <li class="section-item viewed" data-section="intro">
                        <a href="index.html">
                            <span class="section-number">1</span>
                            <span class="section-title">Chapter Overview</span>
                        </a>
                    </li>
                    <li class="section-item viewed" data-section="why-eval">
                        <a href="why-evaluation-matters.html">
                            <span class="section-number">2</span>
                            <span class="section-title">Why Evaluation Matters</span>
                        </a>
                    </li>
                    <li class="section-item viewed" data-section="datasets">
                        <a href="creating-datasets.html">
                            <span class="section-number">3</span>
                            <span class="section-title">Creating Datasets</span>
                        </a>
                    </li>
                    <li class="section-item viewed" data-section="metrics">
                        <a href="defining-metrics.html">
                            <span class="section-number">4</span>
                            <span class="section-title">Defining Metrics</span>
                        </a>
                    </li>
                    <li class="section-item viewed" data-section="loops">
                        <a href="evaluation-loops.html">
                            <span class="section-number">5</span>
                            <span class="section-title">Evaluation Loops</span>
                        </a>
                    </li>
                    <li class="section-item viewed" data-section="best-practices">
                        <a href="best-practices.html">
                            <span class="section-number">6</span>
                            <span class="section-title">Best Practices</span>
                        </a>
                    </li>
                    <li class="section-item viewed" data-section="exercises">
                        <a href="exercises.html">
                            <span class="section-number">7</span>
                            <span class="section-title">Exercises</span>
                        </a>
                    </li>
                    <li class="section-item viewed" data-section="structured-prompting">
                        <a href="structured-prompting.html">
                            <span class="section-number">8</span>
                            <span class="section-title">Structured Prompting</span>
                        </a>
                    </li>
                    <li class="section-item active" data-section="llm-as-judge">
                        <a href="llm-as-a-judge.html">
                            <span class="section-number">9</span>
                            <span class="section-title">LLM-as-a-Judge</span>
                        </a>
                    </li>
                    <li class="section-item" data-section="human-aligned">
                        <a href="human-aligned-evaluation.html">
                            <span class="section-number">10</span>
                            <span class="section-title">Human-Aligned Eval</span>
                        </a>
                    </li>
                    <li class="section-item" data-section="solutions">
                        <a href="solutions.html">
                            <span class="section-number">11</span>
                            <span class="section-title">Solutions</span>
                        </a>
                    </li>
                </ul>
            </nav>
            <div class="sidebar-footer">
                <a href="../chapter-05/index.html" class="next-chapter-btn">
                    Next: Chapter 05 →
                </a>
            </div>
        </aside>

        <button class="sidebar-toggle" id="sidebar-toggle" aria-label="Toggle Sidebar">
            <span></span>
            <span></span>
            <span></span>
        </button>

        <main class="chapter-content" id="main-content">
            <div class="reading-progress" id="reading-progress">
                <div class="progress-bar"></div>
            </div>

            <section class="chapter-hero">
                <div class="chapter-hero-content is-visible fade-in-up">
                    <span class="chapter-label">Chapter 4</span>
                    <h1 class="chapter-title">LLM-as-a-Judge</h1>
                    <p class="chapter-description">Using language models to evaluate nuanced, context-sensitive outputs.
                    </p>
                </div>
            </section>

            <div class="content-wrapper">
                <article class="content-article" id="content-article">
                    <section class="content-section is-visible" id="llm-as-judge" data-section="llm-as-judge">
                        <div class="markdown-content" id="llm-as-judge-content">
                            <!-- Overview -->
                            <div class="content-block">
                                <h2>Overview</h2>
                                <p><strong>LLM-as-a-Judge</strong> is a powerful evaluation paradigm that uses large
                                    language models to assess the quality and impact of model outputs. This approach is
                                    particularly valuable when traditional metrics fail to capture domain-specific
                                    nuances or real-world consequences.</p>
                                <p>This framework becomes essential in safety-critical domains like healthcare, where
                                    standard metrics such as Word Error Rate (WER) correlate poorly with actual clinical
                                    risk.</p>
                            </div>

                            <!-- When to use -->
                            <div class="content-block">
                                <h2>When to Use LLM-as-a-Judge</h2>
                                <h3>1. Domain-Specific Impact Assessment</h3>
                                <div class="code-block">
                                    <pre><code class="language-python"># Standard metrics (WER, BLEU) fail to capture clinical meaning
standard_metrics = {
    "wer": 0.12,  # Low error rate
    "bleu": 0.85,  # High overlap
    # But missed critical negation: "no chest pain" → "chest pain"
}

# LLM-as-a-Judge captures actual impact
clinical_judge = ClinicalImpactJudge()
assessment = clinical_judge.evaluate(
    ground_truth="Patient reports no chest pain",
    hypothesis="Patient reports chest pain"
)
# Result: SIGNIFICANT_CLINICAL_IMPACT (2/2)</code></pre>
                                </div>

                                <h3>2. Nuanced Semantic Evaluation</h3>
                                <p>Traditional metrics struggle with context-dependent meaning, domain terminology, and
                                    complex concept relationships. LLM judges can reason through these nuances.</p>
                            </div>

                            <!-- Implementation -->
                            <div class="content-block">
                                <h2>Implementation Framework</h2>
                                <h3>Core Judge Architecture</h3>
                                <div class="code-block">
                                    <pre><code class="language-python">import dspy
from typing import Dict, List, Tuple, Optional

class LLMJudge(dspy.Module):
    """Base class for LLM-as-a-Judge implementations."""

    def __init__(self,
                 prompt_template: str,
                 output_schema: type,
                 max_tokens: int = 1000):
        super().__init__()
        self.prompt_template = prompt_template
        self.output_schema = output_schema
        # Initialize the judge with Chain of Thought for reasoning
        self.judge = dspy.ChainOfThought(
            self.prompt_template,
            max_tokens=max_tokens
        )

    def evaluate(self, ground_truth: str, hypothesis: str, **context) -> Dict:
        """Evaluate hypothesis against ground truth."""
        # Get LLM evaluation provided prompt and context
        result = self.judge(
            ground_truth=ground_truth,
            hypothesis=hypothesis,
            **context
        )
        # Parse output... (omitted for brevity, see source)
        return self.parse_output(result)

    def parse_output(self, raw_output) -> Dict:
        # Implementation details...
        pass</code></pre>
                                </div>

                                <h3>Clinical Impact Judge Example</h3>
                                <div class="code-block">
                                    <pre><code class="language-python">class ClinicalImpactJudge(LLMJudge):
    """Judge for assessing clinical impact of ASR errors."""
    
    def __init__(self):
        prompt_template = """
        You are an expert medical analyst. Assess the clinical impact of errors.
        
        Core Principle: Would a clinician make different decisions?
        
        Impact Levels:
        - 0: No Clinical Impact
        - 1: Minimal Clinical Impact
        - 2: Significant Clinical Impact
        
        Ground Truth: {ground_truth}
        Transcription: {hypothesis}
        Context: {context}
        """
        super().__init__(prompt_template, dict)</code></pre>
                                </div>
                            </div>

                            <!-- Integration -->
                            <div class="content-block">
                                <h2>Integration with DSPy</h2>
                                <p>You can wrap your LLM Judge as a DSPy Metric to use it within evaluation loops and
                                    optimization.</p>
                                <div class="code-block">
                                    <pre><code class="language-python">class LLMJudgeMetric(dspy.Metric):
    def __init__(self, judge: LLMJudge):
        self.judge = judge

    def __call__(self, example, prediction, trace=None):
        ground_truth = example.outputs()
        hypothesis = prediction.get('output', str(prediction))
        
        result = self.judge.evaluate(
            ground_truth=ground_truth,
            hypothesis=hypothesis
        )
        
        # Return numeric score for optimization
        if 'evaluation' in result:
            return result['evaluation'] / 2.0  # Normalize 0-2 scale to 0-1
        return 0.0</code></pre>
                                </div>
                            </div>

                            <!-- Continue -->
                            <div class="content-block">
                                <div class="continue-btn-container">
                                    <a href="human-aligned-evaluation.html" class="continue-btn">
                                        <span>Continue to Human-Aligned Evaluation</span>
                                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24"
                                            viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
                                            stroke-linecap="round" stroke-linejoin="round">
                                            <path d="M5 12h14M12 5l7 7-7 7" />
                                        </svg>
                                    </a>
                                </div>
                            </div>

                        </div>
                    </section>
                </article>
            </div>
        </main>
    </div>

    <footer>
        <p>&copy; 2025 DSPy Ebook. All rights reserved.</p>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="../../assets/js/main.js?v=1"></script>
    <script src="../../assets/js/chapter_v2.js" defer></script>
    <script>
        document.addEventListener('DOMContentLoaded', () => {
            if (window.initChapter) {
                window.initChapter({});
            }
            setTimeout(() => {
                if (window.Prism) window.Prism.highlightAll();
            }, 100);
        });
    </script>
</body>

</html>