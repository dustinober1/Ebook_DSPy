{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac7f529",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Assertions Module\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Previous Section**: [Composing Modules](./06-composing-modules.md) - Understanding of module composition\n",
    "- **Chapter 2**: Signatures - Strong familiarity with signature design\n",
    "- **Required Knowledge**: Constraint validation, error handling patterns\n",
    "- **Difficulty Level**: Advanced\n",
    "- **Estimated Reading Time**: 60 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this section, you will:\n",
    "- Master the `dspy.Assert` and `dspy.Suggest` constraint system\n",
    "- Learn to implement runtime validation for AI outputs\n",
    "- Build self-refining pipelines with automatic error recovery\n",
    "- Understand the computational constraints framework\n",
    "- Design robust AI applications with guaranteed output quality\n",
    "\n",
    "## Introduction to Assertions\n",
    "\n",
    "Assertions in DSPy provide a powerful mechanism for ensuring the quality and correctness of AI-generated outputs. They act as runtime validators that check if the model's output meets specified constraints, and can automatically trigger refinement when constraints are violated.\n",
    "\n",
    "### Why Assertions Matter\n",
    "\n",
    "**Without Assertions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2327246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brittle - no validation\n",
    "qa = dspy.Predict(\"question -> answer\")\n",
    "result = qa(question=\"What is 2+2?\")\n",
    "# Model might return \"4\", \"Four\", \"The answer is 4\", or even hallucinate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c241ad",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**With Assertions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe44260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust - guaranteed format and correctness\n",
    "qa = dspy.Predict(\"question -> answer\")\n",
    "\n",
    "def validate_numeric_answer(example, pred, trace=None):\n",
    "    # Check if answer is a number\n",
    "    assert pred.answer.isdigit(), \"Answer must be numeric\"\n",
    "    # Check if it's actually correct\n",
    "    assert int(pred.answer) == 4, \"Answer must be correct\"\n",
    "    return True\n",
    "\n",
    "# Configure assertion\n",
    "qa = dspy.Assert(\n",
    "    qa,\n",
    "    validation_fn=validate_numeric_answer,\n",
    "    max_attempts=3\n",
    ")\n",
    "\n",
    "result = qa(question=\"What is 2+2?\")\n",
    "# Guaranteed: result.answer is \"4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b6ebf",
   "metadata": {},
   "source": [
    "## Core Assertion Types\n",
    "\n",
    "### 1. dspy.Assert - Hard Constraints\n",
    "\n",
    "`dspy.Assert` enforces strict constraints that must be satisfied. If a constraint fails, the system automatically retries with refined instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57578bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "class CodeGenerator(dspy.Signature):\n",
    "    \"\"\"Generate Python code for the given task.\"\"\"\n",
    "    task = dspy.InputField(desc=\"Programming task to implement\", type=str)\n",
    "    code = dspy.OutputField(desc=\"Valid Python code\", type=str)\n",
    "\n",
    "# Create the module\n",
    "coder = dspy.ChainOfThought(CodeGenerator)\n",
    "\n",
    "# Define assertion function\n",
    "def validate_syntax(example, pred, trace=None):\n",
    "    \"\"\"Ensure generated code has valid Python syntax.\"\"\"\n",
    "    try:\n",
    "        compile(pred.code, '<string>', 'exec')\n",
    "        return True\n",
    "    except SyntaxError as e:\n",
    "        # Provide helpful error message\n",
    "        raise AssertionError(f\"Syntax error in generated code: {e}\")\n",
    "\n",
    "# Wrap with assertion\n",
    "safe_coder = dspy.Assert(\n",
    "    coder,\n",
    "    validation_fn=validate_syntax,\n",
    "    max_attempts=3,\n",
    "    backtrack=True  # Try different approach on failure\n",
    ")\n",
    "\n",
    "# Use it\n",
    "result = safe_coder(task=\"Create a function to calculate factorial\")\n",
    "print(result.code)  # Guaranteed to be syntactically valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a493438",
   "metadata": {},
   "source": [
    "### 2. dspy.Suggest - Soft Constraints\n",
    "\n",
    "`dspy.Suggest` provides gentle guidance for improving outputs without strict enforcement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EssayWriter(dspy.Signature):\n",
    "    \"\"\"Write an essay on the given topic.\"\"\"\n",
    "    topic = dspy.InputField(desc=\"Essay topic\", type=str)\n",
    "    essay = dspy.OutputField(desc=\"Well-written essay\", type=str)\n",
    "\n",
    "writer = dspy.Predict(EssayWriter)\n",
    "\n",
    "def suggest_improvements(example, pred, trace=None):\n",
    "    \"\"\"Suggest improvements for better essays.\"\"\"\n",
    "    suggestions = []\n",
    "\n",
    "    if len(pred.essay.split()) < 200:\n",
    "        suggestions.append(\"Essay should be at least 200 words\")\n",
    "\n",
    "    if not any(punc in pred.essay for punc in '.!?'):\n",
    "        suggestions.append(\"Include proper punctuation\")\n",
    "\n",
    "    if len([s for s in pred.essay.split() if s[0].isupper()]) < 3:\n",
    "        suggestions.append(\"Start sentences with capital letters\")\n",
    "\n",
    "    if suggestions:\n",
    "        return False, f\"Please improve: {'; '.join(suggestions)}\"\n",
    "    return True, None\n",
    "\n",
    "# Wrap with suggestions\n",
    "improved_writer = dspy.Suggest(\n",
    "    writer,\n",
    "    validation_fn=suggest_improvements,\n",
    "    max_attempts=2,\n",
    "    recovery_hint=\"Focus on clarity, grammar, and completeness\"\n",
    ")\n",
    "\n",
    "result = improved_writer(topic=\"The importance of sleep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af1d4b2",
   "metadata": {},
   "source": [
    "### 3. Multiple Assertions\n",
    "\n",
    "Chain multiple assertions for comprehensive validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(dspy.Signature):\n",
    "    \"\"\"Process and analyze data.\"\"\"\n",
    "    raw_data = dspy.InputField(desc=\"Raw input data\", type=str)\n",
    "    processed_data = dspy.OutputField(desc=\"Processed output\", type=str)\n",
    "    insights = dspy.OutputField(desc=\"Key insights from data\", type=str)\n",
    "\n",
    "processor = dspy.Predict(DataProcessor)\n",
    "\n",
    "# Assertion 1: JSON format\n",
    "def validate_json_format(example, pred, trace=None):\n",
    "    import json\n",
    "    try:\n",
    "        json.loads(pred.processed_data)\n",
    "        return True\n",
    "    except:\n",
    "        raise AssertionError(\"Processed data must be valid JSON\")\n",
    "\n",
    "# Assertion 2: Required fields\n",
    "def validate_required_fields(example, pred, trace=None):\n",
    "    import json\n",
    "    data = json.loads(pred.processed_data)\n",
    "    required = ['id', 'timestamp', 'value']\n",
    "    missing = [f for f in required if f not in data]\n",
    "    if missing:\n",
    "        raise AssertionError(f\"Missing required fields: {missing}\")\n",
    "    return True\n",
    "\n",
    "# Assertion 3: Insights quality\n",
    "def validate_insights(example, pred, trace=None):\n",
    "    if len(pred.insights) < 50:\n",
    "        raise AssertionError(\"Insights must be detailed (min 50 characters)\")\n",
    "    return True\n",
    "\n",
    "# Chain all assertions\n",
    "robust_processor = processor.with_assertions([\n",
    "    validate_json_format,\n",
    "    validate_required_fields,\n",
    "    validate_insights\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2dae72",
   "metadata": {},
   "source": [
    "## Constraint Types\n",
    "\n",
    "### 1. Format Constraints\n",
    "\n",
    "Ensure outputs follow specific structural requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17441eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIResponse(dspy.Signature):\n",
    "    \"\"\"Generate API responses.\"\"\"\n",
    "    request = dspy.InputField(desc=\"API request details\", type=str)\n",
    "    response = dspy.OutputField(desc=\"JSON API response\", type=str)\n",
    "\n",
    "def validate_api_response(example, pred, trace=None):\n",
    "    \"\"\"Ensure valid API response format.\"\"\"\n",
    "    import json\n",
    "    import re\n",
    "\n",
    "    try:\n",
    "        data = json.loads(pred.response)\n",
    "\n",
    "        # Check required structure\n",
    "        assert 'status' in data, \"Missing 'status' field\"\n",
    "        assert 'data' in data, \"Missing 'data' field\"\n",
    "\n",
    "        # Check status codes\n",
    "        assert data['status'] in [200, 201, 400, 404, 500], \\\n",
    "               f\"Invalid status code: {data['status']}\"\n",
    "\n",
    "        # Check data types\n",
    "        assert isinstance(data['status'], int), \"Status must be integer\"\n",
    "        assert isinstance(data['data'], (dict, list)), \"Data must be object or array\"\n",
    "\n",
    "        return True\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        raise AssertionError(\"Response must be valid JSON\")\n",
    "\n",
    "api_generator = dspy.Assert(\n",
    "    dspy.Predict(APIResponse),\n",
    "    validation_fn=validate_api_response,\n",
    "    max_attempts=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f421d",
   "metadata": {},
   "source": [
    "### 2. Semantic Constraints\n",
    "\n",
    "Validate the meaning and correctness of outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031c1c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathTutor(dspy.Signature):\n",
    "    \"\"\"Solve math problems with explanations.\"\"\"\n",
    "    problem = dspy.InputField(desc=\"Math problem to solve\", type=str)\n",
    "    solution = dspy.OutputField(desc=\"Step-by-step solution\", type=str)\n",
    "    answer = dspy.OutputField(desc=\"Final numerical answer\", type=str)\n",
    "\n",
    "def validate_math_solution(example, pred, trace=None):\n",
    "    \"\"\"Validate mathematical correctness.\"\"\"\n",
    "    import re\n",
    "    import math\n",
    "\n",
    "    # Extract numerical answer\n",
    "    numbers = re.findall(r'-?\\d+\\.?\\d*', pred.answer)\n",
    "    if not numbers:\n",
    "        raise AssertionError(\"Answer must contain a number\")\n",
    "\n",
    "    model_answer = float(numbers[-1])\n",
    "\n",
    "    # Verify with actual calculation\n",
    "    if \"square root\" in example.problem.lower():\n",
    "        num = re.search(r'square root of (\\d+)', example.problem.lower())\n",
    "        if num:\n",
    "            correct = math.sqrt(int(num.group(1)))\n",
    "            if abs(model_answer - correct) > 0.01:\n",
    "                raise AssertionError(\"Incorrect square root calculation\")\n",
    "\n",
    "    # Check if solution explains steps\n",
    "    if len(pred.solution.split('\\n')) < 2:\n",
    "        raise AssertionError(\"Solution must show multiple steps\")\n",
    "\n",
    "    return True\n",
    "\n",
    "math_tutor = dspy.Assert(\n",
    "    dspy.Predict(MathTutor),\n",
    "    validation_fn=validate_math_solution,\n",
    "    max_attempts=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f202f08f",
   "metadata": {},
   "source": [
    "### 3. Consistency Constraints\n",
    "\n",
    "Ensure consistency between multiple outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22113893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryGenerator(dspy.Signature):\n",
    "    \"\"\"Generate a coherent story.\"\"\"\n",
    "    prompt = dspy.InputField(desc=\"Story prompt\", type=str)\n",
    "    title = dspy.OutputField(desc=\"Story title\", type=str)\n",
    "    summary = dspy.OutputField(desc=\"Brief summary\", type=str)\n",
    "    content = dspy.OutputField(desc=\"Full story content\", type=str)\n",
    "\n",
    "def validate_story_consistency(example, pred, trace=None):\n",
    "    \"\"\"Ensure story elements are consistent.\"\"\"\n",
    "\n",
    "    # Title should reflect content\n",
    "    title_words = set(pred.title.lower().split())\n",
    "    content_words = set(pred.content.lower().split()[:50])  # First 50 words\n",
    "    overlap = len(title_words.intersection(content_words))\n",
    "\n",
    "    if overlap < 2:\n",
    "        raise AssertionError(\"Title doesn't match story content\")\n",
    "\n",
    "    # Summary should match content\n",
    "    if pred.summary not in pred.content:\n",
    "        # Allow for paraphrasing by checking key concepts\n",
    "        summary_concepts = pred.summary.lower().split()\n",
    "        content_lower = pred.content.lower()\n",
    "\n",
    "        for concept in summary_concepts:\n",
    "            if len(concept) > 4 and concept not in content_lower:\n",
    "                raise AssertionError(f\"Summary mentions '{concept}' not in story\")\n",
    "\n",
    "    # Check story length\n",
    "    if len(pred.content) < 500:\n",
    "        raise AssertionError(\"Story too short (minimum 500 characters)\")\n",
    "\n",
    "    return True\n",
    "\n",
    "story_generator = dspy.Assert(\n",
    "    dspy.ChainOfThought(StoryGenerator),\n",
    "    validation_fn=validate_story_consistency,\n",
    "    max_attempts=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cbdffe",
   "metadata": {},
   "source": [
    "## Advanced Assertion Patterns\n",
    "\n",
    "### 1. Self-Refining Pipelines\n",
    "\n",
    "Build pipelines that improve themselves based on assertion feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9996e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfImprovingWriter(dspy.Module):\n",
    "    \"\"\"A writer that improves its output based on quality metrics.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.writer = dspy.ChainOfThought(\"topic -> draft\")\n",
    "        self.critic = dspy.ChainOfThought(\"draft, criteria -> critique\")\n",
    "        self.improver = dspy.ChainOfThought(\"draft, critique -> improved_draft\")\n",
    "\n",
    "    def forward(self, topic):\n",
    "        # Initial draft\n",
    "        draft = self.writer(topic=topic)\n",
    "\n",
    "        # Quality criteria\n",
    "        criteria = \"\"\"\n",
    "        1. Clarity: Is the writing clear and easy to understand?\n",
    "        2. Completeness: Does it fully address the topic?\n",
    "        3. Engagement: Is it interesting to read?\n",
    "        4. Accuracy: Are all statements factual?\n",
    "        \"\"\"\n",
    "\n",
    "        # Critique the draft\n",
    "        critique = self.critic(draft=draft.draft, criteria=criteria)\n",
    "\n",
    "        # Improve based on critique\n",
    "        improved = self.improver(draft=draft.draft, critique=critique.critique)\n",
    "\n",
    "        # Assert quality\n",
    "        def validate_quality(example, pred, trace=None):\n",
    "            word_count = len(pred.improved_draft.split())\n",
    "            assert word_count > 100, \"Draft too short\"\n",
    "            assert len(pred.improved_draft.split('\\n')) > 3, \"Add more paragraphs\"\n",
    "            return True\n",
    "\n",
    "        # Apply assertion with self-refinement\n",
    "        result = dspy.Assert(\n",
    "            self,\n",
    "            validation_fn=validate_quality,\n",
    "            max_attempts=3\n",
    "        )\n",
    "\n",
    "        return dspy.Prediction(improved_draft=improved.improved_draft)\n",
    "\n",
    "# Use the self-improving writer\n",
    "writer = SelfImprovingWriter()\n",
    "result = writer(topic=\"The benefits of renewable energy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203dca84",
   "metadata": {},
   "source": [
    "### 2. Contextual Assertions\n",
    "\n",
    "Adapt validation based on input context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveValidator:\n",
    "    \"\"\"Validates outputs based on input context.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rules = {\n",
    "            'technical': self.validate_technical,\n",
    "            'creative': self.validate_creative,\n",
    "            'formal': self.validate_formal,\n",
    "            'casual': self.validate_casual\n",
    "        }\n",
    "\n",
    "    def get_style(self, text):\n",
    "        \"\"\"Determine writing style from input.\"\"\"\n",
    "        text = text.lower()\n",
    "        if any(word in text for word in ['code', 'algorithm', 'technical']):\n",
    "            return 'technical'\n",
    "        elif any(word in text for word in ['story', 'poem', 'creative']):\n",
    "            return 'creative'\n",
    "        elif any(word in text for word in ['report', 'formal', 'business']):\n",
    "            return 'formal'\n",
    "        else:\n",
    "            return 'casual'\n",
    "\n",
    "    def validate_technical(self, example, pred, trace=None):\n",
    "        \"\"\"Validate technical content.\"\"\"\n",
    "        assert '}' in pred.output or ';' in pred.output, \\\n",
    "               \"Technical content should include code examples\"\n",
    "        assert any(word in pred.output.lower()\n",
    "                  for word in ['implementation', 'example', 'function']), \\\n",
    "               \"Include practical implementation details\"\n",
    "        return True\n",
    "\n",
    "    def validate_creative(self, example, pred, trace=None):\n",
    "        \"\"\"Validate creative content.\"\"\"\n",
    "        assert len(pred.output) > 200, \"Creative content should be substantial\"\n",
    "        sentences = pred.output.split('.')\n",
    "        assert len(sentences) > 5, \"Include multiple sentences\"\n",
    "        return True\n",
    "\n",
    "    def validate_formal(self, example, pred, trace=None):\n",
    "        \"\"\"Validate formal content.\"\"\"\n",
    "        assert not any(word in pred.output.lower()\n",
    "                      for word in ['hey', 'guys', 'awesome']), \\\n",
    "               \"Avoid informal language in formal writing\"\n",
    "        return True\n",
    "\n",
    "    def validate_casual(self, example, pred, trace=None):\n",
    "        \"\"\"Validate casual content.\"\"\"\n",
    "        return True  # No strict requirements\n",
    "\n",
    "    def validate(self, example, pred, trace=None):\n",
    "        \"\"\"Route to appropriate validator based on context.\"\"\"\n",
    "        style = self.get_style(example.input)\n",
    "        validator = self.rules.get(style, self.validate_casual)\n",
    "        return validator(example, pred, trace)\n",
    "\n",
    "# Use adaptive validation\n",
    "validator = AdaptiveValidator()\n",
    "\n",
    "adaptive_writer = dspy.Assert(\n",
    "    dspy.Predict(\"input -> output\"),\n",
    "    validation_fn=validator.validate,\n",
    "    max_attempts=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37439dc",
   "metadata": {},
   "source": [
    "### 3. Multi-Output Assertions\n",
    "\n",
    "Validate relationships between multiple output fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8778fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieReview(dspy.Signature):\n",
    "    \"\"\"Generate a comprehensive movie review.\"\"\"\n",
    "    movie = dspy.InputField(desc=\"Movie title\", type=str)\n",
    "    rating = dspy.OutputField(desc=\"Rating 1-10\", type=int)\n",
    "    summary = dspy.OutputField(desc=\"Brief summary\", type=str)\n",
    "    detailed_review = dspy.OutputField(desc=\"Full review\", type=str)\n",
    "\n",
    "def validate_review_consistency(example, pred, trace=None):\n",
    "    \"\"\"Ensure all parts of the review are consistent.\"\"\"\n",
    "\n",
    "    # Rating must be in valid range\n",
    "    assert 1 <= pred.rating <= 10, f\"Rating {pred.rating} out of range\"\n",
    "\n",
    "    # High ratings should have positive content\n",
    "    if pred.rating >= 7:\n",
    "        positive_words = ['excellent', 'amazing', 'brilliant', 'outstanding']\n",
    "        assert any(word in pred.detailed_review.lower()\n",
    "                  for word in positive_words), \\\n",
    "               \"High rating should include positive language\"\n",
    "\n",
    "    # Low ratings should include criticism\n",
    "    if pred.rating <= 4:\n",
    "        negative_words = ['disappointing', 'flawed', 'lacking', 'weak']\n",
    "        assert any(word in pred.detailed_review.lower()\n",
    "                  for word in negative_words), \\\n",
    "               \"Low rating should include constructive criticism\"\n",
    "\n",
    "    # Summary should reflect rating\n",
    "    if pred.rating >= 8 and 'not' in pred.summary:\n",
    "        raise AssertionError(\"Summary conflicts with high rating\")\n",
    "\n",
    "    if pred.rating <= 3 and ('great' in pred.summary or 'excellent' in pred.summary):\n",
    "        raise AssertionError(\"Summary conflicts with low rating\")\n",
    "\n",
    "    # Detailed review must be longer than summary\n",
    "    assert len(pred.detailed_review) > len(pred.summary), \\\n",
    "           \"Detailed review should be longer than summary\"\n",
    "\n",
    "    return True\n",
    "\n",
    "review_generator = dspy.Assert(\n",
    "    dspy.Predict(MovieReview),\n",
    "    validation_fn=validate_review_consistency,\n",
    "    max_attempts=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c864f",
   "metadata": {},
   "source": [
    "## Integration with Existing Modules\n",
    "\n",
    "### 1. Assertions with ChainOfThought\n",
    "\n",
    "Add assertions to reasoning chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e874b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicalReasoning(dspy.Signature):\n",
    "    \"\"\"Solve logic puzzles with step-by-step reasoning.\"\"\"\n",
    "    puzzle = dspy.InputField(desc=\"Logic puzzle\", type=str)\n",
    "    reasoning = dspy.OutputField(desc=\"Step-by-step logical reasoning\", type=str)\n",
    "    conclusion = dspy.OutputField(desc=\"Final conclusion\", type=str)\n",
    "    confidence = dspy.OutputField(desc=\"Confidence level (1-10)\", type=int)\n",
    "\n",
    "reasoner = dspy.ChainOfThought(LogicalReasoning)\n",
    "\n",
    "def validate_logical_reasoning(example, pred, trace=None):\n",
    "    \"\"\"Ensure reasoning is logically sound.\"\"\"\n",
    "\n",
    "    # Check for reasoning steps\n",
    "    steps = pred.reasoning.split('\\n')\n",
    "    assert len(steps) >= 3, \"Include at least 3 reasoning steps\"\n",
    "\n",
    "    # Look for logical connectors\n",
    "    connectors = ['therefore', 'because', 'since', 'thus', 'hence']\n",
    "    has_logic = any(connector in pred.reasoning.lower()\n",
    "                   for connector in connectors)\n",
    "    assert has_logic, \"Use logical connectors in reasoning\"\n",
    "\n",
    "    # Conclusion should follow from reasoning\n",
    "    if pred.confidence >= 8:\n",
    "        assert len(pred.conclusion) > 20, \\\n",
    "               \"High confidence conclusions should be well-justified\"\n",
    "\n",
    "    return True\n",
    "\n",
    "logical_reasoner = dspy.Assert(\n",
    "    reasoner,\n",
    "    validation_fn=validate_logical_reasoning,\n",
    "    max_attempts=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8154730b",
   "metadata": {},
   "source": [
    "### 2. Assertions with ReAct\n",
    "\n",
    "Validate agent actions and observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAgent(dspy.Module):\n",
    "    \"\"\"An agent that performs research with validated findings.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.react = dspy.ReAct(\"query -> findings\")\n",
    "\n",
    "    def forward(self, query):\n",
    "        def validate_research(example, pred, trace=None):\n",
    "            \"\"\"Validate research quality.\"\"\"\n",
    "\n",
    "            # Must have taken some actions\n",
    "            if trace and 'tool_calls' not in str(trace):\n",
    "                raise AssertionError(\"Must use search tools for research\")\n",
    "\n",
    "            # Findings should be substantial\n",
    "            assert len(pred.findings) > 100, \"Research findings too brief\"\n",
    "\n",
    "            # Should include sources or evidence\n",
    "            evidence_words = ['according', 'research shows', 'study', 'data']\n",
    "            has_evidence = any(word in pred.findings.lower()\n",
    "                             for word in evidence_words)\n",
    "            assert has_evidence, \"Include evidence or sources in findings\"\n",
    "\n",
    "            return True\n",
    "\n",
    "        # Apply assertion\n",
    "        validated_react = dspy.Assert(\n",
    "            self.react,\n",
    "            validation_fn=validate_research,\n",
    "            max_attempts=3\n",
    "        )\n",
    "\n",
    "        return validated_react(query=query)\n",
    "\n",
    "# Use the validated research agent\n",
    "researcher = ResearchAgent()\n",
    "result = researcher(query=\"Impact of AI on job markets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c7f98a",
   "metadata": {},
   "source": [
    "### 3. Custom Assertion Handlers\n",
    "\n",
    "Create specialized assertion handlers for complex scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c36854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "class AssertionHandler:\n",
    "    \"\"\"Custom handler for complex assertion scenarios.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.attempt_history = []\n",
    "\n",
    "    def handle_assertion_failure(self, assertion_type, error_msg, attempt):\n",
    "        \"\"\"Custom logic for handling assertion failures.\"\"\"\n",
    "        self.attempt_history.append({\n",
    "            'attempt': attempt,\n",
    "            'type': assertion_type,\n",
    "            'error': error_msg,\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "\n",
    "        # Different recovery strategies based on error type\n",
    "        if \"format\" in error_msg.lower():\n",
    "            return \"Please ensure strict adherence to the required format.\"\n",
    "        elif \"length\" in error_msg.lower():\n",
    "            return \"Make your response more detailed and comprehensive.\"\n",
    "        elif \"accuracy\" in error_msg.lower():\n",
    "            return \"Double-check your facts and calculations.\"\n",
    "        else:\n",
    "            return \"Review your response for completeness and accuracy.\"\n",
    "\n",
    "    def generate_recovery_prompt(self, original_input, failed_output, error_msg):\n",
    "        \"\"\"Generate a refined prompt for retry attempts.\"\"\"\n",
    "        recovery_instruction = self.handle_assertion_failure(\n",
    "            \"validation\", error_msg, len(self.attempt_history)\n",
    "        )\n",
    "\n",
    "        return f\"\"\"\n",
    "        Original task: {original_input}\n",
    "\n",
    "        Your previous attempt: {failed_output}\n",
    "\n",
    "        Error: {error_msg}\n",
    "\n",
    "        {recovery_instruction}\n",
    "\n",
    "        Please provide an improved response that addresses the issue.\n",
    "        \"\"\"\n",
    "\n",
    "# Use custom handler\n",
    "handler = AssertionHandler()\n",
    "\n",
    "custom_assert = dspy.Assert(\n",
    "    dspy.Predict(\"task -> result\"),\n",
    "    validation_fn=lambda ex, pred, tr: validate_output(ex, pred, tr),\n",
    "    max_attempts=3,\n",
    "    error_handler=handler.handle_assertion_failure\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c31f9",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Design Effective Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230007ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Specific and actionable error messages\n",
    "def validate_email(example, pred, trace=None):\n",
    "    import re\n",
    "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    if not re.match(pattern, pred.email):\n",
    "        raise AssertionError(\n",
    "            f\"'{pred.email}' is not a valid email. \"\n",
    "            f\"Must follow format: user@domain.com\"\n",
    "        )\n",
    "    return True\n",
    "\n",
    "# Bad: Generic errors\n",
    "def validate_email_bad(example, pred, trace=None):\n",
    "    if '@' not in pred.email:\n",
    "        raise AssertionError(\"Invalid email\")  # Not helpful\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb2ff14",
   "metadata": {},
   "source": [
    "### 2. Balance Strictness and Flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use suggestions for preferences, assertions for requirements\n",
    "def generate_content(topic):\n",
    "    # Hard requirement: must have title\n",
    "    assert hasattr(pred, 'title'), \"Content must have a title\"\n",
    "\n",
    "    # Soft suggestion: prefer subheadings (not mandatory)\n",
    "    suggest_add_subheadings(pred.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8348cfcd",
   "metadata": {},
   "source": [
    "### 3. Handle Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbd7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_validator(example, pred, trace=None):\n",
    "    try:\n",
    "        # Main validation logic\n",
    "        validate_main_logic(example, pred, trace)\n",
    "        return True\n",
    "    except AttributeError:\n",
    "        raise AssertionError(\"Required field missing from output\")\n",
    "    except (TypeError, ValueError):\n",
    "        raise AssertionError(\"Output has incorrect type or format\")\n",
    "    except Exception as e:\n",
    "        raise AssertionError(f\"Validation error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb7588",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "### 1. Assertion Overhead\n",
    "\n",
    "Each assertion adds computational overhead. Use judiciously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Critical assertions\n",
    "validate_safety = dspy.Assert(safety_module, validate_safety_constraints)\n",
    "\n",
    "# Consider: Performance-critical paths might use lighter validation\n",
    "quick_validate = lambda ex, pred: len(pred.output) > 10  # Simple check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d6c5a",
   "metadata": {},
   "source": [
    "### 2. Caching Validation Results\n",
    "\n",
    "Cache expensive validation operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d282912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def cached_syntax_check(code_hash):\n",
    "    \"\"\"Cache syntax validation for identical code.\"\"\"\n",
    "    # Check syntax...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d92589d",
   "metadata": {},
   "source": [
    "### 3. Progressive Validation\n",
    "\n",
    "Validate in order of cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressive_validate(example, pred, trace=None):\n",
    "    # Fast checks first\n",
    "    assert len(pred.output) > 0, \"Empty output\"\n",
    "\n",
    "    # Medium checks\n",
    "    assert pred.output.count('\\n') > 2, \"Need multiple paragraphs\"\n",
    "\n",
    "    # Expensive checks last\n",
    "    validate_semantics(pred.output)  # Slow operation\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21253d1",
   "metadata": {},
   "source": [
    "## Debugging Assertions\n",
    "\n",
    "### 1. Trace Inspection\n",
    "\n",
    "Examine assertion failures for debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_assertion(example, pred, trace=None):\n",
    "    \"\"\"Debug assertion with detailed information.\"\"\"\n",
    "    print(f\"Input: {example}\")\n",
    "    print(f\"Output: {pred}\")\n",
    "    print(f\"Trace: {trace}\")\n",
    "\n",
    "    # Perform validation\n",
    "    result = actual_validation(example, pred, trace)\n",
    "\n",
    "    if not result:\n",
    "        print(\"Validation failed!\")\n",
    "        # Analyze why...\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f387894",
   "metadata": {},
   "source": [
    "### 2. Assertion Metrics\n",
    "\n",
    "Track assertion performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa092719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssertionMetrics:\n",
    "    def __init__(self):\n",
    "        self.stats = {\n",
    "            'total_attempts': 0,\n",
    "            'failures': 0,\n",
    "            'retries': 0,\n",
    "            'success_rate': 0\n",
    "        }\n",
    "\n",
    "    def record_attempt(self, success, retries):\n",
    "        self.stats['total_attempts'] += 1\n",
    "        if not success:\n",
    "            self.stats['failures'] += 1\n",
    "        self.stats['retries'] += retries\n",
    "        self.stats['success_rate'] = (\n",
    "            (self.stats['total_attempts'] - self.stats['failures']) /\n",
    "            self.stats['total_attempts']\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a95294",
   "metadata": {},
   "source": [
    "## Advanced Assertion Patterns\n",
    "\n",
    "### 1. Hierarchical Assertions\n",
    "\n",
    "Multi-level validation with cascading constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, Generic\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "class HierarchicalAssertion(Generic[T], ABC):\n",
    "    \"\"\"Base class for hierarchical assertion systems.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, level: int = 0):\n",
    "        self.name = name\n",
    "        self.level = level\n",
    "        self.children = []\n",
    "        self.parent = None\n",
    "\n",
    "    def add_child(self, child: 'HierarchicalAssertion'):\n",
    "        \"\"\"Add child assertion.\"\"\"\n",
    "        child.parent = self\n",
    "        child.level = self.level + 1\n",
    "        self.children.append(child)\n",
    "\n",
    "    def validate_hierarchy(self, example, pred, trace=None) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Validate entire hierarchy.\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        # Validate current level\n",
    "        local_valid, local_errors = self.validate(example, pred, trace)\n",
    "        if not local_valid:\n",
    "            errors.extend([f\"[{self.name}] {e}\" for e in local_errors])\n",
    "\n",
    "        # Validate children if current level passes\n",
    "        if local_valid:\n",
    "            for child in self.children:\n",
    "                child_valid, child_errors = child.validate_hierarchy(\n",
    "                    example, pred, trace\n",
    "                )\n",
    "                if not child_valid:\n",
    "                    errors.extend(child_errors)\n",
    "\n",
    "        return len(errors) == 0, errors\n",
    "\n",
    "    @abstractmethod\n",
    "    def validate(self, example, pred, trace=None) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Validate at this level.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Example: Document validation hierarchy\n",
    "class DocumentAssertion(HierarchicalAssertion):\n",
    "    \"\"\"Top-level document validation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"document\", level=0)\n",
    "\n",
    "        # Add child assertions\n",
    "        self.add_child(StructureAssertion())\n",
    "        self.add_child(ContentAssertion())\n",
    "        self.add_child(FormatAssertion())\n",
    "\n",
    "    def validate(self, example, pred, trace=None):\n",
    "        \"\"\"Validate document-level constraints.\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        # Basic document checks\n",
    "        if not hasattr(pred, 'content'):\n",
    "            return False, [\"Missing content field\"]\n",
    "\n",
    "        if len(pred.content) < 100:\n",
    "            errors.append(\"Document too short (minimum 100 characters)\")\n",
    "\n",
    "        if len(pred.content) > 10000:\n",
    "            errors.append(\"Document too long (maximum 10000 characters)\")\n",
    "\n",
    "        return len(errors) == 0, errors\n",
    "\n",
    "class StructureAssertion(HierarchicalAssertion):\n",
    "    \"\"\"Validate document structure.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"structure\")\n",
    "\n",
    "    def validate(self, example, pred, trace=None):\n",
    "        \"\"\"Validate structural elements.\"\"\"\n",
    "        errors = []\n",
    "        content = pred.content\n",
    "\n",
    "        # Check for sections\n",
    "        if '#' not in content:\n",
    "            errors.append(\"Document missing section headers\")\n",
    "\n",
    "        # Check for paragraphs\n",
    "        paragraphs = content.split('\\n\\n')\n",
    "        if len(paragraphs) < 3:\n",
    "            errors.append(\"Document needs at least 3 paragraphs\")\n",
    "\n",
    "        # Check for flow\n",
    "        if not self.has_logical_flow(content):\n",
    "            errors.append(\"Document lacks logical flow\")\n",
    "\n",
    "        return len(errors) == 0, errors\n",
    "\n",
    "    def has_logical_flow(self, content: str) -> bool:\n",
    "        \"\"\"Check if content has logical flow.\"\"\"\n",
    "        # Simple heuristic: look for transition words\n",
    "        transitions = ['however', 'therefore', 'furthermore', 'consequently']\n",
    "        return any(word in content.lower() for word in transitions)\n",
    "\n",
    "# Use hierarchical assertions\n",
    "doc_validator = DocumentAssertion()\n",
    "\n",
    "# Wrap with hierarchical validation\n",
    "class DocumentGenerator(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generator = dspy.Predict(\"topic -> content\")\n",
    "        self.hierarchical_validator = doc_validator\n",
    "\n",
    "    def forward(self, topic):\n",
    "        result = self.generator(topic=topic)\n",
    "\n",
    "        # Validate hierarchy\n",
    "        is_valid, errors = self.hierarchical_validator.validate_hierarchy(\n",
    "            example=None, pred=result\n",
    "        )\n",
    "\n",
    "        if not is_valid:\n",
    "            # Refine based on hierarchical feedback\n",
    "            refined_result = self.refine_hierarchically(\n",
    "                result, errors, self.hierarchical_validator\n",
    "            )\n",
    "            return refined_result\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26e8c9d",
   "metadata": {},
   "source": [
    "### 2. Probabilistic Assertions\n",
    "\n",
    "Assertions with confidence-based validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "class ProbabilisticAssertion:\n",
    "    \"\"\"Assertions with probabilistic validation.\"\"\"\n",
    "\n",
    "    def __init__(self, confidence_threshold=0.95):\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.validation_history = []\n",
    "\n",
    "    def validate_with_confidence(self, example, pred, trace=None) -> Tuple[bool, float, str]:\n",
    "        \"\"\"Validate with confidence scoring.\"\"\"\n",
    "        # Calculate confidence score\n",
    "        confidence = self.calculate_confidence(example, pred, trace)\n",
    "\n",
    "        # Determine if passes threshold\n",
    "        passes = confidence >= self.confidence_threshold\n",
    "\n",
    "        # Generate explanation\n",
    "        explanation = self.generate_explanation(confidence, pred)\n",
    "\n",
    "        # Record for learning\n",
    "        self.validation_history.append({\n",
    "            'confidence': confidence,\n",
    "            'passed': passes,\n",
    "            'explanation': explanation\n",
    "        })\n",
    "\n",
    "        return passes, confidence, explanation\n",
    "\n",
    "    def calculate_confidence(self, example, pred, trace=None):\n",
    "        \"\"\"Calculate confidence score for validation.\"\"\"\n",
    "        confidence_factors = []\n",
    "\n",
    "        # Factor 1: Structural consistency\n",
    "        struct_confidence = self.check_structural_consistency(pred)\n",
    "        confidence_factors.append(struct_confidence)\n",
    "\n",
    "        # Factor 2: Semantic coherence\n",
    "        semantic_confidence = self.check_semantic_coherence(pred)\n",
    "        confidence_factors.append(semantic_confidence)\n",
    "\n",
    "        # Factor 3: Historical performance\n",
    "        history_confidence = self.get_historical_confidence()\n",
    "        confidence_factors.append(history_confidence)\n",
    "\n",
    "        # Combine factors (weighted average)\n",
    "        weights = [0.4, 0.4, 0.2]  # Adjust as needed\n",
    "        confidence = sum(w * c for w, c in zip(weights, confidence_factors))\n",
    "\n",
    "        return confidence\n",
    "\n",
    "    def check_structural_consistency(self, pred) -> float:\n",
    "        \"\"\"Check structural consistency of output.\"\"\"\n",
    "        score = 0.0\n",
    "\n",
    "        # Check required fields\n",
    "        required_fields = getattr(pred, '_required_fields', [])\n",
    "        for field in required_fields:\n",
    "            if hasattr(pred, field) and getattr(pred, field):\n",
    "                score += 1.0 / len(required_fields)\n",
    "\n",
    "        # Check field consistency\n",
    "        if hasattr(pred, 'answer') and hasattr(pred, 'confidence'):\n",
    "            # Higher confidence should correlate with longer answers\n",
    "            if pred.confidence > 0.8 and len(pred.answer) < 10:\n",
    "                score *= 0.5  # Penalize inconsistency\n",
    "\n",
    "        return min(score, 1.0)\n",
    "\n",
    "    def check_semantic_coherence(self, pred) -> float:\n",
    "        \"\"\"Check semantic coherence using NLP techniques.\"\"\"\n",
    "        # Simplified coherence check\n",
    "        if not hasattr(pred, 'answer'):\n",
    "            return 0.0\n",
    "\n",
    "        answer = pred.answer\n",
    "\n",
    "        # Check for repeated phrases\n",
    "        words = answer.lower().split()\n",
    "        unique_words = set(words)\n",
    "        repetition_ratio = len(unique_words) / len(words) if words else 0\n",
    "\n",
    "        # Check sentence structure\n",
    "        sentences = answer.split('.')\n",
    "        avg_sentence_length = np.mean([len(s.split()) for s in sentences if s])\n",
    "\n",
    "        # Combine factors\n",
    "        coherence_score = 0.0\n",
    "        coherence_score += repetition_ratio * 0.4\n",
    "        coherence_score += min(avg_sentence_length / 15, 1.0) * 0.3\n",
    "        coherence_score += 0.3 if 5 <= len(sentences) <= 10 else 0.1\n",
    "\n",
    "        return coherence_score\n",
    "\n",
    "    def get_historical_confidence(self) -> float:\n",
    "        \"\"\"Calculate confidence based on historical performance.\"\"\"\n",
    "        if not self.validation_history:\n",
    "            return 0.5  # Neutral for no history\n",
    "\n",
    "        # Recent performance more important\n",
    "        recent_history = self.validation_history[-10:]\n",
    "        success_rate = sum(1 for h in recent_history if h['passed']) / len(recent_history)\n",
    "\n",
    "        return success_rate\n",
    "\n",
    "class AdaptiveThreshold:\n",
    "    \"\"\"Adaptive confidence threshold based on context.\"\"\"\n",
    "\n",
    "    def __init__(self, initial_threshold=0.95):\n",
    "        self.base_threshold = initial_threshold\n",
    "        self.context_adjustments = {}\n",
    "        self.performance_feedback = []\n",
    "\n",
    "    def get_threshold(self, context: dict) -> float:\n",
    "        \"\"\"Get adjusted threshold for context.\"\"\"\n",
    "        threshold = self.base_threshold\n",
    "\n",
    "        # Adjust based on context\n",
    "        context_key = self.get_context_key(context)\n",
    "        if context_key in self.context_adjustments:\n",
    "            threshold *= self.context_adjustments[context_key]\n",
    "\n",
    "        # Adjust based on recent performance\n",
    "        if self.performance_feedback:\n",
    "            recent_performance = np.mean(self.performance_feedback[-5:])\n",
    "            if recent_performance < 0.8:\n",
    "                threshold *= 0.9  # Lower threshold if struggling\n",
    "            elif recent_performance > 0.95:\n",
    "                threshold *= 1.1  # Raise threshold if doing well\n",
    "\n",
    "        return min(max(threshold, 0.5), 0.99)  # Keep within bounds\n",
    "\n",
    "    def update_adjustment(self, context: dict, adjustment: float):\n",
    "        \"\"\"Update context adjustment based on feedback.\"\"\"\n",
    "        context_key = self.get_context_key(context)\n",
    "        self.context_adjustments[context_key] = adjustment\n",
    "\n",
    "    def get_context_key(self, context: dict) -> str:\n",
    "        \"\"\"Generate key for context lookup.\"\"\"\n",
    "        # Simplified context key generation\n",
    "        key_parts = []\n",
    "        if 'domain' in context:\n",
    "            key_parts.append(context['domain'])\n",
    "        if 'complexity' in context:\n",
    "            key_parts.append(f\"complexity_{context['complexity']}\")\n",
    "        return \"_\".join(key_parts) or \"default\"\n",
    "\n",
    "# Usage with probabilistic assertions\n",
    "probabilistic_assert = ProbabilisticAssertion(confidence_threshold=0.9)\n",
    "adaptive_threshold = AdaptiveThreshold()\n",
    "\n",
    "class ProbabilisticValidator(dspy.Module):\n",
    "    def __init__(self, base_module):\n",
    "        super().__init__()\n",
    "        self.base_module = base_module\n",
    "        self.prob_assert = probabilistic_assert\n",
    "        self.adaptive_threshold = adaptive_threshold\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        # Get context\n",
    "        context = {\n",
    "            'domain': kwargs.get('domain', 'general'),\n",
    "            'complexity': kwargs.get('complexity', 'medium')\n",
    "        }\n",
    "\n",
    "        # Get adaptive threshold\n",
    "        threshold = self.adaptive_threshold.get_threshold(context)\n",
    "\n",
    "        # Generate result\n",
    "        result = self.base_module(**kwargs)\n",
    "\n",
    "        # Validate with confidence\n",
    "        passes, confidence, explanation = self.prob_assert.validate_with_confidence(\n",
    "            example=None, pred=result\n",
    "        )\n",
    "\n",
    "        # Check against adaptive threshold\n",
    "        if confidence < threshold:\n",
    "            # Provide feedback for learning\n",
    "            self.adaptive_threshold.update_adjustment(\n",
    "                context,\n",
    "                threshold / confidence  # Adjustment factor\n",
    "            )\n",
    "\n",
    "            # Try to improve\n",
    "            improved = self.improve_result(result, explanation)\n",
    "            if improved:\n",
    "                result = improved\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed452b",
   "metadata": {},
   "source": [
    "### 3. Distributed Assertions\n",
    "\n",
    "Assertions across multiple model calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import asyncio\n",
    "\n",
    "class DistributedAssertionSystem:\n",
    "    \"\"\"Manages assertions across distributed model calls.\"\"\"\n",
    "\n",
    "    def __init__(self, assertion_nodes: Dict[str, 'AssertionNode']):\n",
    "        self.assertion_nodes = assertion_nodes\n",
    "        self.communication_bus = AssertionCommunicationBus()\n",
    "        self.coordinator = AssertionCoordinator(assertion_nodes)\n",
    "\n",
    "    def validate_distributed(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Coordinate distributed validation.\"\"\"\n",
    "        # Create validation plan\n",
    "        plan = self.coordinator.create_validation_plan(inputs)\n",
    "\n",
    "        # Execute in parallel where possible\n",
    "        results = self.execute_validation_plan(plan)\n",
    "\n",
    "        # Aggregate results\n",
    "        aggregated = self.coordinator.aggregate_results(results)\n",
    "\n",
    "        # Resolve conflicts\n",
    "        resolved = self.coordinator.resolve_conflicts(aggregated)\n",
    "\n",
    "        return resolved\n",
    "\n",
    "    def execute_validation_plan(self, plan: Dict) -> Dict:\n",
    "        \"\"\"Execute validation plan with parallel execution.\"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # Identify parallelizable tasks\n",
    "        parallel_tasks = []\n",
    "        sequential_tasks = []\n",
    "\n",
    "        for task_id, task in plan.items():\n",
    "            if task.get('parallelizable', False):\n",
    "                parallel_tasks.append((task_id, task))\n",
    "            else:\n",
    "                sequential_tasks.append((task_id, task))\n",
    "\n",
    "        # Execute parallel tasks\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            future_to_task = {\n",
    "                executor.submit(self.execute_task, task): task_id\n",
    "                for task_id, task in parallel_tasks\n",
    "            }\n",
    "\n",
    "            for future in concurrent.futures.as_completed(future_to_task):\n",
    "                task_id = future_to_task[future]\n",
    "                try:\n",
    "                    results[task_id] = future.result()\n",
    "                except Exception as e:\n",
    "                    results[task_id] = {'error': str(e)}\n",
    "\n",
    "        # Execute sequential tasks\n",
    "        for task_id, task in sequential_tasks:\n",
    "            results[task_id] = self.execute_task(task)\n",
    "\n",
    "        return results\n",
    "\n",
    "class AssertionNode:\n",
    "    \"\"\"Individual assertion node in distributed system.\"\"\"\n",
    "\n",
    "    def __init__(self, node_id: str, assertions: List[dspy.Assert]):\n",
    "        self.node_id = node_id\n",
    "        self.assertions = assertions\n",
    "        self.local_cache = {}\n",
    "\n",
    "    def validate(self, data: Dict[str, Any], context: Dict = None) -> Dict:\n",
    "        \"\"\"Validate with local assertions.\"\"\"\n",
    "        results = {\n",
    "            'node_id': self.node_id,\n",
    "            'validations': [],\n",
    "            'overall_status': 'passed',\n",
    "            'metadata': {\n",
    "                'validation_count': len(self.assertions),\n",
    "                'execution_time': 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for assertion in self.assertions:\n",
    "            try:\n",
    "                # Check cache first\n",
    "                cache_key = self.get_cache_key(data, assertion)\n",
    "                if cache_key in self.local_cache:\n",
    "                    validation_result = self.local_cache[cache_key]\n",
    "                else:\n",
    "                    # Execute assertion\n",
    "                    validation_result = self.execute_assertion(\n",
    "                        assertion, data, context\n",
    "                    )\n",
    "                    # Cache result\n",
    "                    self.local_cache[cache_key] = validation_result\n",
    "\n",
    "                results['validations'].append({\n",
    "                    'assertion_id': id(assertion),\n",
    "                    'result': validation_result,\n",
    "                    'cached': cache_key in self.local_cache\n",
    "                })\n",
    "\n",
    "                if not validation_result['passed']:\n",
    "                    results['overall_status'] = 'failed'\n",
    "\n",
    "            except Exception as e:\n",
    "                results['validations'].append({\n",
    "                    'assertion_id': id(assertion),\n",
    "                    'error': str(e),\n",
    "                    'passed': False\n",
    "                })\n",
    "                results['overall_status'] = 'error'\n",
    "\n",
    "        results['metadata']['execution_time'] = time.time() - start_time\n",
    "\n",
    "        return results\n",
    "\n",
    "# Example: Multi-modal validation system\n",
    "class MultiModalValidationSystem:\n",
    "    \"\"\"Validates outputs across different modalities.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create assertion nodes for each modality\n",
    "        self.text_node = AssertionNode(\n",
    "            'text_validation',\n",
    "            [\n",
    "                dspy.Assert(validate_text_coherence),\n",
    "                dspy.Assert(validate_text_quality),\n",
    "                dspy.Assert(validate_text_length)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.image_node = AssertionNode(\n",
    "            'image_validation',\n",
    "            [\n",
    "                dspy.Assert(validate_image_quality),\n",
    "                dspy.Assert(validate_image_content),\n",
    "                dspy.Assert(validate_image_style)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.multimodal_node = AssertionNode(\n",
    "            'multimodal_validation',\n",
    "            [\n",
    "                dspy.Assert(validate_text_image_consistency),\n",
    "                dspy.Assert(validate_modality_balance)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Create distributed system\n",
    "        self.distributed_system = DistributedAssertionSystem({\n",
    "            'text': self.text_node,\n",
    "            'image': self.image_node,\n",
    "            'multimodal': self.multimodal_node\n",
    "        })\n",
    "\n",
    "    def validate_multimodal_output(self, output: Dict[str, Any]):\n",
    "        \"\"\"Validate multimodal output.\"\"\"\n",
    "        # Prepare inputs for each node\n",
    "        inputs = {\n",
    "            'text': {'text_data': output.get('text', '')},\n",
    "            'image': {'image_data': output.get('image', None)},\n",
    "            'multimodal': {\n",
    "                'text_data': output.get('text', ''),\n",
    "                'image_data': output.get('image', None)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Execute distributed validation\n",
    "        results = self.distributed_system.validate_distributed(inputs)\n",
    "\n",
    "        # Generate comprehensive report\n",
    "        report = self.generate_validation_report(results)\n",
    "\n",
    "        return report\n",
    "\n",
    "    def generate_validation_report(self, results: Dict) -> Dict:\n",
    "        \"\"\"Generate comprehensive validation report.\"\"\"\n",
    "        report = {\n",
    "            'overall_status': 'passed',\n",
    "            'modality_results': {},\n",
    "            'cross_modality_issues': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "\n",
    "        # Process individual modality results\n",
    "        for modality, result in results.items():\n",
    "            if 'error' in result:\n",
    "                report['modality_results'][modality] = {\n",
    "                    'status': 'error',\n",
    "                    'message': result['error']\n",
    "                }\n",
    "                report['overall_status'] = 'failed'\n",
    "            else:\n",
    "                report['modality_results'][modality] = {\n",
    "                    'status': result.get('overall_status', 'unknown'),\n",
    "                    'validations_passed': sum(\n",
    "                        1 for v in result.get('validations', [])\n",
    "                        if v.get('result', {}).get('passed', False)\n",
    "                    ),\n",
    "                    'total_validations': len(result.get('validations', [])),\n",
    "                    'execution_time': result.get('metadata', {}).get('execution_time', 0)\n",
    "                }\n",
    "\n",
    "                if result.get('overall_status') != 'passed':\n",
    "                    report['overall_status'] = 'failed'\n",
    "\n",
    "        # Cross-modality analysis\n",
    "        if 'text' in results and 'image' in results:\n",
    "            text_issues = self.extract_issues(results['text'])\n",
    "            image_issues = self.extract_issues(results['image'])\n",
    "\n",
    "            # Find related issues\n",
    "            for text_issue in text_issues:\n",
    "                for image_issue in image_issues:\n",
    "                    if self.are_related_issues(text_issue, image_issue):\n",
    "                        report['cross_modality_issues'].append({\n",
    "                            'type': 'related',\n",
    "                            'text_issue': text_issue,\n",
    "                            'image_issue': image_issue,\n",
    "                            'severity': 'high'\n",
    "                        })\n",
    "\n",
    "        # Generate recommendations\n",
    "        report['recommendations'] = self.generate_recommendations(report)\n",
    "\n",
    "        return report\n",
    "\n",
    "# Usage\n",
    "multimodal_validator = MultiModalValidationSystem()\n",
    "\n",
    "# Validate multimodal output\n",
    "output = {\n",
    "    'text': 'A beautiful sunset over the mountains',\n",
    "    'image': generated_image\n",
    "}\n",
    "\n",
    "validation_report = multimodal_validator.validate_multimodal_output(output)\n",
    "print(f\"Overall status: {validation_report['overall_status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240979bb",
   "metadata": {},
   "source": [
    "### 4. Learning Assertions\n",
    "\n",
    "Assertions that improve over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34dc663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "class LearningAssertion:\n",
    "    \"\"\"Assertions that learn from validation history.\"\"\"\n",
    "\n",
    "    def __init__(self, assertion_name: str, model_path: str = None):\n",
    "        self.assertion_name = assertion_name\n",
    "        self.model_path = model_path or f\"models/{assertion_name}_model.pkl\"\n",
    "        self.model = self.load_or_create_model()\n",
    "        self.training_data = []\n",
    "        self.feature_extractor = AssertionFeatureExtractor()\n",
    "\n",
    "    def load_or_create_model(self):\n",
    "        \"\"\"Load existing model or create new one.\"\"\"\n",
    "        if Path(self.model_path).exists():\n",
    "            return joblib.load(self.model_path)\n",
    "        else:\n",
    "            return RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    def validate_with_learning(self, example, pred, trace=None):\n",
    "        \"\"\"Validate using learned patterns.\"\"\"\n",
    "        # Extract features\n",
    "        features = self.feature_extractor.extract(example, pred, trace)\n",
    "\n",
    "        # Predict validation outcome\n",
    "        prediction = self.model.predict([features])[0]\n",
    "        confidence = self.model.predict_proba([features])[0].max()\n",
    "\n",
    "        # Get feature importance\n",
    "        feature_importance = self.get_feature_importance(features)\n",
    "\n",
    "        return {\n",
    "            'passed': bool(prediction),\n",
    "            'confidence': float(confidence),\n",
    "            'feature_importance': feature_importance,\n",
    "            'learned': True\n",
    "        }\n",
    "\n",
    "    def learn_from_feedback(self, example, pred, actual_outcome, trace=None):\n",
    "        \"\"\"Learn from actual validation outcomes.\"\"\"\n",
    "        # Extract features\n",
    "        features = self.feature_extractor.extract(example, pred, trace)\n",
    "\n",
    "        # Add to training data\n",
    "        self.training_data.append({\n",
    "            'features': features,\n",
    "            'outcome': actual_outcome\n",
    "        })\n",
    "\n",
    "        # Retrain if enough data\n",
    "        if len(self.training_data) >= 50:\n",
    "            self.retrain_model()\n",
    "\n",
    "    def retrain_model(self):\n",
    "        \"\"\"Retrain the assertion model.\"\"\"\n",
    "        if not self.training_data:\n",
    "            return\n",
    "\n",
    "        # Prepare training data\n",
    "        X = [d['features'] for d in self.training_data]\n",
    "        y = [d['outcome'] for d in self.training_data]\n",
    "\n",
    "        # Retrain\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "        # Save model\n",
    "        joblib.dump(self.model, self.model_path)\n",
    "\n",
    "        # Clear training data to save memory\n",
    "        self.training_data = []\n",
    "\n",
    "    def get_feature_importance(self, features):\n",
    "        \"\"\"Get importance of each feature for this prediction.\"\"\"\n",
    "        if not hasattr(self.model, 'feature_importances_'):\n",
    "            return {}\n",
    "\n",
    "        feature_names = self.feature_extractor.get_feature_names()\n",
    "        importances = self.model.feature_importances_\n",
    "\n",
    "        return {\n",
    "            name: float(imp)\n",
    "            for name, imp in zip(feature_names, importances)\n",
    "        }\n",
    "\n",
    "class AssertionFeatureExtractor:\n",
    "    \"\"\"Extracts features for learning assertions.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_cache = {}\n",
    "\n",
    "    def extract(self, example, pred, trace=None):\n",
    "        \"\"\"Extract comprehensive features.\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Text features\n",
    "        if hasattr(pred, 'answer'):\n",
    "            text_features = self.extract_text_features(pred.answer)\n",
    "            features.update({f\"text_{k}\": v for k, v in text_features.items()})\n",
    "\n",
    "        # Structural features\n",
    "        struct_features = self.extract_structural_features(pred)\n",
    "        features.update({f\"struct_{k}\": v for k, v in struct_features.items()})\n",
    "\n",
    "        # Context features\n",
    "        if example:\n",
    "            context_features = self.extract_context_features(example, pred)\n",
    "            features.update({f\"context_{k}\": v for k, v in context_features.items()})\n",
    "\n",
    "        # Trace features\n",
    "        if trace:\n",
    "            trace_features = self.extract_trace_features(trace)\n",
    "            features.update({f\"trace_{k}\": v for k, v in trace_features.items()})\n",
    "\n",
    "        return features\n",
    "\n",
    "    def extract_text_features(self, text: str) -> Dict:\n",
    "        \"\"\"Extract text-based features.\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Basic statistics\n",
    "        words = text.split()\n",
    "        sentences = text.split('.')\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "\n",
    "        features['word_count'] = len(words)\n",
    "        features['sentence_count'] = len(sentences)\n",
    "        features['paragraph_count'] = len(paragraphs)\n",
    "        features['avg_word_length'] = np.mean([len(w) for w in words]) if words else 0\n",
    "        features['avg_sentence_length'] = np.mean([len(s.split()) for s in sentences if s]) if sentences else 0\n",
    "\n",
    "        # Vocabulary diversity\n",
    "        unique_words = set(words)\n",
    "        features['vocab_diversity'] = len(unique_words) / len(words) if words else 0\n",
    "\n",
    "        # Punctuation patterns\n",
    "        features['exclamation_count'] = text.count('!')\n",
    "        features['question_count'] = text.count('?')\n",
    "        features['comma_count'] = text.count(',')\n",
    "\n",
    "        # Readability approximation\n",
    "        features['readability_score'] = self.calculate_readability(text)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def extract_structural_features(self, pred) -> Dict:\n",
    "        \"\"\"Extract structural features.\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Field presence\n",
    "        all_fields = dir(pred)\n",
    "        features['field_count'] = len(all_fields)\n",
    "        features['has_confidence'] = hasattr(pred, 'confidence')\n",
    "        features['has_reasoning'] = hasattr(pred, 'reasoning')\n",
    "\n",
    "        # Field consistency\n",
    "        if hasattr(pred, 'confidence') and hasattr(pred, 'answer'):\n",
    "            # High confidence with short answer might be suspicious\n",
    "            if pred.confidence > 0.9 and len(pred.answer) < 10:\n",
    "                features['confidence_consistency'] = 0\n",
    "            else:\n",
    "                features['confidence_consistency'] = 1\n",
    "\n",
    "        return features\n",
    "\n",
    "    def calculate_readability(self, text: str) -> float:\n",
    "        \"\"\"Simple readability score.\"\"\"\n",
    "        # Simplified Flesch Reading Ease\n",
    "        words = text.split()\n",
    "        sentences = text.split('.')\n",
    "\n",
    "        if not words or not sentences:\n",
    "            return 0\n",
    "\n",
    "        avg_sentence_length = len(words) / len(sentences)\n",
    "        avg_syllables = np.mean([self.count_syllables(w) for w in words])\n",
    "\n",
    "        readability = 206.835 - 1.015 * avg_sentence_length - 84.6 * avg_syllables\n",
    "        return max(0, min(100, readability))\n",
    "\n",
    "    def count_syllables(self, word: str) -> int:\n",
    "        \"\"\"Approximate syllable count.\"\"\"\n",
    "        vowels = \"aeiouy\"\n",
    "        word = word.lower()\n",
    "        syllables = 0\n",
    "        prev_was_vowel = False\n",
    "\n",
    "        for char in word:\n",
    "            is_vowel = char in vowels\n",
    "            if is_vowel and not prev_was_vowel:\n",
    "                syllables += 1\n",
    "            prev_was_vowel = is_vowel\n",
    "\n",
    "        # Adjust for silent e\n",
    "        if word.endswith('e') and syllables > 1:\n",
    "            syllables -= 1\n",
    "\n",
    "        return max(1, syllables)\n",
    "\n",
    "# Usage with learning assertions\n",
    "learning_assertion = LearningAssertion(\"answer_quality\")\n",
    "\n",
    "class AdaptiveQA(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qa = dspy.ChainOfThought(\"question -> answer\")\n",
    "        self.learning_assertion = learning_assertion\n",
    "\n",
    "    def forward(self, question):\n",
    "        result = self.qa(question=question)\n",
    "\n",
    "        # Validate with learning\n",
    "        validation = self.learning_assertion.validate_with_learning(\n",
    "            example={'question': question},\n",
    "            pred=result\n",
    "        )\n",
    "\n",
    "        if not validation['passed'] and validation['confidence'] > 0.8:\n",
    "            # High confidence failure - likely an error\n",
    "            print(f\"Validation failed with high confidence: {validation['feature_importance']}\")\n",
    "\n",
    "            # Learn from this\n",
    "            self.learning_assertion.learn_from_feedback(\n",
    "                example={'question': question},\n",
    "                pred=result,\n",
    "                actual_outcome=False  # Failed\n",
    "            )\n",
    "\n",
    "            # Try again\n",
    "            result = self.qa(question=question)\n",
    "\n",
    "        return result\n",
    "\n",
    "# Later, with human feedback\n",
    "# learning_assertion.learn_from_feedback(\n",
    "#     example=example,\n",
    "#     pred=prediction,\n",
    "#     actual_outcome=True  # Human confirmed it was good\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ccffb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "DSPy Assertions provide:\n",
    "\n",
    "- **Runtime validation** of model outputs\n",
    "- **Automatic refinement** when constraints fail\n",
    "- **Flexible constraint types** - hard and soft constraints\n",
    "- **Self-improving systems** through iterative refinement\n",
    "- **Production reliability** through guaranteed output quality\n",
    "- **Hierarchical validation** for complex requirements\n",
    "- **Probabilistic assertions** with confidence-based decisions\n",
    "- **Distributed assertions** across multiple model calls\n",
    "- **Learning assertions** that improve from experience\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Use Assert for requirements** - Critical constraints that must pass\n",
    "2. **Use Suggest for preferences** - Guidance for improving quality\n",
    "3. **Write clear error messages** - Help the model understand failures\n",
    "4. **Balance validation cost** - Consider performance implications\n",
    "5. **Compose multiple assertions** - Build comprehensive validation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Self-Refining Pipelines](../07-advanced-topics/07-self-refining-pipelines.md) - Learn advanced patterns\n",
    "- [Constraint-Driven Optimization](../05-optimizers/07-constraint-driven-optimization.md) - Optimize with constraints\n",
    "- [Assertion-Driven Applications](../08-case-studies/06-assertion-driven-applications.md) - Real-world examples\n",
    "- [Exercises](./07-exercises.md) - Practice assertion techniques\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [DSPy Documentation: Assertions](https://dspy-docs.vercel.app/docs/deep-dive/assertions)\n",
    "- [Constraint Programming](https://en.wikipedia.org/wiki/Constraint_programming) - Theoretical foundation\n",
    "- [Runtime Verification](https://en.wikipedia.org/wiki/Runtime_verification) - Validation techniques"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
