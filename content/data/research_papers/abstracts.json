{
  "research_papers": [
    {
      "id": 1,
      "title": "DSPy: Compiling Language Model Calls into State-of-the-Art Retrievers",
      "authors": ["Omar Khattab", "Arnab Nandi", "Christopher Potts", "Matei Zaharia"],
      "year": 2023,
      "venue": "arXiv",
      "abstract": "Recent advances in language models have shifted the paradigm of information retrieval from traditional keyword-based systems to neural retrieval. However, composing language model calls into complex retrieval pipelines remains challenging. We introduce DSPy, a framework that treats language models as programmable components with learnable prompts and demonstrations. DSPy enables systematic optimization of LM-based retrieval systems through automated prompt engineering and example selection."
    },
    {
      "id": 2,
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
      "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus"],
      "year": 2020,
      "venue": "NeurIPS",
      "abstract": "Large pre-trained language models are powerful but suffer from several limitations including lack of current knowledge and hallucination. We propose RAG, which augments language models with dense passage retrieval. RAG effectively combines the knowledge stored in documents with the language generation capabilities of neural models, achieving state-of-the-art performance on multiple open-domain QA benchmarks."
    },
    {
      "id": 3,
      "title": "Attention Is All You Need",
      "authors": ["Ashish Vaswani", "Noam Shazeer", "Parmar Noam"],
      "year": 2017,
      "venue": "NeurIPS",
      "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely."
    },
    {
      "id": 4,
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans"],
      "year": 2022,
      "venue": "arXiv",
      "abstract": "We explore how to better leverage the few-shot prompting ability of large language models. We propose chain-of-thought prompting, which enables language models to decompose complex problems into intermediate reasoning steps. This approach dramatically improves performance on arithmetic, commonsense, and symbolic reasoning tasks."
    },
    {
      "id": 5,
      "title": "Dense Passage Retrieval for Open-Domain Question Answering",
      "authors": ["Vladimir Karpukhin", "Barlas Oguz", "Sewon Min"],
      "year": 2020,
      "venue": "EMNLP",
      "abstract": "We present Dense Passage Retrieval (DPR), a method for retrieving textual passages for open-domain question answering. DPR uses learnable dense vectors to represent both passages and questions, enabling fast and accurate retrieval. Our approach demonstrates that dense retrieval can substantially improve over sparse BM25 retrieval, achieving new state-of-the-art results on multiple QA benchmarks."
    }
  ]
}
