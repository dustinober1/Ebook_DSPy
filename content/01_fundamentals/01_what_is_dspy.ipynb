{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65fd4cd6",
   "metadata": {},
   "source": [
    "# What is DSPy?\n",
    "\n",
    "DSPy (Declarative Self-improving Language Programs, yeah!) is a framework for programming—not prompting—foundation models like GPT-4, Claude, and others. It provides a systematic way to build LM-based applications that are modular, composable, and automatically optimizable.\n",
    "\n",
    "## Historical Context: The Demonstrate-Search-Predict Paper\n",
    "\n",
    "DSPy originated from the groundbreaking research paper **\"Demonstrate-Search-Predict: A Paradigm for Solving Complex, Multi-Hop Reasoning Tasks with Large Language Models\"** by Omar Khattab and colleagues at Stanford University. This work established the foundational principles that would evolve into the DSPy framework.\n",
    "\n",
    "The paper demonstrated that complex reasoning tasks could be decomposed into three systematic stages:\n",
    "\n",
    "1. **DEMONSTRATE**: Learning from examples and demonstrations\n",
    "2. **SEARCH**: Retrieving and synthesizing information from multiple sources\n",
    "3. **PREDICT**: Generating accurate outputs based on gathered evidence\n",
    "\n",
    "This three-stage approach showed that by treating language model tasks as structured programs rather than mere prompts, we could achieve:\n",
    "- Better compositional generalization\n",
    "- More reliable multi-hop reasoning\n",
    "- Systematic optimization through weak supervision\n",
    "- Zero-shot transfer to new tasks\n",
    "\n",
    "The research proved that moving from ad-hoc prompt engineering to structured programming was the key to building reliable LM applications. DSPy is the production-ready implementation of these research insights, providing the tools and abstractions needed to build complex language model programs at scale.\n",
    "\n",
    "---\n",
    "\n",
    "## The Problem: Manual Prompt Engineering\n",
    "\n",
    "Before understanding DSPy, let's look at the traditional approach to working with LLMs.\n",
    "\n",
    "### Traditional Prompt Engineering\n",
    "\n",
    "When you want an LLM to perform a task, you typically write a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f75e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Manual prompt for question answering\n",
    "prompt = \"\"\"\n",
    "You are a knowledgeable assistant. Answer the following question accurately and concisely.\n",
    "\n",
    "Question: What is the capital of France?\n",
    "\n",
    "Provide your answer in a single sentence.\n",
    "\"\"\"\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d5b41",
   "metadata": {},
   "source": [
    "This works for simple cases, but scaling this approach leads to significant problems.\n",
    "\n",
    "---\n",
    "\n",
    "## Problems with Manual Prompting\n",
    "\n",
    "### 1. **Brittle and Hard to Maintain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05169df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for sentiment analysis\n",
    "sentiment_prompt = \"\"\"\n",
    "Analyze the sentiment of this text and classify it as positive, negative, or neutral.\n",
    "Be careful to consider context and sarcasm.\n",
    "Respond with only the sentiment label.\n",
    "\n",
    "Text: {text}\n",
    "Sentiment:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20092bce",
   "metadata": {},
   "source": [
    "**Issues**:\n",
    "- What if the model doesn't follow the \"only label\" instruction?\n",
    "- How do you handle edge cases consistently?\n",
    "- Changes require manual testing of the entire prompt\n",
    "\n",
    "### 2. **Doesn't Compose Well**\n",
    "\n",
    "Suppose you want to chain multiple steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c68725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Summarize\n",
    "summary_prompt = f\"Summarize this: {document}\"\n",
    "summary = call_llm(summary_prompt)\n",
    "\n",
    "# Step 2: Extract entities\n",
    "entity_prompt = f\"Extract entities from: {summary}\"\n",
    "entities = call_llm(entity_prompt)\n",
    "\n",
    "# Step 3: Classify\n",
    "classification_prompt = f\"Classify these entities: {entities}\"\n",
    "result = call_llm(classification_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f5dde",
   "metadata": {},
   "source": [
    "**Issues**:\n",
    "- Error propagation through the pipeline\n",
    "- No systematic way to optimize the entire flow\n",
    "- Debugging is a nightmare\n",
    "\n",
    "### 3. **No Systematic Optimization**\n",
    "\n",
    "How do you improve this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3fa1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = \"\"\"\n",
    "Answer the question based on the context.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587b995",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Manual approach**:\n",
    "- Try different phrasings\n",
    "- Add examples manually\n",
    "- Test each variation\n",
    "- No guarantee of improvement\n",
    "\n",
    "This is like trying to train a neural network by manually adjusting weights!\n",
    "\n",
    "---\n",
    "\n",
    "## The Solution: DSPy\n",
    "\n",
    "DSPy changes the game by letting you **program** with language models instead of **prompting** them.\n",
    "\n",
    "### Key Idea: Separate What from How\n",
    "\n",
    "Instead of telling the model *how* to solve a task (via prompts), you tell it *what* to do (via signatures), and DSPy figures out *how*.\n",
    "\n",
    "**Traditional prompting** (imperative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050df79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"You are an assistant. Answer questions. Question: {q}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580502a2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**DSPy** (declarative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab0fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions accurately.\"\"\"\n",
    "    question: str = dspy.InputField()\n",
    "    answer: str = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0d973",
   "metadata": {},
   "source": [
    "DSPy automatically creates the prompts for you!\n",
    "\n",
    "---\n",
    "\n",
    "## What DSPy Provides\n",
    "\n",
    "### 1. **Signatures**: Task Specifications\n",
    "\n",
    "Signatures define *what* a task does, not *how*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ad7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "class Summarize(dspy.Signature):\n",
    "    \"\"\"Summarize the given text.\"\"\"\n",
    "    document: str = dspy.InputField()\n",
    "    summary: str = dspy.OutputField(desc=\"concise summary in 2-3 sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c25ef7",
   "metadata": {},
   "source": [
    "This is like a type signature in programming—it specifies inputs and outputs.\n",
    "\n",
    "### 2. **Modules**: Composable Components\n",
    "\n",
    "Modules are reusable components that use signatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summarization module\n",
    "summarizer = dspy.Predict(Summarize)\n",
    "\n",
    "# Use it\n",
    "result = summarizer(document=\"Long text here...\")\n",
    "print(result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf6d25",
   "metadata": {},
   "source": [
    "Modules can be combined, extended, and optimized.\n",
    "\n",
    "### 3. **Optimizers**: Automatic Improvement\n",
    "\n",
    "This is where DSPy shines—you can automatically optimize your programs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c52b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your program\n",
    "class RAGPipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.retrieve = dspy.Retrieve(k=3)\n",
    "        self.answer = dspy.ChainOfThought(QuestionAnswer)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        return self.answer(context=context, question=question)\n",
    "\n",
    "# Optimize it automatically\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "optimizer = BootstrapFewShot(metric=your_metric)\n",
    "optimized_rag = optimizer.compile(RAGPipeline(), trainset=your_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67a3b1",
   "metadata": {},
   "source": [
    "DSPy learns better prompts, better examples, and better module compositions!\n",
    "\n",
    "---\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### Signatures\n",
    "\n",
    "Think of signatures as function declarations for LM tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b63739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input -> Output specification\n",
    "class TranslateToFrench(dspy.Signature):\n",
    "    english_text: str = dspy.InputField()\n",
    "    french_text: str = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b154a06",
   "metadata": {},
   "source": [
    "### Modules\n",
    "\n",
    "Pre-built and custom components:\n",
    "\n",
    "- **`dspy.Predict`**: Basic prediction\n",
    "- **`dspy.ChainOfThought`**: Step-by-step reasoning\n",
    "- **`dspy.ReAct`**: Agent-style reasoning with tools\n",
    "- **Custom**: Build your own!\n",
    "\n",
    "### Teleprompters (Optimizers)\n",
    "\n",
    "Automatically improve your program:\n",
    "\n",
    "- **`BootstrapFewShot`**: Generate few-shot examples\n",
    "- **`MIPRO`**: Optimize instructions and demonstrations\n",
    "- **`KNNFewShot`**: Use similarity-based examples\n",
    "\n",
    "---\n",
    "\n",
    "## A Simple Example\n",
    "\n",
    "Let's compare traditional prompting with DSPy:\n",
    "\n",
    "### Traditional Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf00b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def answer_question(question):\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. Answer this question accurately:\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Provide a clear, concise answer.\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Use it\n",
    "answer = answer_question(\"What is machine learning?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a23195",
   "metadata": {},
   "source": [
    "### DSPy Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "# Configure the language model\n",
    "lm = dspy.LM(model=\"openai/gpt-4\")\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Define the task\n",
    "class QuestionAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions accurately.\"\"\"\n",
    "    question: str = dspy.InputField()\n",
    "    answer: str = dspy.OutputField()\n",
    "\n",
    "# Create the module\n",
    "qa = dspy.Predict(QuestionAnswer)\n",
    "\n",
    "# Use it\n",
    "answer = qa(question=\"What is machine learning?\")\n",
    "print(answer.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14df497",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Benefits of the DSPy version**:\n",
    "- ✅ More modular and reusable\n",
    "- ✅ Can be composed with other modules\n",
    "- ✅ Can be automatically optimized\n",
    "- ✅ Prompts are generated automatically\n",
    "- ✅ Easier to maintain and test\n",
    "\n",
    "---\n",
    "\n",
    "## Why DSPy Matters\n",
    "\n",
    "### 1. **Systematic Development**\n",
    "\n",
    "DSPy brings software engineering practices to LM applications:\n",
    "- Modularity and composition\n",
    "- Abstraction and reusability\n",
    "- Systematic testing and optimization\n",
    "\n",
    "### 2. **Automatic Optimization**\n",
    "\n",
    "Instead of manually tweaking prompts:\n",
    "- DSPy learns from your data\n",
    "- Generates optimal prompts\n",
    "- Improves with more examples\n",
    "\n",
    "### 3. **Scalability**\n",
    "\n",
    "Build complex pipelines that:\n",
    "- Chain multiple steps\n",
    "- Handle errors gracefully\n",
    "- Scale to production\n",
    "\n",
    "### 4. **Research-Backed**\n",
    "\n",
    "DSPy is developed by Stanford NLP and backed by research:\n",
    "- Published at NeurIPS, NAACL, and other top venues\n",
    "- Proven effectiveness across tasks\n",
    "- Active research community\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Use Cases\n",
    "\n",
    "DSPy excels at:\n",
    "\n",
    "### Question Answering Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16411f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-based QA\n",
    "retriever = dspy.Retrieve(k=3)\n",
    "# Uses \"String Signature\" shorthand: \"input_fields -> output_fields\"\n",
    "qa = dspy.ChainOfThought(\"context, question -> answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5bf71b",
   "metadata": {},
   "source": [
    "> **Tip**: The string `\"context, question -> answer\"` is a shorthand for defining a Signature class. It's great for quick prototyping!\n",
    ">\n",
    "> **String Signature vs. Class Signature**:\n",
    "> - **String Signature** (`\"input -> output\"`): Best for quick experiments and simple tasks.\n",
    "> - **Class Signature** (`class MySig(dspy.Signature): ...`): Necessary for adding field descriptions, type hints, and stricter validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9ccdb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Multi-Step Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afcaaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex analysis pipelines\n",
    "class AnalysisPipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.extract = dspy.Predict(\"text -> entities\")\n",
    "        self.classify = dspy.ChainOfThought(\"entities -> category\")\n",
    "        self.summarize = dspy.Predict(\"entities, category -> summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907ae35",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Agents and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47dea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct-style agents\n",
    "agent = dspy.ReAct(\"question -> answer\", tools=[search, calculator])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f87ac2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## DSPy vs. Other Frameworks\n",
    "\n",
    "### vs. LangChain\n",
    "\n",
    "**LangChain**: Focuses on orchestration and integrations\n",
    "**DSPy**: Focuses on optimization and systematic improvement\n",
    "\n",
    "DSPy complements LangChain—you can use both together!\n",
    "\n",
    "### vs. Guidance/LMQL\n",
    "\n",
    "**Guidance/LMQL**: Template-based prompt control\n",
    "**DSPy**: Automatic prompt generation and optimization\n",
    "\n",
    "DSPy abstracts away the prompt engineering entirely.\n",
    "\n",
    "### vs. Direct API Calls\n",
    "\n",
    "**Direct APIs**: Maximum control, maximum effort\n",
    "**DSPy**: Abstraction with automatic optimization\n",
    "\n",
    "DSPy is higher-level but more powerful for complex tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use DSPy\n",
    "\n",
    "**DSPy is ideal when you**:\n",
    "- ✅ Build complex LM pipelines with multiple steps\n",
    "- ✅ Want to systematically improve performance\n",
    "- ✅ Need modularity and reusability\n",
    "- ✅ Have data for optimization\n",
    "- ✅ Value maintainability over quick hacks\n",
    "\n",
    "**Consider alternatives when you**:\n",
    "- ❌ Need a simple one-off query\n",
    "- ❌ Have zero data for optimization\n",
    "- ❌ Need very specific prompt control\n",
    "- ❌ Require guaranteed output formats (use Guidance/LMQL)\n",
    "\n",
    "---\n",
    "\n",
    "## The DSPy Philosophy\n",
    "\n",
    "### Programming > Prompting\n",
    "\n",
    "```\n",
    "Traditional:  Human writes prompt → LM executes → Human tweaks prompt → Repeat\n",
    "DSPy:         Human defines task → DSPy optimizes → LM executes → System improves\n",
    "```\n",
    "\n",
    "### Declarative > Imperative\n",
    "\n",
    "```\n",
    "Imperative:   \"Here's how to answer: First read the context, then...\"\n",
    "Declarative:  \"Given context and question, produce an answer\"\n",
    "```\n",
    "\n",
    "### Structured Reasoning > Flat Prompts\n",
    "\n",
    "The Demonstrate-Search-Predict paradigm gives us:\n",
    "```\n",
    "DEMONSTRATE: Learn from examples → Build task understanding\n",
    "SEARCH:      Retrieve evidence → Gather relevant information\n",
    "PREDICT:     Generate output → Produce final answer\n",
    "```\n",
    "\n",
    "### Optimizable > Static\n",
    "\n",
    "```\n",
    "Static:       Fixed prompts that require manual updates\n",
    "Optimizable:  Programs that improve automatically from data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**DSPy is**:\n",
    "- A framework for programming foundation models\n",
    "- Based on signatures (task specs) and modules (components)\n",
    "- Designed for composition and optimization\n",
    "- Research-backed and production-ready\n",
    "\n",
    "**DSPy lets you**:\n",
    "- Define *what* tasks do, not *how*\n",
    "- Build modular, composable pipelines\n",
    "- Automatically optimize from data\n",
    "- Scale to complex applications\n",
    "\n",
    "**Key Advantage**:\n",
    "Instead of manually engineering prompts, you program at a higher level and let DSPy handle the prompt optimization automatically.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand what DSPy is, let's dive deeper into the paradigm shift it represents.\n",
    "\n",
    "**Continue to**: [Programming vs. Prompting](02-programming-vs-prompting.md)\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **DSPy Paper**: [Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714)\n",
    "- **DSPy Website**: [https://dspy.ai](https://dspy.ai)\n",
    "- **DSPy GitHub**: [https://github.com/stanfordnlp/dspy](https://github.com/stanfordnlp/dspy)\n",
    "- **Blog Post**: [Intro to DSPy](https://dspy.ai/blog/)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
