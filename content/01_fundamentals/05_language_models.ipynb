{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d4b8e3",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "DSPy works with various language model providers. This section covers how to configure different LMs, choose the right model for your task, and follow best practices.\n",
    "\n",
    "---\n",
    "\n",
    "## Configuring Language Models\n",
    "\n",
    "### The Basics\n",
    "\n",
    "DSPy uses a consistent interface for all language models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "# Create an LM instance\n",
    "lm = dspy.LM(model=\"provider/model-name\", api_key=\"your-key\")\n",
    "\n",
    "# Set it as the default\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215538d0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Once configured, all DSPy modules will use this LM automatically.\n",
    "\n",
    "---\n",
    "\n",
    "## Supported Providers\n",
    "\n",
    "### OpenAI\n",
    "\n",
    "**Models available**:\n",
    "- `gpt-4o` - Latest flagship model\n",
    "- `gpt-4o-mini` - Fast, cost-effective\n",
    "- `gpt-4-turbo` - Previous flagship\n",
    "- `gpt-3.5-turbo` - Legacy, economical\n",
    "\n",
    "**Configuration**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "lm = dspy.LM(\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    api_key=\"sk-your-key-here\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88f7bc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Best for**: General-purpose tasks, proven reliability\n",
    "\n",
    "### Anthropic (Claude)\n",
    "\n",
    "**Models available**:\n",
    "- `claude-3-5-sonnet-20241022` - Latest, most capable\n",
    "- `claude-3-5-haiku-20241022` - Fast, economical\n",
    "- `claude-3-opus-20240229` - Maximum capability\n",
    "\n",
    "**Configuration**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca259a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    model=\"anthropic/claude-3-5-sonnet-20241022\",\n",
    "    api_key=\"your-anthropic-key\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    ")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3212ce78",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Best for**: Long contexts, detailed analysis, coding\n",
    "\n",
    "### Local Models (Ollama)\n",
    "\n",
    "**Models available**:\n",
    "- `llama3`, `llama3.1`, `llama3.2` - Meta's open models\n",
    "- `mistral`, `mixtral` - Mistral AI models\n",
    "- `phi3` - Microsoft's small model\n",
    "- Many others at [ollama.ai/library](https://ollama.ai/library)\n",
    "\n",
    "**Configuration**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No API key needed!\n",
    "lm = dspy.LM(\n",
    "    model=\"ollama/llama3\",\n",
    "    api_base=\"http://localhost:11434\"\n",
    ")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b91b1a9",
   "metadata": {},
   "source": [
    "**Best for**: Privacy, no API costs, experimentation\n",
    "\n",
    "### Other Providers\n",
    "\n",
    "DSPy also supports:\n",
    "- **Google (Gemini)**: `gemini/gemini-pro`\n",
    "- **Cohere**: `cohere/command`\n",
    "- **Together AI**: `together/model-name`\n",
    "- **Anyscale**: `anyscale/model-name`\n",
    "\n",
    "---\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "### Common Parameters\n",
    "\n",
    "All providers support these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    model=\"provider/model-name\",\n",
    "    api_key=\"your-key\",\n",
    "\n",
    "    # Randomness (0.0 = deterministic, 2.0 = very random)\n",
    "    temperature=0.7,\n",
    "\n",
    "    # Maximum response length\n",
    "    max_tokens=500,\n",
    "\n",
    "    # API endpoint (for local/custom servers)\n",
    "    api_base=\"http://localhost:11434\",\n",
    "\n",
    "    # Request timeout in seconds\n",
    "    timeout=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b584373",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Temperature Guide\n",
    "\n",
    "**Temperature** controls output randomness:\n",
    "\n",
    "| Value | Behavior | Use Case |\n",
    "|-------|----------|----------|\n",
    "| 0.0 - 0.3 | Deterministic, focused | Classification, extraction |\n",
    "| 0.4 - 0.8 | Balanced | General Q&A, summaries |\n",
    "| 0.9 - 1.5 | Creative, diverse | Creative writing, brainstorming |\n",
    "| 1.6 - 2.0 | Very random | Experimental, exploration |\n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d057457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For factual tasks - low temperature\n",
    "factual_lm = dspy.LM(model=\"openai/gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "# For creative tasks - higher temperature\n",
    "creative_lm = dspy.LM(model=\"openai/gpt-4o-mini\", temperature=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5234a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Using Multiple Models\n",
    "\n",
    "You can use different models for different tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "# Fast model for simple tasks\n",
    "fast_lm = dspy.LM(model=\"openai/gpt-4o-mini\")\n",
    "\n",
    "# Powerful model for complex tasks\n",
    "smart_lm = dspy.LM(model=\"openai/gpt-4o\")\n",
    "\n",
    "# Use specific models\n",
    "class Pipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        # Simple classification uses fast model\n",
    "        self.classify = dspy.Predict(\"text -> category\")\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Switch to fast model for this step\n",
    "        with dspy.context(lm=fast_lm):\n",
    "            category = self.classify(text=text).category\n",
    "\n",
    "        # Complex reasoning uses smart model\n",
    "        with dspy.context(lm=smart_lm):\n",
    "            # ... complex processing\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccdf967",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Selection Guide\n",
    "\n",
    "### By Task Type\n",
    "\n",
    "**Classification / Extraction**:\n",
    "- OpenAI: `gpt-4o-mini`\n",
    "- Anthropic: `claude-3-5-haiku-20241022`\n",
    "- Local: `llama3`\n",
    "\n",
    "**Question Answering**:\n",
    "- OpenAI: `gpt-4o-mini` or `gpt-4o`\n",
    "- Anthropic: `claude-3-5-sonnet-20241022`\n",
    "- Local: `llama3.1`\n",
    "\n",
    "**Complex Reasoning**:\n",
    "- OpenAI: `gpt-4o`\n",
    "- Anthropic: `claude-3-5-sonnet-20241022`\n",
    "- Local: `llama3.1:70b` (if you have GPU)\n",
    "\n",
    "**Long Context**:\n",
    "- Anthropic: `claude-3-5-sonnet-20241022` (200K context)\n",
    "- OpenAI: `gpt-4o` (128K context)\n",
    "\n",
    "**Code Generation**:\n",
    "- OpenAI: `gpt-4o`\n",
    "- Anthropic: `claude-3-5-sonnet-20241022`\n",
    "- Local: `codellama`\n",
    "\n",
    "### By Budget\n",
    "\n",
    "**Free / Low Cost**:\n",
    "- Local models via Ollama (free, requires GPU)\n",
    "- `gpt-4o-mini` (~$0.15 per 1M tokens)\n",
    "- `claude-3-5-haiku-20241022` (~$0.25 per 1M tokens)\n",
    "\n",
    "**Balanced**:\n",
    "- `gpt-4o` (~$2.50 per 1M tokens)\n",
    "- `claude-3-5-sonnet-20241022` (~$3 per 1M tokens)\n",
    "\n",
    "**Maximum Capability** (cost is higher):\n",
    "- `gpt-4o` (latest flagship)\n",
    "- `claude-3-opus-20240229` (~$15 per 1M tokens)\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### 1. Start Small, Scale Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc853b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development: Use small, fast models\n",
    "dev_lm = dspy.LM(model=\"openai/gpt-4o-mini\")\n",
    "\n",
    "# Production: Upgrade when needed\n",
    "prod_lm = dspy.LM(model=\"openai/gpt-4o\")\n",
    "\n",
    "# Easy to switch!\n",
    "lm = dev_lm if IS_DEVELOPMENT else prod_lm\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648eb8b",
   "metadata": {},
   "source": [
    "### 2. Use Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db48f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Never hardcode API keys!\n",
    "lm = dspy.LM(\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e381e6",
   "metadata": {},
   "source": [
    "### 3. Set Appropriate Timeouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default timeout might be too short for complex tasks\n",
    "lm = dspy.LM(\n",
    "    model=\"openai/gpt-4o\",\n",
    "    timeout=60  # 60 seconds for complex reasoning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce1ec3a",
   "metadata": {},
   "source": [
    "### 4. Cache Responses (Development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8146235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DSPy has built-in caching\n",
    "dspy.configure(lm=lm, cache=True)\n",
    "\n",
    "# Speeds up development, saves costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a12de",
   "metadata": {},
   "source": [
    "### 5. Handle Rate Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e5d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def call_with_retry(func, max_retries=3):\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            if \"rate limit\" in str(e).lower():\n",
    "                wait_time = (2 ** i) * 1  # Exponential backoff\n",
    "                print(f\"Rate limited. Waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                raise\n",
    "    raise Exception(\"Max retries exceeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a123f46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Configurations\n",
    "\n",
    "### For Learning/Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee00303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast and cheap\n",
    "lm = dspy.LM(\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=300,\n",
    "    cache=True  # Save $ during development\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83586607",
   "metadata": {},
   "source": [
    "### For Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reliable and capable\n",
    "lm = dspy.LM(\n",
    "    model=\"openai/gpt-4o\",\n",
    "    temperature=0.3,  # More deterministic\n",
    "    max_tokens=1000,\n",
    "    timeout=60,\n",
    "    cache=False  # Fresh responses\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa98fdd",
   "metadata": {},
   "source": [
    "### For Privacy-Sensitive Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5970f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local model, no data leaves your machine\n",
    "lm = dspy.LM(\n",
    "    model=\"ollama/llama3\",\n",
    "    api_base=\"http://localhost:11434\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a2f99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Switching Models\n",
    "\n",
    "### Method 1: Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ffe7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set globally\n",
    "dspy.configure(lm=dspy.LM(model=\"openai/gpt-4o-mini\"))\n",
    "\n",
    "# All modules use this model\n",
    "qa = dspy.Predict(\"question -> answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b23ac25",
   "metadata": {},
   "source": [
    "### Method 2: Context Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1dbbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model\n",
    "dspy.configure(lm=dspy.LM(model=\"openai/gpt-4o-mini\"))\n",
    "\n",
    "qa = dspy.Predict(\"question -> answer\")\n",
    "\n",
    "# Temporarily use a different model\n",
    "with dspy.context(lm=dspy.LM(model=\"openai/gpt-4o\")):\n",
    "    result = qa(question=\"Complex question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b5058b",
   "metadata": {},
   "source": [
    "### Method 3: Per-Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7358e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPipeline(dspy.Module):\n",
    "    def __init__(self):\n",
    "        # Each module can have its own LM\n",
    "        self.fast_step = dspy.Predict(\"input -> output\")\n",
    "        self.smart_step = dspy.ChainOfThought(\"input -> output\")\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        # Use fast model\n",
    "        with dspy.context(lm=fast_lm):\n",
    "            temp = self.fast_step(input=input_text).output\n",
    "\n",
    "        # Use smart model\n",
    "        with dspy.context(lm=smart_lm):\n",
    "            result = self.smart_step(input=temp).output\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a2118",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Issue: \"Rate limit exceeded\"\n",
    "\n",
    "**Solution**:\n",
    "1. Reduce request frequency\n",
    "2. Implement exponential backoff\n",
    "3. Upgrade your API plan\n",
    "4. Use a cheaper model for development\n",
    "\n",
    "### Issue: \"Connection timeout\"\n",
    "\n",
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c9747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase timeout\n",
    "lm = dspy.LM(model=\"openai/gpt-4o\", timeout=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edeea07",
   "metadata": {},
   "source": [
    "### Issue: \"Insufficient credits/quota\"\n",
    "\n",
    "**Solution**:\n",
    "1. Check your billing on the provider's dashboard\n",
    "2. Add payment method or increase limits\n",
    "3. Switch to a local model temporarily\n",
    "\n",
    "### Issue: Local model responses are poor quality\n",
    "\n",
    "**Solution**:\n",
    "1. Try a larger model (`llama3.1:70b` instead of `llama3`)\n",
    "2. Adjust temperature\n",
    "3. Provide more context in your signatures\n",
    "4. Consider using a commercial API for better quality\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Optimization Tips\n",
    "\n",
    "### 1. Use Appropriate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d569c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use gpt-4o for simple tasks!\n",
    "# Use gpt-4o-mini instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994027f0",
   "metadata": {},
   "source": [
    "### 2. Limit Token Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c292307",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    max_tokens=200  # Shorter responses = lower cost\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba507aaf",
   "metadata": {},
   "source": [
    "### 3. Cache During Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.configure(lm=lm, cache=True)\n",
    "# Repeated queries use cached results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65bd14",
   "metadata": {},
   "source": [
    "### 4. Batch Similar Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple items together when possible\n",
    "questions = [\"Q1\", \"Q2\", \"Q3\"]\n",
    "\n",
    "# Instead of 3 separate calls, batch them\n",
    "for q in questions:\n",
    "    # DSPy handles this efficiently\n",
    "    result = qa(question=q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484fc2bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Advanced: Custom LM Integration\n",
    "\n",
    "You can integrate any LM that follows the DSPy interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c50f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLM:\n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        # Your custom LM logic here\n",
    "        # Must return a string or list of strings\n",
    "        response = your_custom_model(prompt)\n",
    "        return response\n",
    "\n",
    "# Use it\n",
    "custom_lm = CustomLM()\n",
    "dspy.configure(lm=custom_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62657c54",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Key Concepts**:\n",
    "- DSPy supports multiple LM providers (OpenAI, Anthropic, local, etc.)\n",
    "- Configure once with `dspy.configure(lm=...)`\n",
    "- Use `dspy.context()` to temporarily switch models\n",
    "- Choose models based on task complexity and budget\n",
    "- Start with smaller models, scale up as needed\n",
    "\n",
    "**Best Practices**:\n",
    "- Use environment variables for API keys\n",
    "- Set appropriate timeouts and token limits\n",
    "- Enable caching during development\n",
    "- Choose the right model for each task\n",
    "- Handle rate limits gracefully\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand how to work with language models in DSPy, let's practice with some exercises!\n",
    "\n",
    "**Continue to**: [Exercises](06-exercises.md)\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(model=\"openai/gpt-4o-mini\", api_key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb5b0f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(model=\"anthropic/claude-3-5-sonnet-20241022\", api_key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b96761",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Ollama (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fc2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(model=\"ollama/llama3\", api_base=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4ff14",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Switch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ad548",
   "metadata": {},
   "outputs": [],
   "source": [
    "with dspy.context(lm=different_lm):\n",
    "    result = module(input=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd39f4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **OpenAI Models**: [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)\n",
    "- **Anthropic Models**: [https://docs.anthropic.com/claude/docs/models-overview](https://docs.anthropic.com/claude/docs/models-overview)\n",
    "- **Ollama**: [https://ollama.ai](https://ollama.ai)\n",
    "- **DSPy LM Docs**: [https://dspy.ai/api/language-models](https://dspy.ai/api/language-models)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
