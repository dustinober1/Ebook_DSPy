[{"title": "DSPy: The Comprehensive Guide", "url": "index.html", "content": "The definitive guide to programming‚Äîrather than prompting‚Äîlanguage models. Build robust, optimized, and self-improving AI applications. Start Reading What You Will Learn Start Here Introduction Preface, how to use this book, prerequisites, and complete setup instructions to get started. Read Chapter ‚Üí 01. Fundamentals Understand the core philosophy of DSPy and why \"programming\" beats \"prompting\" for complex LLM tasks. Read Chapter ‚Üí 02. Signatures Learn to define the input/output behavior of your AI modules structurally and type-safely. Read Chapter ‚Üí 03. Modules Discover the building blocks of DSPy programs, from simple Predictors to ChainOfThought and Agents. Read Chapter ‚Üí 04. Evaluation Master the art of measuring performance with custom metrics and systematic evaluation loops. Read Chapter ‚Üí 05. Optimizers Use compilers like MIPRO and BootstrapFewShot to automatically optimize your prompts and weights. Read Chapter ‚Üí 06. Real-World Apps Build RAG systems, classifiers, and intelligent agents ready for production deployment. Read Chapter ‚Üí 07. Advanced Topics Explore adapters, caching, async operations, debugging, and deployment strategies. Read Chapter ‚Üí 08. Case Studies Real-world case studies from enterprise RAG systems to AI code assistants and more. Read Chapter ‚Üí 09. Appendices API reference, troubleshooting guide, additional resources, glossary, and community links. Read Chapter ‚Üí"}, {"title": "Deployment Strategies", "url": "chapters/chapter-07/deployment-strategies.html", "content": "Chapter 7 ¬∑ Section 6 Deployment Strategies Productionize your DSPy applications with Docker, FastAPI, Kubernetes, and robust monitoring. ~30 min read Introduction Deploying AI applications requires handling heavy compute loads, managing expensive API calls, and ensuring high availability. We'll cover containerization and orchestration. Deployment Architecture A typical production stack includes an API Gateway, Application Layer (FastAPI), Service Layer (Redis/Queue), and Data Layer. Containerization with Docker Package your DSPy app for consistent deployment: Dockerfile Copy FROM python:3.11-slim WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] Serving with FastAPI Expose your DSPy module as a REST API: Python Copy from fastapi import FastAPI import dspy app = FastAPI() lm = dspy.LM(model=\"gpt-3.5-turbo\") dspy.settings.configure(lm=lm) rag = ProductionRAG() @app.post(\"/query\") async def query_endpoint(q: str): return rag.forward(question=q) Continue to Self-Refining Pipelines"}, {"title": "Caching & Performance", "url": "chapters/chapter-07/caching-performance.html", "content": "Chapter 7 ¬∑ Section 3 Caching & Performance Optimizing DSPy applications for speed, cost, and reliability through intelligent caching and batching. ~20 min read Introduction Language model calls are expensive. Optimizing your DSPy pipeline is essential for production viability. This guide covers caching strategies, batch processing, and performance monitoring. Caching Strategies 1. Result Caching The simplest form: cache inputs to outputs. Python Copy class ResultCache: def get(self, module, args): key = self._generate_key(module, args) return self.backend.get(key) 2. Semantic Caching Using embeddings to find similar queries and return cached results, increasing hit rates. Python Copy class SemanticCache: def get(self, query): embedding = self.model.encode(query) # Find cosine similarity > threshold... pass Batching & Bulk Processing Processing requests in parallel drastically improves throughput. Python Copy class BatchProcessor: def process_batch(self, items, func): with ThreadPoolExecutor() as executor: return list(executor.map(func, items)) Performance Monitoring You can't optimize what you don't measure. Use a dashboard to track latency and costs. Python Copy class PerformanceDashboard: def record_metrics(self, latency, tokens): self.metrics['latency'].append(latency) self.metrics['cost'].append(calculate_cost(tokens)) Continue to Async & Streaming"}, {"title": "Declarative Compilation", "url": "chapters/chapter-07/declarative-compilation.html", "content": "Chapter 7 ¬∑ Section 8 Declarative Compilation Explore the theory and practice of compiling high-level specifications into optimized DSPy programs. ~30 min read Introduction Declarative compilation separates what needs to be done from how it is done. DSPy compilers automate the optimization of prompts and pipeline structures based on your high-level intent. Specification Analysis The compiler first analyzes your signatures to understand input types, output constraints, and reasoning complexity. Python Copy class SpecificationAnalyzer: def analyze(self, signature): complexity = self.assess_complexity(signature) constraints = self.extract_constraints(signature) return { \"complexity\": complexity, \"constraints\": constraints, \"strategy\": self.recommend_strategy(complexity) } Strategy Selection Based on analysis, the compiler selects the best modules (e.g., Predict vs. ChainOfThought) and optimization strategies (e.g., BootstrapFewShot vs. MIPRO). Python Copy class StrategySelector: def select_modules(self, analysis): if analysis['complexity'] > 0.7: return [ChainOfThought, SelfRefine] else: return [Predict] Meta-Compilation Advanced systems can learn from previous compilations to improve future performance, effectively \"learning to compile\". Continue to Exercises"}, {"title": "Async & Streaming", "url": "chapters/chapter-07/async-streaming.html", "content": "Chapter 7 ¬∑ Section 4 Async & Streaming Build responsive, high-throughput DSPy applications with asynchronous processing and real-time data streaming. ~20 min read Introduction Real-time applications demand non-blocking operations. DSPy supports async patterns to handle multiple concurrent requests and stream data efficiently. Async DSPy Patterns Enable concurrent execution by wrapping synchronous calls or using async-native libraries. Python Copy import asyncio from concurrent.futures import ThreadPoolExecutor class AsyncDSPyModule(dspy.Module): def __init__(self): super().__init__() self.executor = ThreadPoolExecutor() async def aforward(self, *args, **kwargs): loop = asyncio.get_event_loop() return await loop.run_in_executor( self.executor, self.forward, *args, **kwargs ) Streaming Data Processing Handle continuous data streams without blocking the main event loop. Python Copy class StreamProcessor: async def process_stream(self, stream): async for item in stream: result = await self.process_item(item) yield result Real-time Chat Bot with WebSocket Integrating DSPy with WebSockets for instant bidirectional communication. Python Copy class RealTimeChatBot(AsyncDSPyModule): async def process_message(self, message): response = await self.aforward(message) # Send response via WebSocket... pass Continue to Debugging & Tracing"}, {"title": "Adapters & Tools", "url": "chapters/chapter-07/adapters-tools.html", "content": "Chapter 7 ¬∑ Section 2 Adapters & Tools Bridge DSPy with databases, APIs, file systems, and specialized tools. ~20 min read Introduction Adapters make DSPy modular and extensive. They handle communication with external resources like databases, APIs, and file systems, ensuring your core logic remains clean. Built-in vs. Custom Adapters Database Adapter (Example) A typical PostgreSQL adapter pattern: Python Copy class PostgreSQLAdapter(dspy.Adapter): def __init__(self, connection_string): super().__init__() self.connection_string = connection_string def query(self, sql, params=None): # Implementation of query execution... pass Creating Custom Adapters File System Adapter Manage file operations seamlessly within your DSPy pipeline: Python Copy import dspy from pathlib import Path class FileSystemAdapter(dspy.Adapter): def __init__(self, base_path=\".\"): super().__init__() self.base_path = Path(base_path) def read_file(self, filename): return (self.base_path / filename).read_text() def write_file(self, filename, content): (self.base_path / filename).write_text(content) API Adapter Interact with external REST APIs: Python Copy import requests class APIAdapter(dspy.Adapter): def __init__(self, base_url): super().__init__() self.base_url = base_url def get(self, endpoint): return requests.get(f\"{self.base_url}/{endpoint}\").json() Specialized Tools Tools encapsulate specific functionality like calculations or text processing. Calculator Tool Python Copy class CalculatorTool(dspy.Tool): def calculate(self, expression): # Safe evaluation logic... pass Integration Example: Tool-Enabled Agent Combining adapters and tools in a powerful agent: Python Copy class ToolEnabledAgent(dspy.Module): def __init__(self): super().__init__() self.tools = { 'calculator': CalculatorTool(), 'file_system': FileSystemAdapter() } self.decide = dspy.Predict(\"task -> tool, params\") def forward(self, task): decision = self.decide(task=task) # Execute selected tool... pass Continue to Caching & Performance"}, {"title": "Advanced Topics", "url": "chapters/chapter-07/index.html", "content": "Chapter 7 Advanced Topics Master sophisticated conceptual patterns that separate basic implementations from production-ready systems. ~2.5 hours read Overview Welcome to Chapter 7 where we dive deep into advanced DSPy concepts that will transform you from a DSPy practitioner into a DSPy expert. This chapter covers the sophisticated techniques and patterns that separate basic implementations from production-ready, scalable systems. Learning Objectives Adapters and Tools: Extending DSPy with custom components and integrations. Caching and Performance: Building high-performance, responsive applications. Async and Streaming: Handling real-time data and concurrent operations. Debugging and Tracing: Mastering DSPy's debugging capabilities. Deployment Strategies: Taking your DSPy applications to production. Advanced Patterns: Self-refining pipelines and declarative compilation. Chapter Roadmap 01 Adapters & Tools Custom integrations and components. 02 Caching & Performance Optimizing for speed and efficiency. 03 Async & Streaming Real-time data and concurrency. 04 Debugging & Tracing Advanced troubleshooting techniques. 05 Deployment Production strategies and patterns. The DSPy Advanced Ecosystem DSPy's advanced ecosystem allows for powerful customizations: Python Copy # Advanced configuration with caching, tracing, and monitoring import dspy dspy.settings.configure( lm=dspy.LM(model=\"gpt-4\", api_key=\"your-key\"), cache=dspy.Cache(redis_url=\"redis://localhost:6379\"), tracing=dspy.Tracing(enabled=True), performance_monitoring=True ) Start Chapter: Adapters & Tools"}, {"title": "Chapter 7 Exercises", "url": "chapters/chapter-07/exercises.html", "content": "Chapter 7 ¬∑ Section 9 Exercises Advanced practice exercises for mastering DSPy internals, performance, and deployment. ~45 min practice Introduction These exercises challenge you to apply advanced DSPy concepts to solve complex, real-world problems. You'll work with adapters, performance optimization, async programming, debugging, and deployment strategies. Exercise 1: Build a Custom Database Adapter Objective: Create a persistent database adapter for DSPy that caches predictions in PostgreSQL. Python Copy class DatabaseAdapter(dspy.Adapter): \"\"\"Implement a database adapter that caches results.\"\"\" def __init__(self, connection_string): super().__init__() # TODO: Initialize connection pass def get(self, key): # TODO: Retrieve from DB pass def set(self, key, value): # TODO: Store in DB pass Exercise 2: High-Performance Caching System Objective: Implement a multi-level cache (L1 Memory, L2 Redis, L3 Disk) with smart eviction. Python Copy class MultiLevelCache: \"\"\"Implement L1/L2/L3 caching strategy.\"\"\" def get(self, key): # TODO: check memory -> redis -> disk pass Exercise 3: Async Streaming RAG Objective: Build a system that can process concurrent document streams and queries asynchronously. Hint: Use asyncio.Queue and aiohttp. Exercise 5: Kubernetes Deployment Objective: Create a full Kubernetes manifest (Deployment, Service, HPA) for a DSPy application. YAML Copy apiVersion: apps/v1 kind: Deployment metadata: name: dspy-app # TODO: Complete the manifest Continue to Solutions"}, {"title": "Self-Refining Pipelines", "url": "chapters/chapter-07/self-refining-pipelines.html", "content": "Chapter 7 ¬∑ Section 7 Self-Refining Pipelines Build autonomous systems that evaluate and improve their own outputs through iterative refinement. ~25 min read Introduction Self-refining pipelines automatically evaluate their own outputs and trigger refinement steps if quality criteria are not met. This is a powerful pattern for ensuring high-quality results. The Refinement Loop The core concept involves generating an initial result, evaluating it, and then iterating until a threshold is met or max iterations are reached. Python Copy class SelfRefiningModule(dspy.Module): def forward(self, x): output = self.generate(x) for _ in range(self.max_iterations): score = self.evaluate(output) if score > self.threshold: break output = self.refine(output, score) return output Refinement Strategies Strategies include: Incremental Refinement: Making small, targeted fixes. Rewrite: Completely regenerating the output based on feedback. Ensemble: Combining multiple different refinement approaches. Example: Code Refiner A self-refining system for code generation might check for syntax errors and style violations, then automatically fix them. Python Copy class CodeRefiner(dspy.Module): def forward(self, spec): code = self.generator(spec).code for _ in range(3): issues = self.analyzer(code) if not issues: break code = self.fixer(code, issues) return code Continue to Declarative Compilation"}, {"title": "Solutions", "url": "chapters/chapter-07/solutions.html", "content": "Chapter 7 ¬∑ Section 10 Solutions Complete reference implementations for the advanced topics exercises, including database adapters, caching, and deployment. Reference Material Introduction This section provides detailed solutions for the exercises in Chapter 7. These implementations demonstrate best practices for building robust, scalable DSPy applications in production environments. Solution 1: Custom PostgreSQL Adapter This adapter implements connection pooling and caching using psycopg2. Python Copy import dspy import psycopg2 from psycopg2 import pool import json class PostgresAdapter(dspy.Adapter): def __init__(self, connection_string): self.pool = psycopg2.pool.SimpleConnectionPool(1, 10, connection_string) self._init_db() def _init_db(self): conn = self.pool.getconn() try: with conn.cursor() as cur: cur.execute(\"\"\" CREATE TABLE IF NOT EXISTS predictions ( key TEXT PRIMARY KEY, value JSONB ); \"\"\") conn.commit() finally: self.pool.putconn(conn) def get(self, key): conn = self.pool.getconn() try: with conn.cursor() as cur: cur.execute(\"SELECT value FROM predictions WHERE key = %s\", (key,)) result = cur.fetchone() return result[0] if result else None finally: self.pool.putconn(conn) def set(self, key, value): conn = self.pool.getconn() try: with conn.cursor() as cur: cur.execute(\"\"\" INSERT INTO predictions (key, value) VALUES (%s, %s) ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value \"\"\", (key, json.dumps(value))) conn.commit() finally: self.pool.putconn(conn) Solution 2: Multi-Level Caching System A sophisticated caching strategy managing Memory (L1), Redis (L2), and Disk (L3). Python Copy import redis import pickle import os class MultiLevelCache: def __init__(self, redis_url=\"redis://localhost:6379\", disk_path=\"./cache\"): self.l1 = {} self.l2 = redis.from_url(redis_url) self.l3_path = disk_path os.makedirs(disk_path, exist_ok=True) def get(self, key): # Check L1 (Memory) if key in self.l1: return self.l1[key] # Check L2 (Redis) l2_val = self.l2.get(key) if l2_val: val = pickle.loads(l2_val) self.l1[key] = val # Promote to L1 return val # Check L3 (Disk) file_path = os.path.join(self.l3_path, key) if os.path.exists(file_path): with open(file_path, 'rb') as f: val = pickle.load(f) self.l2.set(key, pickle.dumps(val)) # Promote to L2 self.l1[key] = val # Promote to L1 return val return None def set(self, key, value): # Write through to all layers self.l1[key] = value self.l2.set(key, pickle.dumps(value)) with open(os.path.join(self.l3_path, key), 'wb') as f: pickle.dump(value, f) Solution 3: Async Streaming RAG Handling concurrent streams efficiently using asyncio. Python Copy import asyncio import dspy class AsyncRAGController: def __init__(self, rag_module, concurrency=5): self.module = rag_module self.semaphore = asyncio.Semaphore(concurrency) async def process_query(self, query): async with self.semaphore: # Simulate async wrapper for sync DSPy call result = await asyncio.to_thread(self.module.forward, question=query) return result async def ingest_stream(self, document_stream): async for doc in document_stream: # Fake async ingestion await asyncio.sleep(0.01) print(f\"Ingested: {doc[:20]}...\") async def run_pipeline(self, queries, doc_stream): ingest_task = asyncio.create_task(self.ingest_stream(doc_stream)) results = await asyncio.gather(*(self.process_query(q) for q in queries)) await ingest_task return results Solution 5: Kubernetes Manifest Complete production deployment configuration. YAML Copy apiVersion: apps/v1 kind: Deployment metadata: name: dspy-app namespace: adaptive-ai spec: replicas: 3 selector: matchLabels: app: dspy-app template: metadata: labels: app: dspy-app spec: containers: - name: dspy-server image: dspy-app:v1.0 ports: - containerPort: 8000 resources: requests: memory: \"512Mi\" cpu: \"500m\" limits: memory: \"1Gi\" cpu: \"1000m\" env: - name: OPENAI_API_KEY valueFrom: secretKeyRef: name: dspy-secrets key: api-key --- apiVersion: v1 kind: Service metadata: name: dspy-service spec: selector: app: dspy-app ports: - protocol: TCP port: 80 targetPort: 8000 type: LoadBalancer --- apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: dspy-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: dspy-app minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 75 Continue to Chapter 8"}, {"title": "Debugging & Tracing", "url": "chapters/chapter-07/debugging-tracing.html", "content": "Chapter 7 ¬∑ Section 5 Debugging & Tracing Strategies and tools for inspecting, troubleshooting, and profiling DSPy applications. ~25 min read Introduction Debugging AI applications is tricky due to non-deterministic outputs. This section covers how to inspect module state, trace execution flow, and monitor performance. Common Debugging Challenges Non-deterministic Outputs: The same input might yield different results. Hidden Complexity: Logic within optimizers and LMs is often opaque. Token Usage: Costs can spiral without careful monitoring. Building a DSPy Debugger A custom debugger class can help log and inspect execution: Python Copy class DSPyDebugger: def trace(self, module, args, result): log_entry = { \"module\": module, \"args\": args, \"result\": result } self.history.append(log_entry) self._print(log_entry) Function Tracing with Decorators Automatically trace forward passes in your modules: Python Copy def trace_function(func): def wrapper(*args, **kwargs): print(f\"Executing {func.__name__}...\") result = func(*args, **kwargs) print(f\"Result: {result}\") return result return wrapper class TracedModule(dspy.Module): @trace_function def forward(self, x): return self.predict(input=x) Token Usage & Cost Tracking Monitor costs by tracking token usage: Python Copy import tiktoken class TokenTracker: def count_tokens(self, text): return len(self.encoding.encode(text)) def estimate_cost(self): # Calculate based on current pricing... pass Continue to Deployment Strategies"}, {"title": "Evaluation Loops", "url": "chapters/chapter-04/evaluation-loops.html", "content": "Chapter 4 ¬∑ Section 5 Evaluation Loops Systematically assess module performance with parallel execution and detailed reporting. ~30 min read The Evaluate Class DSPy's Evaluate class is the workhorse for systematic testing. It automates the process of running your dataset through your module and applying metrics. Basic Usage import dspy # Setup evaluate = dspy.Evaluate( devset=devset, # Data to test metric=accuracy, # Metric function num_threads=8, # Parallel threads display_progress=True # Show progress bar ) # Run score = evaluate(module) print(f\"Accuracy: {score}%\") Configuration Parameters Parameter Description Default devset List of Examples to evaluate on Required metric Function to score predictions Required num_threads Number of threads for parallel execution 1 display_progress Show progress bar in terminal False display_table Print table of N results (or True for all) False Parallel Evaluation Running evaluations sequentially can be slow. Use num_threads to speed up the process significantly. üí° Recommendation: Use 4-8 threads for standard API keys. Higher values might hit rate limits. # Speed up evaluation 8x evaluate = dspy.Evaluate( devset=large_devset, metric=metric, num_threads=8 ) Analyzing Results To get more than just a single score, use return_all_scores=True or return_outputs=True. # Get detailed results evaluate = dspy.Evaluate( devset=devset, metric=metric, return_outputs=True ) result = evaluate(module) for x, pred, score in result.results: print(f\"Q: {x.question}\") print(f\"Pred: {pred.answer}\") print(f\"Score: {score}\") print(\"---\") Evaluation Workflows 1. Development Loop Quick check on a small subset during active coding. # Check 5 examples quickly evaluate(module, devset=devset[:5]) 2. Pre-Commit Check Ensure no regression before committing code. score = evaluate(module) assert score > 85.0, \"Quality dropped below threshold!\" 3. MLflow Integration Track experiments over time. import mlflow with mlflow.start_run(): score = evaluate(module) mlflow.log_metric(\"accuracy\", score) Next: Best Practices"}, {"title": "Human-Aligned Evaluation", "url": "chapters/chapter-04/human-aligned-evaluation.html", "content": "Chapter 4 Human-Aligned Evaluation Creating evaluation systems that reflect actual human priorities and quality requirements. Overview Traditional evaluation metrics like BERTScore, ROUGE, and BLEU often fail to capture what truly matters to human users, especially in complex, nuanced tasks. Human-aligned evaluation focuses on creating evaluation systems that reflect actual human priorities and domain-specific quality requirements. This section explores how to bridge the gap between automated metrics and human judgment, drawing on techniques like weighted dimensions and granular feedback. The Limitations of Standard Metrics Why Off-the-Shelf Metrics Fail # Example from clinical summarization standard_metrics = { \"bert_score\": 87.19, # High semantic similarity \"rouge_2\": 0.82, # Good n-gram overlap # But missed critical clinical details! } # Human evaluation revealed: # - Omitted key diagnoses # - Missing treatment outcomes # - Incomplete patient history Key Problems: Context-blind: Metrics don't understand task-specific requirements Surface-level: Focus on lexical overlap, not meaningful content Poor correlation: Often weak correlation with actual human judgment Building Human-Aligned Evaluation Systems 1. Understand Your Quality Dimensions class ClinicalQualityDimensions: \"\"\"Quality dimensions for clinical summarization.\"\"\" FACTUAL_ACCURACY = \"Is all information correct?\" CLINICAL_COMPLETENESS = \"Are all critical findings included?\" CONCISENESS = \"Is it appropriately brief?\" CLINICAL_RELEVANCE = \"Is information clinically significant?\" @classmethod def get_weights(cls, context=\"emergency\"): # Different weights for different contexts if context == \"emergency\": return { cls.FACTUAL_ACCURACY: 0.5, cls.CLINICAL_COMPLETENESS: 0.3, # ... } 2. Human-Aligned LLM Judge class HumanAlignedLLMJudge(dspy.Module): \"\"\"LLM judge trained on human feedback patterns.\"\"\" def __init__(self, quality_dimensions, weights=None): super().__init__() self.dimensions = quality_dimensions self.weights = weights # Create evaluation signature explaining dimensions detailedly self.evaluation_signature = dspy.Signature( # ... prompt definition ... ) Case Study Results Metric Before Optimization After Optimization Improvement BERTScore 87.19 87.27 +0.08 LLM Judge 53.90 68.07 +26.3% Human Alignment (œÅ) 0.14 0.28 2x improvement Best Practices Start Clear: Define clear, actionable quality criteria (e.g., rubrics with specific levels). Separate Training/Val: Ensure strict splits to prevent data leakage and overfitting to specific examples. Version Control: Track your evaluation prompts and weights as they evolve. Continue to Solutions"}, {"title": "Why Evaluation Matters", "url": "chapters/chapter-04/why-evaluation-matters.html", "content": "Chapter 4 ¬∑ Section 2 Why Evaluation Matters Understanding the necessity of systematic evaluation in LLM development. ~20 min read When building applications with language models, you face a fundamental challenge: LLM outputs are non-deterministic and difficult to verify. Unlike traditional software where you can test exact outputs, LLM responses vary and require nuanced assessment. The Problem Without Evaluation Without systematic evaluation, you cannot answer critical questions about reliability, hallucinations, or edge cases. import dspy # Build a question-answering system qa = dspy.Predict(\"question -> answer\") # Test it once result = qa(question=\"What causes rain?\") print(result.answer) # \"Rain is caused by water vapor condensing in clouds...\" # Looks good! But is it reliable? # - Does it work for all types of questions? # - How often does it produce incorrect answers? # - Does it hallucinate facts? # - Will it work in production? The Solution: Systematic Evaluation By defining metrics and using datasets, you can quantify performance. import dspy # Define what \"correct\" means def is_correct(example, pred, trace=None): # Check if the answer matches expected output return example.expected_answer.lower() in pred.answer.lower() # Create a test dataset devset = [ dspy.Example(question=\"What causes rain?\", expected_answer=\"condensation\").with_inputs(\"question\"), dspy.Example(question=\"What is photosynthesis?\", expected_answer=\"plants convert sunlight\").with_inputs(\"question\"), # ... more examples ] # Evaluate systematically evaluate = dspy.Evaluate(devset=devset, metric=is_correct, num_threads=8) score = evaluate(qa) print(f\"Accuracy: {score}%\") # Now you know exactly how good it is! Evaluation Enables Optimization In DSPy, evaluation is the foundation of automatic optimization. 1. DATASET Examples with inputs & expected outputs 2. METRIC Function that scores each prediction 3. OPTIMIZER Uses metric scores to improve prompts Types of Evaluation 1. Development Evaluation Fast feedback during development with small datasets (10-50 examples). 2. Validation Evaluation Used to tune hyperparameters and compare approaches (100-500 examples). 3. Test Evaluation Final, unbiased performance estimate on held-out data (500+ examples). 4. Production Evaluation Continuous monitoring of deployed systems to detect drift. Common Evaluation Pitfalls ‚ö†Ô∏è Testing on Training Data: Never evaluate on the same data used to optimize/compile your module. This leads to artificially high scores and poor real-world performance. ‚ö†Ô∏è Non-Representative Data: Ensure your test data reflects the complexity and variety of real user queries. ‚ö†Ô∏è Overfitting to Metrics: Avoid metrics that can be \"gamed\" (e.g., just checking for keywords) without ensuring actual quality. Next: Creating Datasets"}, {"title": "Creating Datasets", "url": "chapters/chapter-04/creating-datasets.html", "content": "Chapter 4 ¬∑ Section 3 Creating Datasets Structure, load, and manage datasets using the DSPy Example class. ~30 min read The Example Class DSPy uses the Example class to represent individual data points for training and evaluation. Basic Example Creation import dspy # Create a simple example example = dspy.Example( question=\"What is the capital of France?\", answer=\"Paris\" ) # Access fields print(example.question) # \"What is the capital of France?\" print(example.answer) # \"Paris\" The with_inputs() Method The with_inputs() method is critical‚Äîit tells DSPy which fields are inputs vs. expected outputs: import dspy # Create example and mark which fields are inputs example = dspy.Example( question=\"What is the capital of France?\", answer=\"Paris\" ).with_inputs(\"question\") # Now DSPy knows: # - \"question\" is an INPUT (given to the module) # - \"answer\" is an OUTPUT (expected result for evaluation) # Access input fields print(example.inputs()) # {\"question\": \"What is the capital of France?\"} üí° Tip: Always use with_inputs() immediately after creating an Example. Without it, DSPy optimizers and evaluators won't know how to use your data. Loading Datasets From Python Dictionaries raw_data = [ {\"q\": \"What is 2+2?\", \"a\": \"4\"}, {\"q\": \"What is 3*3?\", \"a\": \"9\"}, ] # Convert to DSPy Examples dataset = [ dspy.Example(question=item[\"q\"], answer=item[\"a\"]).with_inputs(\"question\") for item in raw_data ] From Hugging Face from dspy.datasets import DataLoader # Load from Hugging Face Hub loader = DataLoader() raw_data = loader.from_huggingface( dataset_name=\"squad\", split=\"train\", fields=(\"question\", \"context\", \"answers\"), input_keys=(\"question\", \"context\") ) Train/Dev/Test Splits Proper data splitting is essential for valid evaluation. Split Purpose Usage Training Optimize prompts/demonstrations Used by optimizer Development Tune hyperparameters, iterate Used during development Test Final unbiased evaluation Used once at the end import random # Shuffle with fixed seed for reproducibility random.Random(42).shuffle(data) # Split into sets trainset = data[:200] # 200 for training devset = data[200:500] # 300 for development testset = data[500:1000] # 500 for testing Data Quality Checklist ‚úÖ Check Required Fields Ensure every example has the necessary input and output fields. ‚úÖ Remove Duplicates Clean your dataset to prevent data leakage and bias. ‚úÖ Verify Inputs Marked Double-check that with_inputs() has been called on every example. Next: Defining Metrics"}, {"title": "Defining Metrics", "url": "chapters/chapter-04/defining-metrics.html", "content": "Chapter 4 ¬∑ Section 4 Defining Metrics Master the art of measuring LLM performance with custom and built-in metrics. ~35 min read Metric Function Anatomy A DSPy metric is simply a Python function that evaluates prediction quality by comparing it to the ground truth. def metric(example, pred, trace=None): \"\"\" Evaluate prediction quality. Args: example: The original Example with inputs AND expected outputs pred: The Prediction (module output) to evaluate trace: Optional trace info (used during optimization) Returns: bool or float: Score indicating quality (True/False or 0.0-1.0) \"\"\" # Compare prediction to expected output return pred.answer == example.answer The Three Parameters 1. example (Ground Truth) Contains both the inputs sent to the model and the expected output labels. 2. pred (Prediction) The actual output generated by your DSPy module. 3. trace (Context) Indicates if the metric is running during optimization (filtering) or evaluation (scoring). Built-in Metrics DSPy provides several ready-to-use metrics for common tasks. Semantic F1 Measures semantic overlap between answers using a language model. from dspy.evaluate import SemanticF1 # Initialize metric = SemanticF1(decompositional=True) # Use score = metric(example, pred) Answer Correctness def answer_correctness(example, pred, trace=None): \"\"\"Check if predicted answer contains the correct answer.\"\"\" correct = example.answer.lower() predicted = pred.answer.lower() return correct in predicted or predicted in correct Creating Custom Metrics Simple Boolean Metrics def sentiment_accuracy(example, pred, trace=None): \"\"\"Check if sentiment prediction matches ground truth.\"\"\" return example.sentiment == pred.sentiment Composite Metrics Combine multiple quality factors into a single score. def comprehensive_qa_metric(example, pred, trace=None): # 1. Correctness correct = example.answer.lower() in pred.answer.lower() # 2. Completeness (length heuristic) complete = len(pred.answer) > 20 # 3. No uncertainty certain = \"I don't know\" not in pred.answer # Scoring logic if not correct: return 0.0 if not complete: return 0.5 if not certain: return 0.7 return 1.0 The Trace Parameter Deep Dive The trace parameter is what enables DSPy's powerful optimization. Mode trace value Goal Return Type Optimization not None (Object) Select best demos bool (True/False) Evaluation None Measure performance float (Score) def smart_metric(example, pred, trace=None): score = calculate_score(example, pred) if trace is not None: # Optimization: Be strict! Only accept perfect examples return score >= 0.9 # Evaluation: Return the actual score return score Specialized Metrics for Long-form Content Evaluating long responses (like articles) requires more sophisticated metrics. Topic Coverage: Uses ROUGE scores to check if key concepts are covered. FactScore: Breaks text into atomic claims and verifies each against a knowledge source. Verifiability: Checks if claims are supported by citations. Next: Evaluation Loops"}, {"title": "Structured Prompting", "url": "chapters/chapter-04/structured-prompting.html", "content": "Chapter 4 Structured Prompting A systematic methodology for creating robust evaluation prompts. Overview Structured Prompting is a systematic methodology for creating evaluation prompts that ensures consistency, reliability, and robustness in language model assessment. Introduced in late 2024, this approach addresses the variability and inconsistency issues that plague ad-hoc prompt engineering in evaluation scenarios. The key innovation is the formalization of prompt creation into a structured process that: Standardizes prompt components Ensures comprehensive coverage of evaluation aspects Reduces ambiguity in task instructions Enables reproducible evaluation across different models and settings Why Structured Prompting Matters Problems with Ad-Hoc Prompting Traditional ad-hoc prompting suffers from several issues: Inconsistency: Different evaluators create wildly different prompts Ambiguity: Unclear instructions lead to model confusion Coverage Gaps: Important aspects of the task may be omitted Reproducibility: Difficult to replicate results across setups Bias: Unconscious biases in prompt formulation Benefits of Structured Prompting # Ad-hoc approach (problematic) ad_hoc_prompt = \"Tell me about the medical risks in this trial.\" # Structured approach (robust) structured_prompt = \"\"\" Task: Risk Assessment Evaluation Context: You are evaluating a medical research paper for potential risks. Please analyze the following randomized controlled trial (RCT). Instructions: 1. Identify all potential risks mentioned 2. Categorize risks by severity (mild/moderate/severe) 3. Note the frequency of each risk 4. Assess if risks are adequately addressed 5. Provide a confidence score for your assessment Format your response as: - Risk Category: [Name] - Frequency - Severity - Overall Assessment: [Summary] - Confidence Score: [0-1] Trial Text: {trial_text} \"\"\" The Structured Prompting Framework Core Components A structured prompt consists of five essential components: Task Definition: Clear specification of what to evaluate Context Setting: Background information and role definition Explicit Instructions: Step-by-step guidance Output Format: Precise formatting requirements Examples: Demonstration of expected responses Implementation in DSPy import dspy from typing import Dict, List, Optional class StructuredPromptEvaluator(dspy.Module): \"\"\"Base class for structured prompting evaluators.\"\"\" def __init__(self, task_spec: Dict): super().__init__() self.task_spec = task_spec self.prompt_template = self._build_structured_prompt() def _build_structured_prompt(self) -> str: \"\"\"Build a structured prompt from task specification.\"\"\" components = [] # Task Definition components.append(f\"Task: {self.task_spec['task_name']}\") components.append(f\"Objective: {self.task_spec['objective']}\") # Context Setting if 'context' in self.task_spec: components.append(f\"Context: {self.task_spec['context']}\") # Instructions components.append(\"\\nInstructions:\") for i, instruction in enumerate(self.task_spec['instructions'], 1): components.append(f\"{i}. {instruction}\") # Output Format components.append(\"\\nOutput Format:\") components.append(self.task_spec['output_format']) # Examples (if provided) if 'examples' in self.task_spec: components.append(\"\\nExamples:\") for example in self.task_spec['examples']: components.append(f\"Input: {example['input']}\") components.append(f\"Output: {example['output']}\\n\") # Input placeholder components.append(\"\\nInput: {input}\") return \"\\n\".join(components) def forward(self, **kwargs): \"\"\"Execute the structured prompt.\"\"\" prompt = self.prompt_template.format(**kwargs) return dspy.Predict(prompt) Advanced Structured Prompting Techniques 1. Template-Based Prompt Generation A template system allows you to generate structured prompts dynamically based on task configurations. class PromptTemplate: \"\"\"Template system for generating structured prompts.\"\"\" # ... (See Markdown source for full implementation) ... def generate_prompt(self, task_config: Dict) -> str: # Implementation details... pass See full source code in the markdown file for complete implementation details. Integration with DSPy Evaluation Structured Evaluation Metrics class StructuredMetric(dspy.Metric): \"\"\"Custom metric for evaluating structured prompt outputs.\"\"\" def __init__(self, structure_validator, content_evaluator): self.structure_validator = structure_validator self.content_evaluator = content_evaluator def __call__(self, example, pred, trace=None): \"\"\"Evaluate both structure and content quality.\"\"\" # Check if output follows required structure structure_score = self.structure_validator(pred.output) # Evaluate content quality content_score = self.content_evaluator( example=example, prediction=pred.output ) # Combine scores total_score = 0.6 * structure_score + 0.4 * content_score return total_score Continue to LLM-as-a-Judge"}, {"title": "LLM-as-a-Judge", "url": "chapters/chapter-04/llm-as-a-judge.html", "content": "Chapter 4 LLM-as-a-Judge Using language models to evaluate nuanced, context-sensitive outputs. Overview LLM-as-a-Judge is a powerful evaluation paradigm that uses large language models to assess the quality and impact of model outputs. This approach is particularly valuable when traditional metrics fail to capture domain-specific nuances or real-world consequences. This framework becomes essential in safety-critical domains like healthcare, where standard metrics such as Word Error Rate (WER) correlate poorly with actual clinical risk. When to Use LLM-as-a-Judge 1. Domain-Specific Impact Assessment # Standard metrics (WER, BLEU) fail to capture clinical meaning standard_metrics = { \"wer\": 0.12, # Low error rate \"bleu\": 0.85, # High overlap # But missed critical negation: \"no chest pain\" ‚Üí \"chest pain\" } # LLM-as-a-Judge captures actual impact clinical_judge = ClinicalImpactJudge() assessment = clinical_judge.evaluate( ground_truth=\"Patient reports no chest pain\", hypothesis=\"Patient reports chest pain\" ) # Result: SIGNIFICANT_CLINICAL_IMPACT (2/2) 2. Nuanced Semantic Evaluation Traditional metrics struggle with context-dependent meaning, domain terminology, and complex concept relationships. LLM judges can reason through these nuances. Implementation Framework Core Judge Architecture import dspy from typing import Dict, List, Tuple, Optional class LLMJudge(dspy.Module): \"\"\"Base class for LLM-as-a-Judge implementations.\"\"\" def __init__(self, prompt_template: str, output_schema: type, max_tokens: int = 1000): super().__init__() self.prompt_template = prompt_template self.output_schema = output_schema # Initialize the judge with Chain of Thought for reasoning self.judge = dspy.ChainOfThought( self.prompt_template, max_tokens=max_tokens ) def evaluate(self, ground_truth: str, hypothesis: str, **context) -> Dict: \"\"\"Evaluate hypothesis against ground truth.\"\"\" # Get LLM evaluation provided prompt and context result = self.judge( ground_truth=ground_truth, hypothesis=hypothesis, **context ) # Parse output... (omitted for brevity, see source) return self.parse_output(result) def parse_output(self, raw_output) -> Dict: # Implementation details... pass Clinical Impact Judge Example class ClinicalImpactJudge(LLMJudge): \"\"\"Judge for assessing clinical impact of ASR errors.\"\"\" def __init__(self): prompt_template = \"\"\" You are an expert medical analyst. Assess the clinical impact of errors. Core Principle: Would a clinician make different decisions? Impact Levels: - 0: No Clinical Impact - 1: Minimal Clinical Impact - 2: Significant Clinical Impact Ground Truth: {ground_truth} Transcription: {hypothesis} Context: {context} \"\"\" super().__init__(prompt_template, dict) Integration with DSPy You can wrap your LLM Judge as a DSPy Metric to use it within evaluation loops and optimization. class LLMJudgeMetric(dspy.Metric): def __init__(self, judge: LLMJudge): self.judge = judge def __call__(self, example, prediction, trace=None): ground_truth = example.outputs() hypothesis = prediction.get('output', str(prediction)) result = self.judge.evaluate( ground_truth=ground_truth, hypothesis=hypothesis ) # Return numeric score for optimization if 'evaluation' in result: return result['evaluation'] / 2.0 # Normalize 0-2 scale to 0-1 return 0.0 Continue to Human-Aligned Evaluation"}, {"title": "Chapter 4: Evaluation", "url": "chapters/chapter-04/index.html", "content": "Chapter 4 Evaluation The foundation of reliable DSPy systems: datasets, metrics, and systematic measurement. ~4-5 hours Intermediate Evaluation is the foundation of building reliable DSPy applications. This chapter teaches you how to measure, validate, and systematically improve your LLM programs through rigorous evaluation practices. üöÄ What You'll Learn üìä Why Evaluation Matters Understand the critical role of evaluation in building reliable AI systems. üóÇÔ∏è Creating Datasets Learn to build, structure, and manage datasets using DSPy's Example class. üìè Defining Metrics Design metrics that accurately measure what matters for your specific task. üîÑ Evaluation Loops Run systematic evaluations and integrate them into your development workflow. ‚úÖ Best Practices Follow proven patterns for reliable, reproducible evaluations. üìù Structured Prompting Systematic methodology for creating robust evaluation prompts. ‚öñÔ∏è LLM-as-a-Judge Using language models to evaluate nuanced, context-sensitive tasks. üéØ Human-Aligned Eval Creating evaluation systems that reflect actual human priorities. The Evaluation Imperative Without evaluation, you're flying blind: Without Evaluation # How good is this? No idea! qa = dspy.Predict(\"question -> answer\") result = qa(question=\"What is the capital of France?\") print(result.answer) # \"Paris\" - but is it always right? With Evaluation import dspy # Define what \"good\" means def accuracy(example, pred, trace=None): return example.answer.lower() == pred.answer.lower() # Measure systematically evaluate = dspy.Evaluate( devset=test_data, metric=accuracy, num_threads=8, display_progress=True ) # Know exactly how good it is score = evaluate(qa) print(f\"Accuracy: {score}%\") # \"Accuracy: 87.5%\" The Evaluation-Optimization Connection In DSPy, evaluation isn't just for measurement‚Äîit's the engine that drives optimization: Dataset \"What to test\" Metric \"How to measure\" Optimizer \"How to improve\" Better Module \"The result\" Key Insight: The quality of your optimization is bounded by the quality of your evaluation. Prerequisites Chapter 1-3: Completed fundamentals, signatures, and modules Working DSPy setup: with API keys configured Basic statistics knowledge: (averages, percentages) Understanding of train/test splits: in machine learning Start Reading"}, {"title": "Best Practices", "url": "chapters/chapter-04/best-practices.html", "content": "Chapter 4 ¬∑ Section 6 Best Practices Proven strategies for reliable, reproducible, and effective evaluation. ~25 min read Dataset Curation 1. Ensure Representative Data Your evaluation set should mirror real-world distribution. A dataset of only \"easy\" questions will give you a false sense of security. # BAD: Homogeneous data data = [dspy.Example(q=\"2+2?\", a=\"4\"), dspy.Example(q=\"3+3?\", a=\"6\")] # GOOD: Diverse data with edge cases data = [ dspy.Example(q=\"2+2?\", a=\"4\"), # Simple dspy.Example(q=\"What is the meaning of lif\", a=\"Likely typo...\"), # Edge case dspy.Example(q=\"\", a=\"Please provide input\") # Empty input ] 2. Balance Your Dataset Ensure key categories are equally represented to prevent bias towards majority classes. Metric Design 1. Measure What Matters Don't use proxy metrics just because they are easy. Length does not equal quality. 2. Make Metrics Robust Handle formatting variations (case sensitivity, punctuation) gracefully. def robust_metric(example, pred, trace=None): # Normalize before comparing clean_gold = example.answer.lower().strip() clean_pred = pred.answer.lower().strip() return clean_gold == clean_pred Avoiding Data Leakage ‚ö†Ô∏è Critical: Data leakage‚Äîwhere test data overlaps with training data‚Äîinvalidates your entire evaluation. Prevention Strategies Split by Date: Train on past data, test on future data. Deduplicate: Remove identical or near-identical examples before splitting. Verify Disjoint Sets: Programmatically check that len(set(train) & set(test)) == 0. Reproducibility 1. Fix Random Seeds import random random.seed(42) 2. Version Control Data Treat your datasets like code. Track versions to understand performance changes over time. Checklist for Every Evaluation ‚úÖ Data Representation Does the data cover edge cases and real-world variety? ‚úÖ Metric Validity Does the metric actually measure success for the user? ‚úÖ No Leakage Are training and testing sets completely disjoint? ‚úÖ Baselines Did you compare against a simple baseline? Next: Exercises"}, {"title": "Exercises", "url": "chapters/chapter-04/exercises.html", "content": "Chapter 4 ¬∑ Section 7 Exercises Put your evaluation skills to the test with these hands-on coding challenges. ~2-3 hours Hands-on Exercise 1: Creating a Quality Dataset Difficulty: ‚≠ê‚≠ê Intermediate Objective Create a well-structured evaluation dataset for a sentiment analysis task. Requirements Create a dataset of at least 30 examples with fields: text, sentiment (pos/neg/neu), and confidence. Ensure valid distribution: 10+ positive, 10+ negative, 5+ neutral. Include at least 5 edge cases (sarcasm, mixed sentiment, short text). Properly split data into train (60%), dev (20%), and test (20%). Starter Code import dspy import random def create_sentiment_dataset(): \"\"\"Returns: Tuple of (trainset, devset, testset)\"\"\" examples = [] # TODO: Add examples (positive, negative, neutral, edge cases) # Hint: Use dspy.Example(...).with_inputs(\"text\") # TODO: Shuffle with fixed seed # TODO: Split data return trainset, devset, testset Exercise 2: Designing a Custom Metric Difficulty: ‚≠ê‚≠ê Intermediate Objective Design a comprehensive metric for evaluating a question-answering system. Requirements Create a metric that combines: Correctness (40%): Does the answer contain the expected info? Completeness (30%): are all key points addressed? Conciseness (20%): Is the answer brief (10-100 words)? Format (10%): No repeated words or odd punctuation? The metric handles the trace parameter correctly (stricter during optimization). Starter Code def qa_quality_metric(example, pred, trace=None): # TODO: Implement sub-scores correctness = ... completeness = ... conciseness = ... format_score = ... final_score = (0.4 * correctness + 0.3 * completeness + 0.2 * conciseness + 0.1 * format_score) if trace is not None: return final_score >= 0.7 # Stricter for optimization return final_score Exercise 3: Systematic Evaluation Difficulty: ‚≠ê‚≠ê‚≠ê Intermediate-Advanced Objective Build a function that runs evaluation and provides a detailed report, including error analysis. Requirements Function takes a module, dataset, and metric. Runs evaluation and captures detailed results. Categorizes errors (e.g., empty response, wrong answer). Identifies the best and worst performing examples. Returns a dictionary with aggregate score and analysis. Starter Code def comprehensive_evaluation(module, devset, metric): results = { 'score': 0, 'errors': [], 'best_examples': [], 'worst_examples': [] } # TODO: Iterate, predict, score, and analyze return results Structured Prompting"}, {"title": "Solutions", "url": "chapters/chapter-04/solutions.html", "content": "Chapter 4 ¬∑ Section 11 Solutions Detailed implementations for the evaluation exercises. Exercise 1: Creating a Quality Dataset This solution demonstrates how to create a balanced, diverse dataset with edge cases and proper splitting. import dspy import random def create_sentiment_dataset(): examples = [] # 1. POSITIVE EXAMPLES (10+) positive_texts = [ \"This product is amazing! Best purchase ever!\", \"Absolutely love the quality.\", \"Great customer service, very helpful.\", \"Fast shipping and item as described.\", \"Exceeded my expectations entirely.\", \"I would recommend this to everyone.\", \"Fantastic value for the money.\", \"Works perfectly out of the box.\", \"Simple to use and effective.\", \"Five stars, no complaints!\" ] for text in positive_texts: examples.append(dspy.Example(text=text, sentiment=\"positive\", confidence=0.9).with_inputs(\"text\")) # 2. NEGATIVE EXAMPLES (10+) negative_texts = [ \"Terrible quality, broke immediately.\", \"Waste of money, do not buy.\", \"Customer service was rude and unhelpful.\", \"Shipping took forever and arrived damaged.\", \"Completely different from the description.\", \"Not worth the price tag.\", \"Stopped working after one day.\", \"Frustrating to set up.\", \"Poor design choices everywhere.\", \"I want a refund.\" ] for text in negative_texts: examples.append(dspy.Example(text=text, sentiment=\"negative\", confidence=0.9).with_inputs(\"text\")) # 3. NEUTRAL EXAMPLES (5+) neutral_texts = [ \"It's okay, nothing special.\", \"Average product for the price.\", \"Does the job, but has flaws.\", \"Delivered on time.\", \"It is what it is.\" ] for text in neutral_texts: examples.append(dspy.Example(text=text, sentiment=\"neutral\", confidence=0.5).with_inputs(\"text\")) # 4. EDGE CASESS (5+) edge_cases = [ (\"Great, another broken item.\", \"negative\", 0.8), # Sarcasm (\"Food was good, service was bad.\", \"neutral\", 0.6), # Mixed (\"Is this worth it?\", \"neutral\", 0.3), # Question (\"meh\", \"neutral\", 0.4), # Short (\"LOVE IT!!! üòç\", \"positive\", 0.95), # Emoji/formatting ] for text, sent, conf in edge_cases: examples.append(dspy.Example(text=text, sentiment=sent, confidence=conf).with_inputs(\"text\")) # 5. SHUFFLE random.seed(42) random.shuffle(examples) # 6. SPLIT (60/20/20) n = len(examples) train_end = int(n * 0.6) dev_end = int(n * 0.8) trainset = examples[:train_end] devset = examples[train_end:dev_end] testset = examples[dev_end:] return trainset, devset, testset Exercise 2: Designing a Custom Metric This metric implements weighted sub-scores and handles the trace parameter for optimization. def qa_quality_metric(example, pred, trace=None): # 1. Correctness (40%) # Simple inclusion check (could use semantic comparison in production) expected = example.answer.lower() actual = pred.answer.lower() correctness = 1.0 if expected in actual else 0.0 # 2. Completeness (30%) # Check if key concepts from answer are present key_terms = expected.split() found_terms = sum(1 for term in key_terms if term in actual) completeness = found_terms / len(key_terms) if key_terms else 1.0 # 3. Conciseness (20%) # Ideal length between 10 and 100 words word_count = len(actual.split()) if 10 <= word_count <= 100: conciseness = 1.0 elif word_count < 5 or word_count > 200: conciseness = 0.0 else: conciseness = 0.5 # 4. Format (10%) # Check for basic punctuation format_score = 1.0 if actual.strip().endswith(('.', '!', '?')) else 0.0 # Weighted Sum final_score = ( 0.4 * correctness + 0.3 * completeness + 0.2 * conciseness + 0.1 * format_score ) # Trace Handling if trace is not None: # Optimization Goal: High quality (>0.7) return final_score >= 0.7 return final_score Exercise 3: Systematic Evaluation A comprehensive evaluation function that provides actionable insights. def comprehensive_evaluation(module, devset, metric): results = { 'total_score': 0.0, 'count': 0, 'scores': [], 'errors': [], 'best_examples': [], 'worst_examples': [] } detailed_results = [] for idx, example in enumerate(devset): try: # Predict pred = module(**example.inputs()) # Score score = metric(example, pred) # Store results['total_score'] += score results['count'] += 1 results['scores'].append(score) detailed_results.append({ 'example': example, 'pred': pred, 'score': score }) # Error Analysis Categories if score < 0.5: error_type = \"low_quality\" if len(pred.answer) == 0: error_type = \"empty_response\" results['errors'].append({ 'index': idx, 'type': error_type, 'input': example.question, 'expected': example.answer, 'actual': pred.answer }) except Exception as e: results['errors'].append({ 'index': idx, 'type': 'exception', 'message': str(e) }) # Averages if results['count'] > 0: results['average_score'] = results['total_score'] / results['count'] # Sort for best/worst sorted_res = sorted(detailed_results, key=lambda x: x['score'], reverse=True) results['best_examples'] = sorted_res[:3] results['worst_examples'] = sorted_res[-3:] return results Next: Chapter 5 - Optimizers"}, {"title": "What Are Signatures?", "url": "chapters/chapter-02/what-are-signatures.html", "content": "Chapter 2 ¬∑ Section 2 What Are Signatures? Signatures are declarative specifications that define what a language model task does‚Äîwithout specifying how. ~15 min read Signature: A Task Contract A Signature in DSPy is a specification that defines the inputs and outputs of a language model task, similar to a function signature in programming. üí° Think of It Like a Function Signature In Python, a function signature tells you what goes in and what comes out: # Python function signature def add_numbers(a: int, b: int) -> int: \"\"\"Add two numbers and return the result.\"\"\" return a + b Similarly, a DSPy signature defines the contract for an LM task: # DSPy signature class QuestionAnswer(dspy.Signature): \"\"\"Answer questions accurately.\"\"\" question: str = dspy.InputField() # What goes in answer: str = dspy.OutputField() # What comes out üí° Key Insight: You don't write the implementation (the prompt)‚ÄîDSPy generates it for you based on your signature! üß© Components of a Signature Every signature has three essential parts: üìÉ Docstring (Task Description) Describes what the task does. This becomes part of the prompt that DSPy generates. class Summarize(dspy.Signature): \"\"\"Summarize the given text in 2-3 sentences.\"\"\" # ‚Üê Docstring ... üì• Input Fields Define what data the task receives. Use dspy.InputField(). class Summarize(dspy.Signature): \"\"\"Summarize the given text.\"\"\" text: str = dspy.InputField() # ‚Üê Input field ... üì§ Output Fields Define what the task produces. Use dspy.OutputField(). class Summarize(dspy.Signature): \"\"\"Summarize the given text.\"\"\" text: str = dspy.InputField() summary: str = dspy.OutputField() # ‚Üê Output field üìù A Complete Example Here's a signature for sentiment analysis with all components: import dspy class SentimentAnalysis(dspy.Signature): \"\"\"Analyze the sentiment of the given text and classify it.\"\"\" # Input: The text to analyze text: str = dspy.InputField( desc=\"The text to analyze for sentiment\" ) # Outputs: Classification and confidence sentiment: str = dspy.OutputField( desc=\"The sentiment: 'positive', 'negative', or 'neutral'\" ) confidence: str = dspy.OutputField( desc=\"Confidence level: 'high', 'medium', or 'low'\" ) Using this signature: # Create a predictor from the signature analyzer = dspy.Predict(SentimentAnalysis) # Use it result = analyzer(text=\"I absolutely love this product!\") print(f\"Sentiment: {result.sentiment}\") # positive print(f\"Confidence: {result.confidence}\") # high üîÆ Why Signatures Work When you use a signature, DSPy automatically: 1Ô∏è‚É£ Generates a Prompt DSPy creates an optimized prompt from your docstring, field names, and descriptions. 2Ô∏è‚É£ Formats Your Input Your input data is properly formatted and inserted into the prompt. 3Ô∏è‚É£ Calls the LM The formatted prompt is sent to your configured language model. 4Ô∏è‚É£ Parses the Output The LM's response is parsed and returned as a structured object with your named output fields. üéØ The magic: You focus on what you want, and DSPy handles how to get it! üìê Two Ways to Define Signatures DSPy offers two syntax options: Inline Signatures # Quick, simple syntax qa = dspy.Predict(\"question -> answer\") # With multiple fields summarize = dspy.Predict( \"document, max_length -> summary\" ) ‚úÖ Quick prototyping ‚úÖ Simple tasks ‚ö†Ô∏è Limited control Class-Based Signatures # Full-featured syntax class QA(dspy.Signature): \"\"\"Answer questions accurately.\"\"\" question: str = dspy.InputField() answer: str = dspy.OutputField( desc=\"concise answer\" ) ‚úÖ Full documentation ‚úÖ Type hints ‚úÖ Production-ready We'll cover both in detail in the following sections. üé® Common Signature Patterns Here are signatures for common NLP tasks: # Question Answering class QA(dspy.Signature): \"\"\"Answer the question based on the context.\"\"\" context: str = dspy.InputField() question: str = dspy.InputField() answer: str = dspy.OutputField() # Summarization class Summarize(dspy.Signature): \"\"\"Create a concise summary.\"\"\" document: str = dspy.InputField() summary: str = dspy.OutputField() # Classification class Classify(dspy.Signature): \"\"\"Classify the text into a category.\"\"\" text: str = dspy.InputField() category: str = dspy.OutputField() # Translation class Translate(dspy.Signature): \"\"\"Translate text to the target language.\"\"\" text: str = dspy.InputField() target_language: str = dspy.InputField() translation: str = dspy.OutputField() # Entity Extraction class ExtractEntities(dspy.Signature): \"\"\"Extract named entities from text.\"\"\" text: str = dspy.InputField() entities: list[str] = dspy.OutputField() üìù Key Takeaways Signatures are contracts that define task inputs and outputs Three components: docstring, input fields, output fields DSPy generates prompts automatically from your signature Two syntax options: inline (quick) and class-based (full-featured) Signatures enable optimization because they're declarative Continue to Inline Signatures"}, {"title": "Class-Based Signatures", "url": "chapters/chapter-02/class-signatures.html", "content": "Chapter 2 ¬∑ Section 4 Class-Based Signatures The full-featured, production-ready way to define LM tasks with documentation, type hints, and field descriptions. ~15 min read üèóÔ∏è Why Class-Based Signatures? While inline signatures are great for prototyping, class-based signatures provide the full power of DSPy: üìÉ Documentation Docstrings describe the task and become part of the prompt. üè∑Ô∏è Type Hints Specify expected types for structured outputs. üìù Field Descriptions Guide the LM with detailed field-level instructions. ‚ôªÔ∏è Reusability Import and reuse signatures across your codebase. üìê Basic Structure A class-based signature inherits from dspy.Signature: import dspy class TaskName(dspy.Signature): \"\"\"Description of what this task does.\"\"\" # Input fields input_name: type = dspy.InputField() # Output fields output_name: type = dspy.OutputField() Let's break down each component: üè∑Ô∏è Class Name Use PascalCase. Choose a name that describes the task (e.g., Summarize, TranslateToFrench, ExtractEntities). üìÉ Docstring A clear, concise description of the task. This becomes the main instruction in the generated prompt. üì• InputField Marks a field as input. The user provides these values when calling the module. üì§ OutputField Marks a field as output. The LM generates these values. üí° Complete Examples Question Answering class QuestionAnswer(dspy.Signature): \"\"\"Answer questions based on the provided context.\"\"\" context: str = dspy.InputField( desc=\"Background information to answer the question\" ) question: str = dspy.InputField( desc=\"The question to answer\" ) answer: str = dspy.OutputField( desc=\"A concise, accurate answer based on the context\" ) # Usage qa = dspy.Predict(QuestionAnswer) result = qa( context=\"The Eiffel Tower was completed in 1889.\", question=\"When was the Eiffel Tower completed?\" ) print(result.answer) # 1889 Sentiment Analysis class SentimentAnalysis(dspy.Signature): \"\"\"Analyze the sentiment of the given text.\"\"\" text: str = dspy.InputField( desc=\"The text to analyze\" ) sentiment: str = dspy.OutputField( desc=\"The sentiment: 'positive', 'negative', or 'neutral'\" ) confidence: float = dspy.OutputField( desc=\"Confidence score from 0.0 to 1.0\" ) reasoning: str = dspy.OutputField( desc=\"Brief explanation for the classification\" ) # Usage analyzer = dspy.Predict(SentimentAnalysis) result = analyzer(text=\"This product exceeded my expectations!\") print(f\"Sentiment: {result.sentiment}\") # positive print(f\"Confidence: {result.confidence}\") # 0.95 print(f\"Reasoning: {result.reasoning}\") # Expresses strong satisfaction Code Review class CodeReview(dspy.Signature): \"\"\"Review code for bugs, style issues, and improvements.\"\"\" code: str = dspy.InputField( desc=\"The code to review\" ) language: str = dspy.InputField( desc=\"Programming language of the code\" ) bugs: list[str] = dspy.OutputField( desc=\"List of potential bugs found\" ) style_issues: list[str] = dspy.OutputField( desc=\"List of style/formatting issues\" ) suggestions: list[str] = dspy.OutputField( desc=\"List of improvement suggestions\" ) overall_quality: str = dspy.OutputField( desc=\"Quality rating: 'excellent', 'good', 'needs_work', or 'poor'\" ) ‚úçÔ∏è Writing Effective Docstrings The docstring is crucial‚Äîit becomes the main instruction for the LM: ‚ùå Weak Docstrings # Too vague \"\"\"Do the task.\"\"\" # Just restates the class name \"\"\"Summarize class.\"\"\" # No docstring at all class MyTask(dspy.Signature): text: str = dspy.InputField() output: str = dspy.OutputField() ‚úÖ Strong Docstrings # Clear and specific \"\"\"Summarize the text in 2-3 sentences, capturing the main points.\"\"\" # Includes constraints \"\"\"Extract all person names mentioned in the text. Return an empty list if no names are found.\"\"\" # Provides context \"\"\"Translate the text to French, preserving tone and style.\"\"\" üí° Pro tip: Think of the docstring as instructions you'd give to a helpful assistant. Be clear about what you want and any constraints. üîÄ Working with Multiple Outputs Class-based signatures shine when you need structured, multi-field outputs: class ArticleAnalysis(dspy.Signature): \"\"\"Analyze a news article comprehensively.\"\"\" article: str = dspy.InputField(desc=\"The article text\") # Multiple structured outputs title: str = dspy.OutputField(desc=\"Article headline\") summary: str = dspy.OutputField(desc=\"2-3 sentence summary\") topics: list[str] = dspy.OutputField(desc=\"Main topics covered\") sentiment: str = dspy.OutputField(desc=\"Overall tone\") key_entities: list[str] = dspy.OutputField(desc=\"People, places, orgs mentioned\") reading_time: int = dspy.OutputField(desc=\"Estimated reading time in minutes\") # All outputs accessible as attributes analyzer = dspy.Predict(ArticleAnalysis) result = analyzer(article=article_text) print(f\"Title: {result.title}\") print(f\"Topics: {result.topics}\") print(f\"Entities: {result.key_entities}\") üß¨ Signature Composition You can create variations of signatures for related tasks: # Base translation signature class Translate(dspy.Signature): \"\"\"Translate text to the target langua"}, {"title": "Field Types & Descriptions", "url": "chapters/chapter-02/field-types.html", "content": "Chapter 2 ¬∑ Section 5 Field Types & Descriptions Use type hints and descriptions to guide LM behavior and get precisely structured outputs. ~12 min read üéØ Why Types and Descriptions Matter Field types and descriptions are powerful tools for shaping LM outputs: üè∑Ô∏è Type Hints Tell DSPy what format you expect (string, list, number, boolean). üìù Descriptions Provide constraints and context that guide the LM's response. ‚úÖ Validation DSPy can validate and parse outputs based on your type hints. üìä Supported Field Types Basic Types import dspy class BasicTypes(dspy.Signature): \"\"\"Demonstrate basic field types.\"\"\" # String - most common, default type text: str = dspy.InputField() summary: str = dspy.OutputField() # Integer word_count: int = dspy.OutputField(desc=\"Number of words\") # Float confidence: float = dspy.OutputField(desc=\"Score from 0.0 to 1.0\") # Boolean is_valid: bool = dspy.OutputField(desc=\"True if valid, False otherwise\") Collection Types class CollectionTypes(dspy.Signature): \"\"\"Demonstrate collection field types.\"\"\" # List of strings keywords: list[str] = dspy.OutputField( desc=\"List of relevant keywords\" ) # List of integers page_numbers: list[int] = dspy.OutputField( desc=\"Relevant page numbers\" ) # Note: Dictionaries require careful handling # Use typing.Dict or structured output patterns Literal Types (Constrained Values) from typing import Literal class ConstrainedOutput(dspy.Signature): \"\"\"Use Literal for constrained output values.\"\"\" text: str = dspy.InputField() # Only these specific values allowed sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = dspy.OutputField() priority: Literal[\"low\", \"medium\", \"high\", \"critical\"] = dspy.OutputField() rating: Literal[1, 2, 3, 4, 5] = dspy.OutputField() üí° Pro tip: Literal types are great for classification tasks where you want specific output values! ‚úçÔ∏è Writing Effective Descriptions The desc parameter is your main tool for guiding outputs: Format Specifications class FormattedOutputs(dspy.Signature): \"\"\"Generate formatted outputs.\"\"\" document: str = dspy.InputField() # Specify format summary: str = dspy.OutputField( desc=\"2-3 sentences maximum\" ) date: str = dspy.OutputField( desc=\"Date in YYYY-MM-DD format\" ) phone: str = dspy.OutputField( desc=\"Phone number in format: (XXX) XXX-XXXX\" ) bullet_points: str = dspy.OutputField( desc=\"Formatted as a bulleted list with - prefix\" ) Value Constraints class ConstrainedValues(dspy.Signature): \"\"\"Outputs with value constraints.\"\"\" text: str = dspy.InputField() # Numeric ranges rating: int = dspy.OutputField( desc=\"Rating from 1 to 10\" ) confidence: float = dspy.OutputField( desc=\"Confidence between 0.0 and 1.0\" ) # Categorical constraints category: str = dspy.OutputField( desc=\"One of: 'tech', 'finance', 'health', 'sports'\" ) # Length constraints title: str = dspy.OutputField( desc=\"Short title, maximum 10 words\" ) Contextual Instructions class ContextualOutputs(dspy.Signature): \"\"\"Outputs with contextual guidance.\"\"\" article: str = dspy.InputField( desc=\"News article to analyze\" ) # Tone guidance summary: str = dspy.OutputField( desc=\"Professional, objective summary for business readers\" ) # Audience consideration explanation: str = dspy.OutputField( desc=\"Simple explanation suitable for a general audience\" ) # Action-oriented action_items: list[str] = dspy.OutputField( desc=\"Specific, actionable items the reader should take\" ) üîó Combining Types and Descriptions The most effective signatures combine type hints with descriptive text: from typing import Literal class ProductReview(dspy.Signature): \"\"\"Analyze a product review comprehensively.\"\"\" review_text: str = dspy.InputField( desc=\"The full text of the customer review\" ) # Literal type + description sentiment: Literal[\"positive\", \"negative\", \"mixed\"] = dspy.OutputField( desc=\"Overall sentiment of the review\" ) # Float type + constrained range rating_prediction: float = dspy.OutputField( desc=\"Predicted star rating from 1.0 to 5.0\" ) # List type + specific guidance pros: list[str] = dspy.OutputField( desc=\"Positive aspects mentioned, 3-5 items\" ) cons: list[str] = dspy.OutputField( desc=\"Negative aspects mentioned, 3-5 items\" ) # Boolean + clear criteria would_recommend: bool = dspy.OutputField( desc=\"True if reviewer would recommend the product\" ) # String + format specification one_liner: str = dspy.OutputField( desc=\"One sentence summary, max 15 words\" ) Example Usage import dspy # Configure LM lm = dspy.LM(\"openai/gpt-4o-mini\") dspy.configure(lm=lm) # Create predictor reviewer = dspy.Predict(ProductReview) # Analyze a review result = reviewer( review_text=\"\"\" I've been using this laptop for 3 months now. The battery life is amazing - easily lasts 12 hours! The keyboard is comfortable for long typing sessions. However, the trackpad is a bit small and the speakers are mediocre. For the price point though, it's a solid choice for students or light work. \"\"\" ) print(f\"Sentiment: {result.sentiment}\") # positive print(f\"Predicted Rating: {result.rating_pred"}, {"title": "Chapter 2: Signatures", "url": "chapters/chapter-02/index.html", "content": "Chapter 2 Signatures Signatures are the foundation of DSPy - they define what your language model tasks do without specifying how. 7 Sections ~2-3 hours ‚≠ê Beginner üìù Mastering Signatures The declarative way to define LM tasks What You'll Learn By the end of this chapter, you will: ‚úÖ Understand what signatures are and why they matter ‚úÖ Create inline signatures for quick prototyping ‚úÖ Build class-based signatures for production use ‚úÖ Use field types and descriptions effectively ‚úÖ Design signatures for common NLP tasks ‚úÖ Follow best practices for signature design Chapter Overview This chapter covers everything you need to know about DSPy signatures: üìñ What Are Signatures? Understand signatures as task specifications - the contract between inputs and outputs. ‚ö° Inline Signatures Learn the quick, string-based syntax for simple tasks and rapid prototyping. üèóÔ∏è Class-Based Signatures Master the full power of typed, documented signatures for production applications. üîß Field Types & Descriptions Use type hints and descriptions to guide LM behavior and improve outputs. üèãÔ∏è Exercises Practice creating signatures for various real-world tasks. Why Signatures Matter Signatures are the core abstraction in DSPy. They separate what you want from how it's accomplished: ‚ùå Traditional Prompting # You write the HOW prompt = \"\"\" You are a helpful assistant. Please summarize the following text in 2-3 sentences. Be concise and capture the main points. Text: {text} Summary: \"\"\" ‚ö†Ô∏è Fragile instructions ‚ö†Ô∏è Hard to optimize ‚úÖ DSPy Signature # You specify the WHAT class Summarize(dspy.Signature): \"\"\"Summarize text concisely.\"\"\" text: str = dspy.InputField() summary: str = dspy.OutputField( desc=\"2-3 sentence summary\" ) ‚úÖ Declarative and clean ‚úÖ Auto-optimizable Key Concepts Preview üìù InputField Defines what data goes into your task. Can include descriptions to guide the LM. üì§ OutputField Defines what data comes out of your task. Descriptions shape the output format. üìÉ Docstrings The class docstring becomes the task instruction. Keep it clear and focused. üè∑Ô∏è Type Hints Use Python type hints (str, list, bool) for structured outputs. Prerequisites Before starting this chapter, ensure you have: ‚úÖ Completed Chapter 1 (DSPy Fundamentals) ‚úÖ DSPy installed and configured ‚úÖ API key for at least one LM provider ‚úÖ Basic Python class knowledge (inheritance, attributes) üí° New to DSPy? Start with Chapter 1: DSPy Fundamentals Estimated Time üìñ Reading 45-60 min üíª Running examples 45 min üèãÔ∏è Exercises 1-1.5 hours ‚è±Ô∏è Total 2-3 hours üöÄ Let's Begin! Ready to master signatures? Let's start by understanding what they are and why they're so powerful. Continue to What Are Signatures?"}, {"title": "Inline Signatures", "url": "chapters/chapter-02/inline-signatures.html", "content": "Chapter 2 ¬∑ Section 3 Inline Signatures The quick, string-based syntax for rapid prototyping and simple tasks. ~10 min read ‚ö° What Are Inline Signatures? Inline signatures are a shorthand notation for defining simple signatures using a string format. They're perfect for quick experiments and simple tasks. import dspy # Inline signature: input -> output qa = dspy.Predict(\"question -> answer\") # Use it immediately result = qa(question=\"What is the capital of France?\") print(result.answer) # Paris üìê Basic Syntax The inline signature syntax follows this pattern: \"input_field1, input_field2 -> output_field1, output_field2\" üì• Left Side: Inputs Field names before the arrow are input fields. Separate multiple inputs with commas. ‚û°Ô∏è Arrow: Separator The -> arrow separates inputs from outputs. üì§ Right Side: Outputs Field names after the arrow are output fields. The LM will generate these. üí° Examples Single Input, Single Output # Question answering qa = dspy.Predict(\"question -> answer\") # Summarization summarize = dspy.Predict(\"document -> summary\") # Translation translate = dspy.Predict(\"text -> french_translation\") Multiple Inputs # Context-based QA qa = dspy.Predict(\"context, question -> answer\") # Call it with multiple inputs result = qa( context=\"Paris is the capital of France.\", question=\"What is the capital of France?\" ) print(result.answer) # Paris Multiple Outputs # Sentiment with confidence sentiment = dspy.Predict(\"text -> sentiment, confidence\") result = sentiment(text=\"I love this product!\") print(result.sentiment) # positive print(result.confidence) # high Multiple Inputs and Outputs # Complex task analyzer = dspy.Predict(\"text, task_type -> result, explanation, confidence\") result = analyzer( text=\"The stock market rose 5% today.\", task_type=\"sentiment analysis\" ) print(result.result) # positive print(result.explanation) # Market gains indicate positive sentiment print(result.confidence) # high üß© Using with Different Modules Inline signatures work with all DSPy modules: import dspy # Basic prediction predict = dspy.Predict(\"question -> answer\") # With chain-of-thought reasoning cot = dspy.ChainOfThought(\"question -> answer\") # With multiple attempts and voting ensemble = dspy.ChainOfThoughtWithHint(\"question, hint -> answer\") # ReAct agent with tools agent = dspy.ReAct(\"goal -> plan, action\") üí° Pro tip: ChainOfThought automatically adds a rationale field to outputs, showing the model's reasoning! # ChainOfThought adds reasoning cot = dspy.ChainOfThought(\"question -> answer\") result = cot(question=\"What is 25 * 4?\") print(result.rationale) # Let me calculate: 25 * 4 = 100 print(result.answer) # 100 ‚úÖ Field Naming Best Practices ‚úÖ Use Descriptive Names Field names become part of the prompt, so make them meaningful. # Good: descriptive \"customer_review -> sentiment, key_points\" # Bad: vague \"x -> y, z\" ‚úÖ Use snake_case Python convention for variable names works best. # Good \"source_text, target_language -> translated_text\" # Avoid \"sourceText, targetLanguage -> translatedText\" ‚úÖ Be Specific About Output Format Field names hint at expected format. # Suggests yes/no answer \"text -> is_spam\" # Suggests a list \"article -> key_topics\" # Suggests a number \"product_reviews -> average_rating\" üéØ When to Use Inline Signatures ‚úÖ Great For Quick prototyping and experiments Simple, one-off tasks Interactive exploration (notebooks) Learning DSPy basics Tasks with obvious field names ‚ùå Consider Class-Based When You need detailed field descriptions Tasks require documentation Building production systems Fields need type hints Reusing signatures across modules üîß Practical Examples Building a Quick Chatbot import dspy # Simple chatbot signature chat = dspy.Predict(\"user_message, chat_history -> assistant_response\") # Simulate a conversation history = \"\" while True: user_input = input(\"You: \") if user_input.lower() == \"quit\": break result = chat(user_message=user_input, chat_history=history) print(f\"Bot: {result.assistant_response}\") # Update history history += f\"User: {user_input}\\nAssistant: {result.assistant_response}\\n\" Content Moderation # Content moderation with reasoning moderate = dspy.ChainOfThought(\"content -> is_appropriate, reason\") result = moderate(content=\"Check out this amazing product!\") print(f\"Appropriate: {result.is_appropriate}\") # yes print(f\"Reason: {result.reason}\") # No harmful content detected Data Extraction # Extract structured data extract = dspy.Predict(\"email_text -> sender_name, subject, action_items\") email = \"\"\" From: John Smith Subject: Q4 Planning Meeting Hi team, Please review the attached slides before Friday. We need to finalize the budget by next week. \"\"\" result = extract(email_text=email) print(result.sender_name) # John Smith print(result.subject) # Q4 Planning Meeting print(result.action_items) # Review slides, finalize budget üìù Key Takeaways Inline signatures use string syntax: \"inputs -> outputs\" Separate multiple fields with commas Field names matter - they become part of the p"}, {"title": "Exercises", "url": "chapters/chapter-02/exercises.html", "content": "Chapter 2 ¬∑ Section 6 Exercises Practice creating signatures for various real-world tasks. 5 Exercises ~1-1.5 hours üèãÔ∏è Practice Your Signature Skills These exercises will help you master DSPy signatures. Start with the basics and work up to more complex designs. üí° Tip: Try to complete each exercise before looking at the solutions. The best way to learn is by doing! Exercise 1 ‚≠ê Beginner Inline Signature Basics Task: Create inline signatures for the following tasks and test them with sample inputs. a) Simple Translation Create an inline signature that takes text and translates it to Spanish. b) Headline Generator Create an inline signature that takes an article and generates a catchy headline. c) Fact Checker Create an inline signature with two inputs (claim, context) that outputs whether the claim is supported. Expected Skills: Using dspy.Predict() with string signatures Handling single and multiple inputs Accessing output fields Exercise 2 ‚≠ê Beginner Your First Class-Based Signature Task: Convert the following inline signature to a class-based signature with proper documentation: # Convert this inline signature: qa = dspy.Predict(\"context, question -> answer\") Requirements: Create a class named ContextQA Add a descriptive docstring Include type hints for all fields Add descriptions using the desc= parameter Test with a sample question Exercise 3 ‚≠ê‚≠ê Intermediate Multi-Output Signature Task: Design a signature for comprehensive email analysis. Requirements: Input: email_text (the full email content) Outputs: subject - What the email is about sender_intent - What the sender wants (request, inform, complaint, etc.) urgency - How urgent (low, medium, high) action_required - Boolean indicating if action is needed suggested_response - A brief suggested reply Test your signature with this email: Hi Team, The client meeting has been moved to tomorrow at 3 PM instead of Thursday. Please confirm your availability ASAP as we need to finalize attendees. This is critical - they're making a decision on our proposal next week. Thanks, Sarah Exercise 4 ‚≠ê‚≠ê Intermediate Using Literal Types Task: Create a content moderation signature using Literal types for constrained outputs. Requirements: Input: content - Text to moderate Outputs: category - One of: \"safe\", \"mild\", \"sensitive\", \"harmful\" flags - List of issues detected (violence, hate_speech, adult_content, spam, misinformation) confidence - Float between 0.0 and 1.0 requires_review - Boolean for human review explanation - Reasoning for the classification Test with various content types: \"Check out this amazing recipe for chocolate cake!\" \"Breaking: Aliens confirmed to live among us!\" (misinformation) A legitimate news headline of your choice Exercise 5 ‚≠ê‚≠ê‚≠ê Advanced Signature Collection for a Blog Platform Task: Design a complete set of signatures for a blog platform's AI features. Create these signatures: 1Ô∏è‚É£ GenerateBlogPost Takes a topic and key points, generates a complete blog post with title, introduction, body sections, and conclusion. 2Ô∏è‚É£ SEOOptimizer Takes a blog post and generates SEO metadata: meta title, meta description, keywords, and suggested URL slug. 3Ô∏è‚É£ SocialMediaPosts Takes a blog post and generates social media content: tweet (280 chars), LinkedIn post, and Instagram caption. 4Ô∏è‚É£ ContentAnalyzer Takes a blog post and outputs: reading time, difficulty level, target audience, topics covered, and improvement suggestions. Bonus Challenge: Create a simple script that uses all four signatures in sequence to demonstrate a complete content pipeline. üí° Exercise Tips üìù Write Clear Docstrings The docstring is your main instruction to the LM - make it specific! üè∑Ô∏è Use Appropriate Types Choose types that match your expected output format (str, list, bool, Literal) üìè Add Constraints in Descriptions Use descriptions to specify formats, lengths, and valid values üß™ Test with Edge Cases Try unusual inputs to see how your signatures handle them üéâ Great Work! Completed the exercises? Check your solutions! View Solutions"}, {"title": "Solutions", "url": "chapters/chapter-02/solutions.html", "content": "Chapter 2 ¬∑ Section 7 Solutions Complete solutions with explanations for all Chapter 2 exercises. 5 Solutions ‚ö†Ô∏è Spoiler Alert! Try to complete the exercises on your own before viewing these solutions. Learning by doing is the most effective approach! Solution 1 ‚≠ê Beginner Inline Signature Basics import dspy from dotenv import load_dotenv import os # Load environment variables load_dotenv() # Configure DSPy lm = dspy.LM(\"openai/gpt-4o-mini\") dspy.configure(lm=lm) # ============================================ # Exercise 1a: Simple Translation # ============================================ translate_to_spanish = dspy.Predict(\"text -> spanish_translation\") result = translate_to_spanish(text=\"Hello, how are you today?\") print(\"1a) Translation:\") print(f\" Input: Hello, how are you today?\") print(f\" Spanish: {result.spanish_translation}\") print() # ============================================ # Exercise 1b: Headline Generator # ============================================ generate_headline = dspy.Predict(\"article -> headline\") article = \"\"\" Scientists at MIT have developed a new battery technology that could triple the range of electric vehicles while cutting charging time in half. The breakthrough uses a novel silicon-graphene composite that remains stable over thousands of charge cycles. \"\"\" result = generate_headline(article=article) print(\"1b) Headline Generator:\") print(f\" Generated Headline: {result.headline}\") print() # ============================================ # Exercise 1c: Fact Checker # ============================================ fact_check = dspy.Predict(\"claim, context -> is_supported, explanation\") claim = \"The Eiffel Tower is located in London.\" context = \"The Eiffel Tower is a famous landmark in Paris, France. It was built for the 1889 World's Fair.\" result = fact_check(claim=claim, context=context) print(\"1c) Fact Checker:\") print(f\" Claim: {claim}\") print(f\" Supported: {result.is_supported}\") print(f\" Explanation: {result.explanation}\") üí° Key Concepts Inline signatures use the arrow syntax (->) to separate inputs from outputs. Multiple inputs/outputs are comma-separated. Solution 2 ‚≠ê Beginner Your First Class-Based Signature import dspy from dotenv import load_dotenv load_dotenv() lm = dspy.LM(\"openai/gpt-4o-mini\") dspy.configure(lm=lm) # ============================================ # Class-based signature for Context QA # ============================================ class ContextQA(dspy.Signature): \"\"\"Answer questions accurately based on the provided context. Only use information from the context to answer the question. If the answer cannot be found in the context, say so.\"\"\" context: str = dspy.InputField( desc=\"Background information containing the answer\" ) question: str = dspy.InputField( desc=\"The question to answer based on the context\" ) answer: str = dspy.OutputField( desc=\"A concise, accurate answer derived from the context\" ) # Create predictor qa = dspy.Predict(ContextQA) # Test with sample question context = \"\"\" The Great Wall of China is one of the most impressive architectural feats in history. Construction began during the 7th century BC, with the most well-known sections built during the Ming Dynasty (1368-1644). The wall stretches approximately 13,170 miles (21,196 kilometers) and was primarily built to protect against invasions from the north. \"\"\" result = qa( context=context, question=\"How long is the Great Wall of China?\" ) print(\"Context QA Signature Test:\") print(f\"Question: How long is the Great Wall of China?\") print(f\"Answer: {result.answer}\") print() # Test with question not in context result2 = qa( context=context, question=\"What color is the Great Wall?\" ) print(f\"Question: What color is the Great Wall?\") print(f\"Answer: {result2.answer}\") üí° Key Concepts The docstring provides the main instruction. Field descriptions add specific guidance. Notice how we instruct the model to acknowledge when information isn't available. Solution 3 ‚≠ê‚≠ê Intermediate Multi-Output Signature import dspy from typing import Literal from dotenv import load_dotenv load_dotenv() lm = dspy.LM(\"openai/gpt-4o-mini\") dspy.configure(lm=lm) # ============================================ # Email Analysis Signature # ============================================ class EmailAnalysis(dspy.Signature): \"\"\"Analyze an email to extract key information and determine the appropriate response priority.\"\"\" email_text: str = dspy.InputField( desc=\"The full text of the email including sender and body\" ) subject: str = dspy.OutputField( desc=\"Brief summary of what the email is about (5-10 words)\" ) sender_intent: Literal[\"request\", \"inform\", \"complaint\", \"question\", \"update\", \"invitation\"] = dspy.OutputField( desc=\"The primary intent of the sender\" ) urgency: Literal[\"low\", \"medium\", \"high\"] = dspy.OutputField( desc=\"How urgently this email needs attention\" ) action_required: bool = dspy.OutputField( desc=\"Whether the recipient needs to take action\" ) suggested_response: str = dspy.OutputField( desc=\"A brief sug"}, {"title": "Programming vs. Prompting", "url": "chapters/chapter-01/programming-vs-prompting.html", "content": "Chapter 1 ¬∑ Section 3 Programming vs. Prompting The shift from prompting to programming language models is the core innovation of DSPy. ~20 min read üèóÔ∏è The Three-Stage Architecture: DEMONSTRATE-SEARCH-PREDICT At the heart of DSPy lies the three-stage architecture that originated from the Demonstrate-Search-Predict research paper. This architecture provides a systematic way to structure complex reasoning tasks. üéØ DEMONSTRATE Learn from examples and build task understanding # Define what the task does class TaskSignature(dspy.Signature): input_field: str = dspy.InputField() output_field: str = dspy.OutputField() # Examples provide demonstrations trainset = [ dspy.Example(input_field=\"Example 1\", output_field=\"Output 1\"), dspy.Example(input_field=\"Example 2\", output_field=\"Output 2\"), ] üîç SEARCH Retrieve and synthesize information from multiple sources class SearchModule(dspy.Module): def __init__(self): self.retrieve = dspy.Retrieve(k=5) self.select = dspy.Predict(\"documents, query -> relevant_docs\") def forward(self, query): docs = self.retrieve(query).passages return self.select(documents=docs, query=query) üí° PREDICT Generate final outputs based on gathered evidence class PredictModule(dspy.Module): def __init__(self): self.generate = dspy.ChainOfThought(\"context, query -> answer\") def forward(self, context, query): result = self.generate(context=context, query=query) return result.answer Benefits of Three-Stage Architecture Composability: Each stage can be optimized independently Transparency: Clear separation of concerns Flexibility: Different strategies can be swapped Debugging: Issues can be isolated to specific stages üîÑ Key Differences Imperative vs. Declarative Prompting (Imperative) # You tell the model HOW to do it prompt = \"\"\" First, read the context carefully. Then, identify the key information. Next, formulate an answer. Finally, provide your response in one sentence. Context: {context} Question: {question} \"\"\" DSPy (Declarative) # You tell the model WHAT to do class AnswerQuestion(dspy.Signature): \"\"\"Answer questions based on context.\"\"\" context: str = dspy.InputField() question: str = dspy.InputField() answer: str = dspy.OutputField(desc=\"concise answer\") DSPy figures out the HOW! Manual vs. Automatic Prompting: Manual optimization # Try different prompts manually prompts = [ \"Answer: {question}\", \"Provide a clear answer to: {question}\", \"Question: {question}\\nAnswer:\", ] for prompt in prompts: result = test(prompt) # Manual testing DSPy: Automatic optimization # Define your program program = dspy.ChainOfThought(AnswerQuestion) # Optimize automatically optimizer = BootstrapFewShot(metric=accuracy) optimized_program = optimizer.compile(program, trainset=data) Static vs. Composable Prompting: Static, monolithic # One big prompt for the entire task mega_prompt = \"\"\" Step 1: Extract entities from the text Step 2: Classify each entity Step 3: Summarize the entities Step 4: Generate final output Text: {text} \"\"\" DSPy: Modular, composable # Separate, reusable components class Pipeline(dspy.Module): def __init__(self): self.extract = dspy.Predict(\"text -> entities\") self.classify = dspy.Predict(\"entities -> categories\") self.summarize = dspy.Predict(\"categories -> summary\") def forward(self, text): entities = self.extract(text=text).entities categories = self.classify(entities=entities).categories return self.summarize(categories=categories).summary ‚úÖ Benefits of the Programming Paradigm üß© Modularity Break complex tasks into simple components: # Each component is independent and testable extract = dspy.Predict(\"text -> entities\") classify = dspy.Predict(\"entities -> categories\") generate = dspy.Predict(\"categories -> summary\") ‚ôªÔ∏è Reusability Create once, use everywhere: # Define a reusable QA signature class QA(dspy.Signature): context: str = dspy.InputField() question: str = dspy.InputField() answer: str = dspy.OutputField() # Use in different contexts basic_qa = dspy.Predict(QA) reasoning_qa = dspy.ChainOfThought(QA) üß™ Testability Test components independently: def test_entity_extraction(): extractor = dspy.Predict(\"text -> entities\") result = extractor(text=\"Apple released iPhone in 2007\") assert \"Apple\" in result.entities assert \"iPhone\" in result.entities ‚ö° Automatic Optimization Improve systematically: def accuracy_metric(example, prediction): return prediction.answer == example.answer optimizer = BootstrapFewShot(metric=accuracy_metric) optimized = optimizer.compile(MyProgram(), trainset=data) üîß Maintainability Changes are localized and manageable: # Change one signature class ImprovedQA(dspy.Signature): \"\"\"Better QA with sources.\"\"\" context: str = dspy.InputField() question: str = dspy.InputField() answer: str = dspy.OutputField() sources: list[str] = dspy.OutputField() # Added field # All modules using this signature adapt automatically üìä Paradigm Comparison Aspect Prompting Programming (DSPy) Approach Imperative (\"how\") Declarative (\"what\") Optimization Manual trial & error Automatic from dat"}, {"title": "What is DSPy?", "url": "chapters/chapter-01/what-is-dspy.html", "content": "Chapter 1 ¬∑ Section 2 What is DSPy? DSPy is a framework for programming‚Äînot prompting‚Äîfoundation models like GPT-4, Claude, and others. ~15 min read DSPy: Declarative Self-improving Language Programs DSPy provides a systematic way to build LM-based applications that are modular, composable, and automatically optimizable. üìú Historical Context: The Demonstrate-Search-Predict Paper DSPy originated from the groundbreaking research paper \"Demonstrate-Search-Predict: A Paradigm for Solving Complex, Multi-Hop Reasoning Tasks with Large Language Models\" by Omar Khattab and colleagues at Stanford University. The paper demonstrated that complex reasoning tasks could be decomposed into three systematic stages: üéØ DEMONSTRATE Learning from examples and demonstrations üîç SEARCH Retrieving and synthesizing information from multiple sources üí° PREDICT Generating accurate outputs based on gathered evidence This three-stage approach showed that by treating language model tasks as structured programs rather than mere prompts, we could achieve: ‚úÖ Better compositional generalization ‚úÖ More reliable multi-hop reasoning ‚úÖ Systematic optimization through weak supervision ‚úÖ Zero-shot transfer to new tasks ‚ùå The Problem: Manual Prompt Engineering Before understanding DSPy, let's look at the traditional approach to working with LLMs. Traditional Prompt Engineering When you want an LLM to perform a task, you typically write a prompt: import openai # Manual prompt for question answering prompt = \"\"\" You are a knowledgeable assistant. Answer the following question accurately and concisely. Question: What is the capital of France? Provide your answer in a single sentence. \"\"\" response = openai.chat.completions.create( model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": prompt}] ) print(response.choices[0].message.content) This works for simple cases, but scaling this approach leads to significant problems. Problems with Manual Prompting ‚ö†Ô∏è Brittle and Hard to Maintain Small changes can dramatically affect results. There's no systematic way to know which prompt version is best. # Prompt for sentiment analysis sentiment_prompt = \"\"\" Analyze the sentiment of this text and classify it as positive, negative, or neutral. Be careful to consider context and sarcasm. Respond with only the sentiment label. Text: {text} Sentiment: \"\"\" What if the model doesn't follow instructions? How do you handle edge cases? ‚ö†Ô∏è Doesn't Compose Well Chaining multiple steps is manual and error-prone: # Step 1: Summarize summary_prompt = f\"Summarize this: {document}\" summary = call_llm(summary_prompt) # Step 2: Extract entities entity_prompt = f\"Extract entities from: {summary}\" entities = call_llm(entity_prompt) # Step 3: Classify classification_prompt = f\"Classify these entities: {entities}\" result = call_llm(classification_prompt) Error propagation, no systematic optimization, debugging nightmare ‚ö†Ô∏è No Systematic Optimization The only way to improve is trial and error: Try different phrasings manually Add examples by hand Test each variation No guarantee of improvement This is like training a neural network by manually adjusting weights! ‚úÖ The Solution: DSPy DSPy changes the game by letting you program with language models instead of prompting them. Key Idea: Separate What from How Instead of telling the model how to solve a task (via prompts), you tell it what to do (via signatures), and DSPy figures out how. This shift brings determinism and reproducibility to your AI development. Since your logic is defined in code (Python classes), it is version-controllable, testable, and modular‚Äîunlike a giant string of text that might break if you change one word. Traditional prompting (imperative) prompt = \"You are an assistant. Answer questions. Question: {q}\" DSPy (declarative) class QuestionAnswer(dspy.Signature): \"\"\"Answer questions accurately.\"\"\" question: str = dspy.InputField() answer: str = dspy.OutputField() DSPy automatically creates the prompts for you! What DSPy Provides üìù Signatures Task specifications that define what a task does, not how: class Summarize(dspy.Signature): \"\"\"Summarize the given text.\"\"\" document: str = dspy.InputField() summary: str = dspy.OutputField(desc=\"concise summary in 2-3 sentences\") Like a type signature in programming‚Äîspecifies inputs and outputs. üß© Modules Reusable components that use signatures: # Create a summarization module summarizer = dspy.Predict(Summarize) # Use it result = summarizer(document=\"Long text here...\") print(result.summary) Modules can be combined, extended, and optimized. ‚ö° Optimizers Automatically improve your programs: # Optimize automatically from dspy.teleprompt import BootstrapFewShot optimizer = BootstrapFewShot(metric=your_metric) optimized_rag = optimizer.compile(RAGPipeline(), trainset=your_data) DSPy learns better prompts, examples, and module compositions! Core Concepts üìù Signatures Think of signatures as function declarations for LM tasks: # Input -> Output specification class TranslateToFrench(dspy"}, {"title": "Installation & Setup", "url": "chapters/chapter-01/installation-setup.html", "content": "Chapter 1 ¬∑ Section 4 Installation & Setup Prepare your environment for DSPy development with this quick setup guide. ~10 min read ‚úÖ Quick Setup Checklist Ensure you have: Python 3.9 or higher installed Virtual environment created and activated DSPy installed (pip install dspy-ai) LM provider API key configured python-dotenv installed üöÄ Installation 1. Install DSPy With your virtual environment activated: pip install dspy-ai This installs the latest stable version of DSPy. 2. Install Additional Dependencies For the examples in this chapter, we'll use OpenAI and Anthropic clients, plus python-dotenv specifically: pip install openai anthropic python-dotenv What these provide: openai: OpenAI API client (for GPT models) anthropic: Anthropic API client (for Claude models) python-dotenv: Load environment variables from .env files ‚öôÔ∏è Configuration Set Up Your API Key Create a .env file in your project directory: # Create .env file touch .env Add your API key: OPENAI_API_KEY=sk-your-actual-api-key-here Or for Anthropic: ANTHROPIC_API_KEY=your-anthropic-api-key-here üîí Security: Never commit .env files to version control! Add .env to your .gitignore. üîç Verification Let's verify DSPy is installed correctly. Check DSPy Version python -c \"import dspy; print(f'DSPy version: {dspy.__version__}')\" Expected output: DSPy version: 2.5.x (or higher) Quick Test Create a file test_dspy.py: \"\"\"Quick test to verify DSPy installation.\"\"\" import os from dotenv import load_dotenv import dspy # Load environment variables load_dotenv() def main(): print(\"Testing DSPy installation...\") print(f\"DSPy version: {dspy.__version__}\") # Check if API key is available api_key = os.getenv(\"OPENAI_API_KEY\") if api_key: print(\"‚úì API key found\") # Try to configure a language model try: lm = dspy.LM(model=\"openai/gpt-4o-mini\", api_key=api_key) dspy.configure(lm=lm) print(\"‚úì Language model configured successfully\") # Simple test class TestSignature(dspy.Signature): question: str = dspy.InputField() answer: str = dspy.OutputField() predictor = dspy.Predict(TestSignature) result = predictor(question=\"What is 1+1?\") print(f\"‚úì Test prediction: {result.answer}\") print(\"\\n‚úÖ DSPy is working correctly!\") except Exception as e: print(f\"‚úó Error: {e}\") print(\"\\n‚ö†Ô∏è Check your API key and internet connection\") else: print(\"‚úó API key not found\") print(\"Please set OPENAI_API_KEY in your .env file\") if __name__ == \"__main__\": main() Run it: python test_dspy.py Expected output: Testing DSPy installation... DSPy version: 2.5.x ‚úì API key found ‚úì Language model configured successfully ‚úì Test prediction: 2 ‚úÖ DSPy is working correctly! üõ†Ô∏è Troubleshooting ModuleNotFoundError: No module named 'dspy' Ensure your virtual environment is activated and you've run pip install dspy-ai inside it. API key not found Check your .env file format, location, and that load_dotenv() is called. Connection Error Verify your API key is valid (check billing/credits) and you have internet access. üíª IDE Setup (Optional) Visual Studio Code Install recommended extensions: Python (Microsoft) Pylance (Microsoft) Python Indent Configure VS Code to use your virtual environment interpreter. Jupyter Notebook pip install jupyter ipykernel python -m ipykernel install --user --name=dspy-env Continue to Your First DSPy Program"}, {"title": "Chapter 1: DSPy Fundamentals", "url": "chapters/chapter-01/index.html", "content": "Chapter 1 DSPy Fundamentals Welcome to your DSPy journey! This chapter introduces you to DSPy and gets you started building your first LM-powered applications. 6 Sections ~3-4 hours ‚≠ê Beginner üöÄ Welcome to DSPy Your journey from prompt engineering to prompt programming starts here What You'll Learn By the end of this chapter, you will: ‚úÖ Understand what DSPy is and why it matters ‚úÖ Grasp the paradigm shift from prompting to programming ‚úÖ Install and configure DSPy in your development environment ‚úÖ Write and run your first DSPy program ‚úÖ Configure and work with different language models ‚úÖ Build simple question-answering applications Chapter Overview This chapter covers the essential foundations you need to start building with DSPy: üìñ What is DSPy? Learn what DSPy is, why it was created, and how it differs from traditional prompt engineering approaches. üîÑ Programming vs. Prompting Understand the fundamental paradigm shift from manual prompt engineering to programmatic LM pipelines. üíª Your First DSPy Program Write and run a complete DSPy application from scratch. ü§ñ Language Models Learn how to configure and work with different LM providers (OpenAI, Anthropic, local models). üèãÔ∏è Exercises Practice what you've learned with hands-on exercises. What Makes DSPy Different? Before diving in, here's a quick preview of what makes DSPy special: ‚ùå Traditional Prompting # Manual prompt engineering prompt = \"\"\" You are a helpful assistant. Answer the question clearly. Question: What is the capital of France? Answer: \"\"\" response = openai.chat.completions.create( model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": prompt}] ) ‚ö†Ô∏è Brittle and hard to maintain ‚ö†Ô∏è Doesn't compose well ‚ö†Ô∏è Manual tuning required ‚ö†Ô∏è No systematic optimization ‚úÖ DSPy Approach import dspy # Define the task signature class QuestionAnswer(dspy.Signature): \"\"\"Answer questions clearly.\"\"\" question: str = dspy.InputField() answer: str = dspy.OutputField() # Use it with automatic prompting qa = dspy.Predict(QuestionAnswer) response = qa(question=\"What is the capital of France?\") ‚úÖ Clean, modular code ‚úÖ Easy to compose and reuse ‚úÖ Automatically optimizable ‚úÖ Systematic improvement Prerequisites Before starting this chapter, ensure you have: ‚úÖ Python 3.9+ installed ‚úÖ Basic Python knowledge (functions, classes, imports) ‚úÖ Virtual environment set up (from setup instructions) ‚úÖ API key for at least one LM provider ‚úÖ Text editor or IDE ready to use üí° Need help with prerequisites? Review Chapter 0: Prerequisites Learning Approach This chapter uses a hands-on approach: üìö Concepts Clear explanations of core ideas üíª Examples Working code you can run üèãÔ∏è Practice Exercises to reinforce learning üî¨ Experimentation Encouragement to modify and explore üí° Pro Tip: Don't just read‚Äîrun every example and complete the exercises! Estimated Time üìñ Reading 1-1.5 hours üíª Running examples 1 hour üèãÔ∏è Exercises 1-1.5 hours ‚è±Ô∏è Total 3-4 hours Feel free to spread this over multiple sessions! Key Takeaways (Preview) By the end of this chapter, you'll understand: DSPy is a framework for programming (not prompting) LM-based applications Signatures define tasks declaratively using input/output specifications Modules are composable building blocks that can be optimized automatically LMs are configurable and DSPy works with multiple providers Programming > Prompting for building robust, maintainable applications üöÄ Let's Begin! Ready to learn DSPy? Start with understanding what DSPy is and why it matters. Remember: Learning is a journey. Take your time, experiment freely, and don't hesitate to ask questions! Continue to What is DSPy?"}, {"title": "Exercises", "url": "chapters/chapter-01/exercises.html", "content": "Chapter 1 ¬∑ Section 6 Exercises Practice what you've learned in this chapter with these hands-on exercises. ~1-2 hours üìã Exercise Overview Exercise Difficulty Topics Covered Est. Time Exercise 1 ‚≠ê Beginner Installation verification 10-15 min Exercise 2 ‚≠ê Beginner Basic signatures 15-20 min Exercise 3 ‚≠ê‚≠ê Intermediate Language model configuration 20-25 min Exercise 4 ‚≠ê‚≠ê Intermediate Building a Q&A system 30-40 min Exercise 5 ‚≠ê‚≠ê‚≠ê Advanced Multi-step pipeline 45-60 min Exercise 1 ‚≠ê Beginner Verify Your DSPy Installation Objective: Confirm that DSPy is properly installed and configured. Requirements Create a script that: Imports DSPy successfully Prints the DSPy version Checks for an API key in environment variables Creates and configures a language model Runs a simple test prediction Starter Code \"\"\" Exercise 1: Verify DSPy Installation \"\"\" import os from dotenv import load_dotenv import dspy def main(): # TODO: Load environment variables # TODO: Print DSPy version # TODO: Check for API key # TODO: Configure language model # TODO: Run a test prediction pass if __name__ == \"__main__\": main() Expected Output DSPy Installation Check ======================= ‚úì DSPy version: 2.5.x ‚úì API key found ‚úì Language model configured ‚úì Test prediction successful Test question: What is 2+2? Test answer: 4 All checks passed! Exercise 2 ‚≠ê Beginner Create Custom Signatures Objective: Practice creating DSPy signatures for different tasks. Requirements Create signatures for: Translation: Translate English text to Spanish Sentiment Analysis: Classify text as positive, negative, or neutral Summarization: Create a brief summary of text Entity Extraction: Extract named entities from text Starter Code \"\"\" Exercise 2: Create Custom Signatures \"\"\" import dspy # TODO: Create Translation signature class Translate(dspy.Signature): pass # TODO: Create Sentiment Analysis signature class AnalyzeSentiment(dspy.Signature): pass # TODO: Create Summarization signature class Summarize(dspy.Signature): pass # TODO: Create Entity Extraction signature class ExtractEntities(dspy.Signature): pass def test_signatures(): \"\"\"Test each signature\"\"\" # TODO: Test each signature with dspy.Predict pass if __name__ == \"__main__\": test_signatures() Success Criteria ‚òê All four signatures are properly defined ‚òê Each signature has a clear docstring ‚òê Field names are descriptive ‚òê Output fields have helpful descriptions ‚òê All signatures work with dspy.Predict Exercise 3 ‚≠ê‚≠ê Intermediate Configure Multiple Language Models Objective: Learn to configure and switch between different language models. Requirements Create a program that: Configures three different LMs (fast, powerful, local) Defines a simple Q&A signature Tests the same question with all three models Compares the responses Measures and reports response time for each Expected Output Testing Multiple Language Models ================================= Question: Explain quantum computing in simple terms Model: gpt-4o-mini Time: 1.2s Response: Quantum computing uses quantum mechanics to process information... Model: gpt-4o Time: 2.1s Response: Quantum computing is a revolutionary approach that leverages... Summary: -------- Fastest: gpt-4o-mini (1.2s) Most detailed: gpt-4o Exercise 4 ‚≠ê‚≠ê Intermediate Build a Simple Q&A System Objective: Build a complete question-answering system with context. Requirements Create a Q&A system that: Takes a context (paragraph of text) and a question Uses DSPy to answer the question based only on the context Provides a confidence level (high/medium/low) Cites which part of the context was used Handles cases where the answer isn't in the context Test Data test_cases = [ { \"context\": \"Paris is the capital of France. It has a population of about 2.1 million people.\", \"question\": \"What is the capital of France?\" }, { \"context\": \"Python was created by Guido van Rossum and released in 1991.\", \"question\": \"Who created Python?\" }, { \"context\": \"The Great Wall of China is over 13,000 miles long.\", \"question\": \"What is the main programming language used in AI?\" # Not in context! } ] Exercise 5 ‚≠ê‚≠ê‚≠ê Advanced Multi-Step Classification Pipeline Objective: Build a multi-step pipeline that processes text through multiple stages. Requirements Create a pipeline that: Step 1: Extracts the main topic from input text Step 2: Classifies the sentiment (positive/negative/neutral) Step 3: Determines the intended audience (general/technical/academic) Step 4: Generates a summary tailored to the audience Returns all results in a structured format Starter Code \"\"\" Exercise 5: Multi-Step Classification Pipeline \"\"\" import dspy from dotenv import load_dotenv load_dotenv() # TODO: Define signatures for each step class TextAnalysisPipeline(dspy.Module): \"\"\"Multi-step text analysis pipeline.\"\"\" def __init__(self): # TODO: Initialize modules for each step pass def forward(self, text): \"\"\"Process text through the pipeline.\"\"\" # TODO: Implement pipeline logic # TODO: Return structured results pass def test_pipeline(): \"\"\"T"}, {"title": "Solutions", "url": "chapters/chapter-01/solutions.html", "content": "Chapter 1 ¬∑ Section 7 Solutions Complete solutions for all Chapter 1 exercises with detailed explanations. ~30 min read ‚ö†Ô∏è Important: Try to solve the exercises yourself before looking at these solutions. Learning comes from struggle! Solution 1 ‚≠ê Beginner Verify Your DSPy Installation This solution demonstrates proper DSPy setup verification with clear status messages. \"\"\" Exercise 1: Verify DSPy Installation - SOLUTION \"\"\" import os from dotenv import load_dotenv import dspy def main(): print(\"DSPy Installation Check\") print(\"=\" * 23) print() # Step 1: Load environment variables load_dotenv() # Step 2: Print DSPy version try: version = dspy.__version__ print(f\"‚úì DSPy version: {version}\") except AttributeError: print(\"‚úì DSPy imported successfully (version not available)\") # Step 3: Check for API key api_key = os.getenv(\"OPENAI_API_KEY\") if api_key: # Show first/last 4 chars for verification masked = f\"{api_key[:4]}...{api_key[-4:]}\" print(f\"‚úì API key found: {masked}\") else: print(\"‚úó API key not found in environment\") print(\" Set OPENAI_API_KEY in your .env file\") return # Step 4: Configure language model try: lm = dspy.LM(\"openai/gpt-4o-mini\", api_key=api_key) dspy.configure(lm=lm) print(\"‚úì Language model configured\") except Exception as e: print(f\"‚úó Failed to configure LM: {e}\") return # Step 5: Run a test prediction try: qa = dspy.Predict(\"question -> answer\") result = qa(question=\"What is 2+2?\") print(\"‚úì Test prediction successful\") print() print(f\"Test question: What is 2+2?\") print(f\"Test answer: {result.answer}\") except Exception as e: print(f\"‚úó Test prediction failed: {e}\") return print() print(\"All checks passed! ‚ú®\") if __name__ == \"__main__\": main() üí° Key Concepts This solution uses dotenv for environment variable management, proper error handling at each step, and masks the API key for security when displaying. Solution 2 ‚≠ê Beginner Create Custom Signatures This solution shows how to create well-documented DSPy signatures for common NLP tasks. \"\"\" Exercise 2: Create Custom Signatures - SOLUTION \"\"\" import os from dotenv import load_dotenv import dspy load_dotenv() # Translation Signature class Translate(dspy.Signature): \"\"\"Translate English text to Spanish.\"\"\" english_text: str = dspy.InputField( desc=\"The English text to translate\" ) spanish_text: str = dspy.OutputField( desc=\"The Spanish translation of the input text\" ) # Sentiment Analysis Signature class AnalyzeSentiment(dspy.Signature): \"\"\"Classify the sentiment of the given text.\"\"\" text: str = dspy.InputField( desc=\"The text to analyze for sentiment\" ) sentiment: str = dspy.OutputField( desc=\"The sentiment: 'positive', 'negative', or 'neutral'\" ) confidence: str = dspy.OutputField( desc=\"Confidence level: 'high', 'medium', or 'low'\" ) # Summarization Signature class Summarize(dspy.Signature): \"\"\"Create a brief summary of the provided text.\"\"\" text: str = dspy.InputField( desc=\"The text to summarize\" ) summary: str = dspy.OutputField( desc=\"A concise summary in 1-2 sentences\" ) # Entity Extraction Signature class ExtractEntities(dspy.Signature): \"\"\"Extract named entities from text.\"\"\" text: str = dspy.InputField( desc=\"The text to extract entities from\" ) people: str = dspy.OutputField( desc=\"Comma-separated list of person names found\" ) organizations: str = dspy.OutputField( desc=\"Comma-separated list of organization names found\" ) locations: str = dspy.OutputField( desc=\"Comma-separated list of location names found\" ) def test_signatures(): \"\"\"Test each signature with sample inputs.\"\"\" # Configure the LM lm = dspy.LM(\"openai/gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\")) dspy.configure(lm=lm) print(\"Testing Custom Signatures\") print(\"=\" * 40) print() # Test Translation print(\"1. Translation Test\") print(\"-\" * 20) translator = dspy.Predict(Translate) result = translator(english_text=\"Hello, how are you today?\") print(f\" English: Hello, how are you today?\") print(f\" Spanish: {result.spanish_text}\") print() # Test Sentiment Analysis print(\"2. Sentiment Analysis Test\") print(\"-\" * 20) analyzer = dspy.Predict(AnalyzeSentiment) result = analyzer(text=\"I absolutely love this product! Best purchase ever!\") print(f\" Text: I absolutely love this product! Best purchase ever!\") print(f\" Sentiment: {result.sentiment}\") print(f\" Confidence: {result.confidence}\") print() # Test Summarization print(\"3. Summarization Test\") print(\"-\" * 20) summarizer = dspy.Predict(Summarize) long_text = ( \"Machine learning is a subset of artificial intelligence that enables \" \"computers to learn from data without being explicitly programmed. \" \"It uses algorithms to identify patterns in data and make predictions \" \"or decisions based on new, unseen data.\" ) result = summarizer(text=long_text) print(f\" Original: {long_text[:60]}...\") print(f\" Summary: {result.summary}\") print() # Test Entity Extraction print(\"4. Entity Extraction Test\") print(\"-\" * 20) extractor = dspy.Predict(ExtractEntities) result = extractor( text=\"Elon Musk announced that Tesla will open a new fa"}, {"title": "Your First DSPy Program", "url": "chapters/chapter-01/first-program.html", "content": "Chapter 1 ¬∑ Section 4 Your First DSPy Program Let's write your first DSPy program! This hands-on section will walk you through creating a simple question-answering application step by step. ~15 min read üéØ What We'll Build We'll create a program that: Takes a question as input Uses a language model to generate an answer Returns the answer This is the \"Hello World\" of DSPy! üìù The Complete Program Here's the full program. Don't worry if you don't understand everything yet‚Äîwe'll break it down step by step. hello_dspy.py \"\"\" Your First DSPy Program A simple question-answering application \"\"\" import os from dotenv import load_dotenv import dspy # Load environment variables load_dotenv() def main(): # Step 1: Configure the language model lm = dspy.LM( model=\"openai/gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\") ) dspy.configure(lm=lm) # Step 2: Define the task signature class QuestionAnswer(dspy.Signature): \"\"\"Answer questions with factual information.\"\"\" question: str = dspy.InputField() answer: str = dspy.OutputField() # Step 3: Create a predictor qa = dspy.Predict(QuestionAnswer) # Step 4: Use it! question = \"What is the capital of France?\" result = qa(question=question) # Step 5: Display the result print(f\"Question: {question}\") print(f\"Answer: {result.answer}\") if __name__ == \"__main__\": main() üîç Step-by-Step Breakdown ‚öôÔ∏è Configure the Language Model lm = dspy.LM( model=\"openai/gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\") ) dspy.configure(lm=lm) What's happening: dspy.LM() creates a language model instance We specify which model to use (gpt-4o-mini) We provide the API key from environment variables dspy.configure() sets this as the default LM Think of this as: Setting up your \"engine\" that powers all DSPy operations. üìã Define the Task Signature class QuestionAnswer(dspy.Signature): \"\"\"Answer questions with factual information.\"\"\" question: str = dspy.InputField() answer: str = dspy.OutputField() What's happening: We create a class that inherits from dspy.Signature The docstring describes what the task does question is an input field (what we provide) answer is an output field (what we want back) Think of this as: A contract that says \"Give me a question, I'll give you an answer.\" üî® Create a Predictor qa = dspy.Predict(QuestionAnswer) What's happening: dspy.Predict is a module that makes predictions We pass our QuestionAnswer signature to it This creates a predictor that can answer questions Think of this as: Creating a function that implements our contract. ‚ñ∂Ô∏è Use It! question = \"What is the capital of France?\" result = qa(question=question) What's happening: We call our predictor like a function We pass the question as a keyword argument DSPy automatically generates a prompt, calls the LM, and returns the result Think of this as: Just using the function we created! üì§ Display the Result print(f\"Answer: {result.answer}\") What's happening: result is a prediction object We access the answer field (from our signature) DSPy has extracted this from the LM's response Think of this as: Getting the output from our function. ‚ñ∂Ô∏è Running Your Program 1. Save the File Save the code above as hello_dspy.py. 2. Ensure Your Environment is Ready # Activate virtual environment source venv/bin/activate # Check .env file exists with API key cat .env 3. Run It! python hello_dspy.py 4. Expected Output Question: What is the capital of France? Answer: Paris üîÆ What's Happening Behind the Scenes? When you run this program, DSPy: Generates a prompt based on your signature: Answer questions with factual information. --- Question: What is the capital of France? Answer: Calls the language model with this prompt Parses the response and extracts the answer Returns a structured result with the answer field populated üí° You didn't write the prompt‚ÄîDSPy did it for you! üß™ Experimenting Try modifying the program to explore DSPy: Experiment 1: Different Questions questions = [ \"What is the capital of France?\", \"Who invented the telephone?\", \"What is 25 multiplied by 4?\", ] for question in questions: result = qa(question=question) print(f\"Q: {question}\") print(f\"A: {result.answer}\\n\") Experiment 2: Add Field Descriptions class QuestionAnswer(dspy.Signature): \"\"\"Answer questions with factual information.\"\"\" question: str = dspy.InputField() answer: str = dspy.OutputField(desc=\"concise answer in one sentence\") The description helps guide the model's response format! Experiment 3: Multiple Output Fields class DetailedQA(dspy.Signature): \"\"\"Answer questions with details.\"\"\" question: str = dspy.InputField() answer: str = dspy.OutputField() confidence: str = dspy.OutputField(desc=\"high, medium, or low\") explanation: str = dspy.OutputField(desc=\"brief reasoning\") Experiment 4: Use Chain of Thought # Change one line! qa = dspy.ChainOfThought(QuestionAnswer) # Now it shows reasoning result = qa(question=\"What is the capital of France?\") print(f\"Reasoning: {result.rationale}\") print(f\"Answer: {result.answer}\") Just one line change adds step-by-step r"}, {"title": "Language Models", "url": "chapters/chapter-01/language-models.html", "content": "Chapter 1 ¬∑ Section 5 Language Models DSPy works with various language model providers. Learn how to configure different LMs and choose the right model for your task. ~15 min read üîß Configuring Language Models DSPy uses a consistent interface for all language models: import dspy # Create an LM instance lm = dspy.LM(model=\"provider/model-name\", api_key=\"your-key\") # Set it as the default dspy.configure(lm=lm) Once configured, all DSPy modules will use this LM automatically. üåê Supported Providers üí≥ OpenAI Models available: gpt-4o - Latest flagship model gpt-5-mini - Fast, cost-effective gpt-4-turbo - Previous flagship lm = dspy.LM( model=\"openai/gpt-5-mini\", api_key=\"sk-your-key-here\", temperature=0.7, max_tokens=500 ) dspy.configure(lm=lm) Best for: General-purpose tasks, proven reliability ü§ñ Anthropic (Claude) Models available: claude-4-5-sonnet-20241022 - Latest, most capable claude-4-5-haiku-20241022 - Fast, economical claude-4-5-opus-20240229 - Maximum capability lm = dspy.LM( model=\"anthropic/claude-4-5-sonnet-20241022\", api_key=\"your-anthropic-key\", temperature=0.7, max_tokens=1000 ) dspy.configure(lm=lm) Best for: Long contexts, detailed analysis, coding üè† Local Models (Ollama) Models available: llama3, llama3.1 - Meta's open models mistral, mixtral - Mistral AI models phi3 - Microsoft's small model # No API key needed! lm = dspy.LM( model=\"ollama/llama3\", api_base=\"http://localhost:11434\" ) dspy.configure(lm=lm) Best for: Privacy, no API costs, experimentation üå°Ô∏è Temperature Guide Temperature controls output randomness: Value Behavior Use Case 0.0 - 0.3 Deterministic, focused Classification, extraction 0.4 - 0.8 Balanced General Q&A, summaries 0.9 - 1.5 Creative, diverse Creative writing, brainstorming 1.6 - 2.0 Very random Experimental, exploration # For factual tasks - low temperature factual_lm = dspy.LM(model=\"openai/gpt-5-mini\", temperature=0.1) # For creative tasks - higher temperature creative_lm = dspy.LM(model=\"openai/gpt-5-mini\", temperature=1.2) üîÄ Using Multiple Models You can use different models for different tasks: import dspy # Fast model for simple tasks fast_lm = dspy.LM(model=\"openai/gpt-5-mini\") # Powerful model for complex tasks smart_lm = dspy.LM(model=\"openai/gpt-4o\") class Pipeline(dspy.Module): def __init__(self): self.classify = dspy.Predict(\"text -> category\") self.analyze = dspy.ChainOfThought(\"text, category -> analysis\") def forward(self, text): # Use fast model for classification with dspy.context(lm=fast_lm): category = self.classify(text=text).category # Use smart model for complex analysis with dspy.context(lm=smart_lm): analysis = self.analyze(text=text, category=category).analysis return analysis üìä Model Selection Guide By Task Type Classification / Extraction gpt-5-mini or claude-4-5-haiku Question Answering gpt-5-mini or claude-4-5-sonnet Complex Reasoning gpt-4o or claude-4-5-sonnet Long Context grok-4-1-fast (2M tokens), gemini-3-pro-preview (1M tokens) Code Generation gpt-4o or claude-4-5-sonnet By Budget Free / Low Cost Ollama (local), gpt-5-mini Balanced gpt-4o, claude-4-5-sonnet Maximum Capability gemini-3-pro-preview, claude-4-5-opus ‚úÖ Best Practices üìà Start Small, Scale Up # Development: Use small, fast models dev_lm = dspy.LM(model=\"openai/gpt-5-mini\") # Production: Upgrade when needed prod_lm = dspy.LM(model=\"openai/gpt-4o\") # Easy to switch! lm = dev_lm if IS_DEVELOPMENT else prod_lm dspy.configure(lm=lm) üîê Use Environment Variables import os from dotenv import load_dotenv load_dotenv() # Never hardcode API keys! lm = dspy.LM( model=\"openai/gpt-5-mini\", api_key=os.getenv(\"OPENAI_API_KEY\") ) ‚è±Ô∏è Set Appropriate Timeouts # Default timeout might be too short for complex tasks lm = dspy.LM( model=\"openai/gpt-4o\", timeout=60 # 60 seconds for complex reasoning ) üíæ Enable Caching in Development # DSPy has built-in caching dspy.configure(lm=lm, cache=True) # Speeds up development, saves costs üìã Quick Reference OpenAI lm = dspy.LM(model=\"openai/gpt-5-mini\", api_key=key) Anthropic lm = dspy.LM(model=\"anthropic/claude-4-5-sonnet-20241022\", api_key=key) Ollama (Local) lm = dspy.LM(model=\"ollama/llama3\", api_base=\"http://localhost:11434\") Switch Models with dspy.context(lm=different_lm): result = module(input=data) üìù Summary Key Concepts: DSPy supports multiple LM providers (OpenAI, Anthropic, local, etc.) Configure once with dspy.configure(lm=...) Use dspy.context() to temporarily switch models Choose models based on task complexity and budget Start with smaller models, scale up as needed Best Practices: Use environment variables for API keys Set appropriate timeouts and token limits Enable caching during development Choose the right model for each task Continue to Exercises"}, {"title": "Extreme Multi-Label Classification", "url": "chapters/chapter-06/extreme-multilabel-classification.html", "content": "Chapter 6 Extreme Multi-Label Classification Scaling classification systems to handle millions of potential labels using efficient indexing, clustering, and DSPy. Introduction to XML Extreme Multi-Label Classification (XML) involves tagging data with labels from a massive set‚Äîpotentially millions (e.g., Wikipedia tags, Amazon product categories). DSPy, combined with efficient retrieval strategies, makes this tractable. Core Challenges Scale: O(|Labels|) prediction is impossible. Sparsity: Most labels appear rarely (long-tail distribution). Ambiguity: With millions of labels, many will be semantically similar. Architecture 1. Label Embeddings & Indexing Instead of a classifier outputting 1M scores, we embed labels into a vector space and use Approximate Nearest Neighbor (ANN) search (like FAISS) to retrieve candidates. 2. Candidate Selection We first narrow down the millions of labels to a manageable candidate set (e.g., top 50). candidates = label_index.search(query_embedding, k=50) 3. DSPy Reranking/Classification DSPy then acts as a sophisticated reranker or final classifier, looking only at the candidates. class DSPyXMLClassifier(dspy.Module): def __init__(self): self.candidate_selector = dspy.ChainOfThought(\"text, candidates -> selected_labels\") def forward(self, text, candidates): return self.candidate_selector(text=text, candidates=candidates) Hierarchical Strategies Organizing labels into trees (e.g., Cat -> Pet -> Animal) allows you to classify level-by-level, drastically reducing the search space at each step. Next: Long-Form Generation"}, {"title": "Long-Form Article Generation", "url": "chapters/chapter-06/long-form-generation.html", "content": "Chapter 6 Long-Form Generation Building systems that can research, plan, and write coherent, factual, and well-cited long-form articles. Introduction Generating high-quality long-form content (like 5,000-word articles) is fundamentally different from short-form chat. It requires planning, maintaining context across sections, and rigorous fact-checking. System Architecture A robust long-form generation pipeline typically involves: Outline Generator: Breaks the topic into logical sections. Section Generator: Writes one section at a time. Context Manager: Ensures the current section knows what was written before. Citation Manager: Inserts and tracks references. 1. Context Management The ContextManager is crucial for coherence. It feeds a summary of previous sections into the generator for the current section. class ContextManager(dspy.Module): def __init__(self): self.summarize_context = dspy.ChainOfThought(\"previous_sections, current_section -> context_summary\") 2. Section Generation with Citations This module writes the content, explicitly integrating retrieved research data as citations. class SectionGenerator(dspy.Module): def __init__(self): self.generate = dspy.ChainOfThought(\"topic, section_title, research_data -> content, citations\") Quality Assurance Automated QA is essential for long outputs. We can build modules to check for: Factuality: Verifying claims against source documents. Completeness: Ensuring the outline was fully covered. Neutrality: checking for biased language. Next: Outline Generation"}, {"title": "Code Generation", "url": "chapters/chapter-06/code-generation.html", "content": "Chapter 6 ¬∑ Section 7 Code Generation Build automated programming assistants to write, test, debug, and optimize code. ~15 min read Introduction Code generation is a transformative application of language models. DSPy allows you to build sophisticated coding assistants that can not only generate boilerplate but also implement complex algorithms, integrate APIs, write tests, and debug errors. Understanding Code Generation Common Usecases Boilerplate Generation: Quickly scrambling standard project structures. Algorithm Implementation: Translating requirements into logic. Test Generation: Creating unit tests for better coverage. Refactoring & Optimization: Improving code quality and performance. DSPy Code Generator A basic module for generating code from requirements: Python Copy import dspy class CodeGenerator(dspy.Module): def __init__(self, language=\"python\"): super().__init__() self.language = language self.generate_code = dspy.Predict( f\"requirement, language[{language}] -> code, explanation\" ) self.validate_syntax = dspy.Predict( f\"code, language[{language}] -> syntax_valid, syntax_errors\" ) def forward(self, requirement): # Generate code generation = self.generate_code( requirement=requirement, language=self.language ) # Validate syntax (basic check) validation = self.validate_syntax( code=generation.code, language=self.language ) return dspy.Prediction( code=generation.code, explanation=generation.explanation, syntax_valid=validation.syntax_valid ) Specialized Applications Unit Test Generator Generating robust test suites automatically: Python Copy class UnitTestGenerator(dspy.Module): def __init__(self): super().__init__() self.analyze_function = dspy.Predict( \"function_code -> signature, edge_cases\" ) self.generate_tests = dspy.ChainOfThought( \"function_info, edge_cases -> test_cases, assertions\" ) def forward(self, function_code): analysis = self.analyze_function(function_code=function_code) tests = self.generate_tests( function_info=analysis.signature, edge_cases=analysis.edge_cases ) return dspy.Prediction( test_code=tests.test_cases, assertions=tests.assertions ) Code Refactoring Assistant Improving code quality and maintainability: Python Copy class CodeRefactoringAssistant(dspy.Module): def __init__(self): super().__init__() self.analyze_quality = dspy.Predict( \"code -> issues, suggestions\" ) self.refactor = dspy.ChainOfThought( \"original_code, issues, suggestions -> refactored_code\" ) def forward(self, code): analysis = self.analyze_quality(code=code) refactored = self.refactor( original_code=code, issues=analysis.issues, suggestions=analysis.suggestions ) return dspy.Prediction( refactored_code=refactored.refactored_code, improvements=analysis.suggestions ) Best Practices Context Awareness: Provide existing code context to generators for better integration. Validation: Always validate generated code syntax and functionality where possible. Security: Scan generated code for potential vulnerabilities. Optimization: Use DSPy optimizers to fine-tune the prompts for specific coding standards or languages. Next: Perspective-Driven Research"}, {"title": "Entity Extraction", "url": "chapters/chapter-06/entity-extraction.html", "content": "Chapter 6 ¬∑ Section 5 Entity Extraction Mine structured information from unstructured text with precision. ~15 min read Introduction Entity extraction, or Named Entity Recognition (NER), is pivotal for transforming unstructured text into structured data. It underpins applications like resume parsing, contract analysis, and medical record processing. DSPy provides the tools to build sophisticated extraction systems that handle real-world complexity. Understanding Entity Extraction Common Entity Types Person: John Smith, Dr. Sarah Johnson Organization: Google, Microsoft, Stanford University Location: New York, 123 Main Street Date/Time: January 15, 2024, 3:30 PM Money: $50,000, ‚Ç¨1.2 million Real-World Applications Resume Processing: Extract skills, experience, and education. Contract Analysis: Identify parties, dates, and clauses. Medical Records: Extract diagnoses, medications, and procedures. Building Entity Extractors Basic Entity Extractor A simple module to extract specified entity types: Python Copy import dspy class BasicEntityExtractor(dspy.Module): def __init__(self, entity_types): super().__init__() self.entity_types = entity_types types_str = \", \".join(entity_types) self.extract = dspy.Predict( f\"text, entity_types[{types_str}] -> entities\" ) def forward(self, text): result = self.extract( text=text, entity_types=\", \".join(self.entity_types) ) # Assume result.entities is parsed into a structured list return dspy.Prediction( entities=result.entities, raw_output=result.entities ) Advanced Entity Extractor with Context Handling context, validation, and disambiguation: Python Copy class AdvancedEntityExtractor(dspy.Module): def __init__(self, entity_types, context_window=100): super().__init__() self.entity_types = entity_types types_str = \", \".join(entity_types) self.find_entities = dspy.ChainOfThought( f\"text, context, entity_types[{types_str}] -> entities_with_positions\" ) self.validate_entities = dspy.Predict( \"entity, text_context -> is_valid, corrected_entity, confidence\" ) self.disambiguate = dspy.Predict( \"entity, context, possible_meanings -> disambiguated_entity, reasoning\" ) def forward(self, text, document_context=None): context = document_context[-100:] if document_context else text # Find entities extraction = self.find_entities( text=text, context=context, entity_types=\", \".join(self.entity_types) ) # Logic to parse, validate, and disambiguate entities would follow... # Returns structured entities list return dspy.Prediction( entities=extraction.entities_with_positions, extraction_reasoning=extraction.rationale ) Specialized Applications Resume/CV Parser Extracting structured data from resumes: Python Copy class ResumeParser(dspy.Module): def __init__(self): super().__init__() self.contact_info = dspy.Predict( \"resume_text -> name, email, phone, location, linkedin\" ) self.extract_sections = dspy.Predict( \"resume_text -> work_experience, education, skills, certifications\" ) self.parse_experience = dspy.ChainOfThought( \"experience_section -> detailed_experiences\" ) def forward(self, resume_text): contact = self.contact_info(resume_text=resume_text) sections = self.extract_sections(resume_text=resume_text) # Detailed parsing logic... return dspy.Prediction( contact_info=contact, sections=sections ) Best Practices Ambiguity: Use context to disambiguate entities (e.g., \"Apple\" fruit vs. company). Validation: Use regex or rules to validate specific types like emails or dates. Nested Entities: Handle overlapping entities carefully (e.g., \"University of [California]\"). Optimization: Use BootstrapFewShot with a custom F1 metric to improve extraction performance. Next: Intelligent Agents"}, {"title": "Multi-hop Search", "url": "chapters/chapter-06/multi-hop-search.html", "content": "Chapter 6 ¬∑ Section 3 Multi-hop Search Master complex reasoning across documents given dispersed information. ~20 min read Introduction Many real-world questions cannot be answered with a single document retrieval. They require multi-hop reasoning‚Äîfinding information from multiple sources and connecting the dots to arrive at a comprehensive answer. Multi-hop search systems excel at answering complex questions that involve relationships, comparisons, and synthesizing information across different documents. üí° Key Concept: Multi-hop reasoning mimics how humans research complex topics: we find one piece of information, which leads us to search for another, and finally we combine them to form an answer. Understanding Multi-hop Search What is Multi-hop Reasoning? Multi-hop reasoning generally involves three stages: First hop: Initial information retrieval based on the original question. Intermediate hops: Finding related information based on the results of previous steps. Final hop: Synthesizing all gathered information to answer the original question comprehensively. Example Scenarios Comparative Questions: \"How does the cost of living in San Francisco compare to Austin?\" (Requires finding data for SF, then Austin, then comparing). Chain Questions: \"Which companies did the founders of Google work for before starting Google?\" (Find founders, then find their previous employment history). Aggregation Questions: \"What is the total market cap of all tech companies founded after 2010?\" (Find companies founded >2010, find their market caps, sum them up). Causal Questions: \"What factors led to the 2008 financial crisis and how did it affect the housing market?\" (Find causes, then trace effects on housing). Building Multi-hop Systems Basic Multi-hop Architecture Here is a classic implementation of a multi-hop search module in DSPy using a loop to iteratively retrieve information: Python Copy import dspy class MultiHopSearch(dspy.Module): def __init__(self, max_hops=3): super().__init__() self.max_hops = max_hops self.retrieve = dspy.Retrieve(k=5) self.generate_query = dspy.Predict(\"question, context -> next_query\") self.generate_answer = dspy.ChainOfThought(\"question, all_contexts -> answer\") def forward(self, question): all_contexts = [] current_question = question for hop in range(self.max_hops): # Retrieve documents for current query retrieved = self.retrieve(question=current_question) contexts = retrieved.passages all_contexts.extend(contexts) # Generate next query based on retrieved information next_query_result = self.generate_query( question=question, context=\"\\n\".join(contexts) ) # Check if we have enough information (heuristic) if \"sufficient\" in next_query_result.next_query.lower(): break current_question = next_query_result.next_query # Generate final answer using all retrieved contexts final_answer = self.generate_answer( question=question, all_contexts=\"\\n\\n\".join(all_contexts) ) return dspy.Prediction( answer=final_answer.answer, contexts=all_contexts, hops=hop + 1, reasoning=final_answer.rationale ) Advanced Multi-hop with Question Decomposition Instead of sequential hopping, you can decompose a complex question into simpler sub-questions first: Python Copy class DecomposedMultiHop(dspy.Module): def __init__(self): super().__init__() self.decompose = dspy.Predict(\"question -> subquestions\") self.retrieve = dspy.Retrieve(k=5) self.answer_subquestion = dspy.Predict(\"subquestion, context -> subanswer\") self.synthesize = dspy.ChainOfThought(\"question, subanswers -> final_answer\") def forward(self, question): # Decompose the complex question decomposition = self.decompose(question=question) subquestions = decomposition.subquestions.split(\";\") subanswers = [] # Answer each subquestion for subq in subquestions: subq = subq.strip() if subq: # Retrieve relevant context context = self.retrieve(question=subq).passages # Answer subquestion subanswer = self.answer_subquestion( subquestion=subq, context=\"\\n\".join(context) ) subanswers.append({ \"subquestion\": subq, \"answer\": subanswer.subanswer, \"context\": context }) # Synthesize final answer subanswers_text = \"\\n\".join([ f\"Q: {sa['subquestion']}\\nA: {sa['answer']}\" for sa in subanswers ]) synthesis = self.synthesize( question=question, subanswers=subanswers_text ) return dspy.Prediction( answer=synthesis.answer, subquestions=subquestions, subanswers=subanswers, reasoning=synthesis.rationale ) Graph-based Multi-hop Search For knowledge-intensive tasks, treating information as a graph of connected entities can be powerful. Python Copy class GraphMultiHop(dspy.Module): def __init__(self): super().__init__() self.retrieve = dspy.Retrieve(k=5) self.extract_entities = dspy.Predict(\"text -> entities\") self.find_connections = dspy.Predict(\"entities, question -> search_queries\") self.traverse_graph = dspy.ChainOfThought(\"question, entities, paths -> answer\") def forward(self, question): visited_entities = set() all_paths = [] # Initial retrieval initial_docs = self.retrieve(ques"}, {"title": "GraphRAG with TiDB", "url": "chapters/chapter-06/graphrag-wikipedia-tidb-tutorial.html", "content": "Chapter 6 GraphRAG from Scratch Build a complete Graph-based Retrieval-Augmented Generation system using DSPy, extracting knowledge from Wikipedia and storing it in TiDB Vector. Introduction Standard RAG retrieves isolated chunks of text. GraphRAG retrieves a structured graph of interconnected entities, enabling the AI to \"reason\" over multi-hop relationships (e.g., \"Company A bought Company B which owns Product C\"). In this tutorial, we will: Ingest Wikipedia data. Extract entities and relationships using a DSPy module. Store the knowledge graph in TiDB Serverless (which supports vector search). Query the graph to answer complex questions. The Extraction Pipeline We define a multi-stage DSPy module to process raw text into a structured graph. class KnowledgeGraphExtractor(dspy.Module): def __init__(self): self.entity_extractor = dspy.ChainOfThought(\"text -> entities\") self.rel_extractor = dspy.ChainOfThought(\"text, entities -> relationships\") def forward(self, text): # 1. Find entities (people, orgs, places) entities = self.entity_extractor(text=text) # 2. Find how they relate rels = self.rel_extractor(text=text, entities=entities) return {'entities': entities, 'relationships': rels} Storage Layer (TiDB) We use TiDB with vector support to store entity embeddings. This allows us to perform hybrid searches: looking up entities by name (SQL) or by semantic meaning (Vector). Why GraphRAG? Compared to standard vector-only RAG, this approach offers: 81% higher relationship accuracy on complex queries. 63% reduction in hallucinations, as answers are grounded in explicit graph edges. Multi-hop reasoning capabilities that flat vector search cannot match. Next: Framework Comparisons"}, {"title": "RAG Systems", "url": "chapters/chapter-06/rag-systems.html", "content": "Chapter 6 ¬∑ Section 2 RAG Systems Building intelligent document Q&A pipelines. ~45 min read What is RAG? Retrieval-Augmented Generation (RAG) systems combine the strengths of information retrieval with language generation to answer questions based on large collections of documents. QUESTION RETRIEVE GENERATE ANSWER Building a Basic RAG System In DSPy, a RAG system is a Module that uses a dspy.Retrieve component alongside a generation component (dspy.Predict or dspy.ChainOfThought). import dspy class BasicRAG(dspy.Module): def __init__(self, num_passages=5): super().__init__() # 1. Define Retrieval Component self.retrieve = dspy.Retrieve(k=num_passages) # 2. Define Generation Component self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\") def forward(self, question): # 1. Retrieve Phase context = self.retrieve(question).passages # 2. Generation Phase prediction = self.generate_answer(context=context, question=question) return dspy.Prediction( context=context, answer=prediction.answer ) Advanced RAG Techniques Real-world questions often require more than simple keyword search. Here are two powerful patterns. 1. Multi-Stage RAG Perform broad retrieval first, then re-rank or filter to find the most precise information. class AdvancedRAG(dspy.Module): def __init__(self): super().__init__() self.retrieve = dspy.Retrieve(k=10) self.rerank = dspy.Predict(\"query, documents -> ranked_documents\") self.generate = dspy.ChainOfThought(\"question, context -> answer\") def forward(self, question): # 1. Broad Retrieval (Get 10 docs) initial_docs = self.retrieve(question).passages # 2. Re-rank (Filter to top 3) reranked = self.rerank( query=question, documents=initial_docs ) top_context = reranked.ranked_documents[:3] # 3. Generate Answer return self.generate(question=question, context=top_context) 2. Query Expansion Sometimes the user's question isn't the best search query. Use an LLM step to improve the search terms. class QueryExpansionRAG(dspy.Module): def __init__(self): super().__init__() self.expand_query = dspy.ChainOfThought(\"question -> search_terms\") self.retrieve = dspy.Retrieve(k=5) self.generate = dspy.ChainOfThought(\"context, question -> answer\") def forward(self, question): # 1. Expand Query search_terms = self.expand_query(question=question).search_terms # 2. Retrieve using optimized terms context = self.retrieve(search_terms).passages # 3. Generate return self.generate(context=context, question=question) Optimizing RAG Systems RAG pipelines are excellent candidates for DSPy optimization because prompts for \"retrieval query generation\" and \"answering based on context\" are hard to hand-tune. ‚ú® Optimization Tip: Use MIPRO if your RAG pipeline has multiple steps (like query expansion + re-ranking + generation), as it can optimize prompts across the entire chain simultaneously. # Define a metric that checks if the answer is grounded in the retrieved context def rag_metric(example, pred, trace=None): # Check if answer is correct AND supported by context return \"correct\" in pred.answer and has_citations(pred.answer) # Compile optimizer = dspy.MIPRO(metric=rag_metric) compiled_rag = optimizer.compile(AdvancedRAG(), trainset=trainset) Next: Multi-Hop Search"}, {"title": "Extreme Few-Shot Learning", "url": "chapters/chapter-06/extreme-few-shot-learning.html", "content": "Chapter 6 Extreme Few-Shot Learning Achieving high performance with as few as 10 labeled examples using specialized optimization and data amplification strategies. The \"10 Examples\" Challenge In many real-world scenarios, you don't have thousands of labeled examples. You might have ten. Extreme few-shot learning focuses on squeezing every drop of signal from this tiny dataset. Core Strategies Prompt Optimization: Tuning the prompt instruction is often more effective than fine-tuning weights on small data. Data Amplification: Generating synthetic variations (paraphrases, counterfactuals) to expand the training set. Bootstrap Few-Shot: Using the model itself to generate demonstrations, filtering for high-confidence outputs. Implementation Example Here's how you might set up a trainer for this scenario: class ExtremeFewShotTrainer: def train_with_10_examples(self, signature, examples): # 1. Analyze input/output patterns in the 10 examples analysis = self._analyze_examples(examples) # 2. Optimize prompt using those insights final_program = self._prompt_optimization_training(signature, analysis) # 3. Compile with BootstrapFewShot (using limited demos) optimizer = dspy.BootstrapFewShot(metric=my_metric, max_bootstrapped_demos=3) return optimizer.compile(final_program, trainset=examples) Best Practices When data is this scarce, quality control is paramount. Ensure your 10 examples are diverse, perfectly accurate, and cover edge cases. Rely heavily on Chain of Thought to guide the model's reasoning, as it can compensate for the lack of training data. Next: IR Model Training"}, {"title": "Outline Generation", "url": "chapters/chapter-06/outline-generation.html", "content": "Chapter 6 Outline Generation Transforming scattered research findings into cohesive, hierarchically structured article outlines using DSPy. Introduction An outline is the skeleton of any good article. In automated writing systems, the outline generator acts as the architect, converting unstructured research notes into a blueprint that guides the writing process. The Generation Process We break down outline generation into four distinct steps: Research Analysis: Identify key themes and cluster findings. Structure Planning: Determine the best hierarchy (e.g., chronological vs. thematic). Drafting: Create the initial section and subsection hierarchy. Optimization: Refine for flow, balance, and completeness. 1. Research Analyzer First, we group research findings into clusters to see what major topics emerge. class ResearchAnalyzer(dspy.Module): def __init__(self): self.identify_themes = dspy.ChainOfThought(\"research_data, topic -> themes\") self.cluster_findings = dspy.Predict(\"findings, themes -> clusters\") 2. Outline Planner Then, we use those clusters to build the actual outline tree. class OutlinePlanner(dspy.Module): def __init__(self): self.create_hierarchy = dspy.Predict(\"main_sections, clusters -> hierarchical_outline\") Outline Optimization Just like a human writer, the model creates a draft and then critiques it. The OutlineOptimizer checks if the flow is logical, if any research was ignored, or if sections are redundant, and then rewrites the structure accordingly. Next: Extreme Few-Shot Learning"}, {"title": "Classification Tasks", "url": "chapters/chapter-06/classification-tasks.html", "content": "Chapter 6 ¬∑ Section 4 Classification Tasks Build robust systems for categorize text, from simple binary classifiers to complex hierarchical models. ~15 min read Introduction Text classification focuses on automatically assigning categories to text. It powers diverse applications ranging from spam detection to intent recognition. DSPy simplifies building sophisticated classifiers that handle the complexity and nuances of real-world data effectively. Understanding Text Classification Classification Types Binary Classification: Two classes (e.g., spam vs. not spam, positive vs. negative). Multi-class Classification: Multiple exclusive categories (e.g., news topics: sports, politics, tech). Multi-label Classification: Multiple non-exclusive categories (e.g., tagging a blog post with \"AI\" and \"Tutorial\"). Hierarchical Classification: Nested categories (e.g., Product > Electronics > Smartphones). Real-World Applications Content Moderation: Detecting inappropriate or harmful content. Customer Support: Routing tickets to appropriate departments (Billing, Tech Support). Market Analysis: Categorizing news and sentiment in social media. Intent Recognition: Understanding user goals in chatbots. Building Basic Classifiers Simple Binary Classifier A basic module to classify text into two opposing categories: Python Copy import dspy class BinaryClassifier(dspy.Module): def __init__(self, positive_class=\"positive\", negative_class=\"negative\"): super().__init__() self.positive_class = positive_class self.negative_class = negative_class self.classify = dspy.Predict(\"text -> classification, confidence\") def forward(self, text): result = self.classify(text=text) # Normalize classification classification = result.classification.lower() if self.positive_class in classification: label = self.positive_class elif self.negative_class in classification: label = self.negative_class else: # Fallback based on confidence label = self.positive_class if float(result.confidence) > 0.5 else self.negative_class return dspy.Prediction( classification=label, confidence=result.confidence, raw_prediction=result.classification ) Multi-class Classifier For selecting one category from many: Python Copy class MultiClassClassifier(dspy.Module): def __init__(self, categories): super().__init__() self.categories = categories categories_str = \", \".join(categories) self.classify = dspy.Predict( f\"text, categories[{categories_str}] -> classification, confidence, reasoning\" ) def forward(self, text): result = self.classify(text=text, categories=\", \".join(self.categories)) # Ensure classification is capable of matching a known category if result.classification not in self.categories: # Logic to find closest category match would go here pass return dspy.Prediction( classification=result.classification, confidence=result.confidence, reasoning=result.reasoning ) Advanced Classification Techniques Hierarchical Classification Classifying into a taxonomy level by level: Python Copy class HierarchicalClassifier(dspy.Module): def __init__(self, hierarchy): super().__init__() self.hierarchy = hierarchy self.root_categories = list(hierarchy.keys()) # Level 1: Root classifier self.root_classifier = dspy.Predict( f\"text, root_categories[{', '.join(self.root_categories)}] -> root_category\" ) # Level 2: Sub-category classifiers self.sub_classifiers = {} for root, subs in hierarchy.items(): subs_str = \", \".join(subs) self.sub_classifiers[root] = dspy.Predict( f\"text, sub_categories[{subs_str}] -> sub_category\" ) def forward(self, text): # First level classification root_result = self.root_classifier( text=text, root_categories=\", \".join(self.root_categories) ) root_category = root_result.root_category # Handle matching logic... # Second level classification if root_category in self.sub_classifiers: sub_result = self.sub_classifiers[root_category]( text=text, sub_categories=\", \".join(self.hierarchy[root_category]) ) sub_category = sub_result.sub_category else: sub_category = \"Unknown\" return dspy.Prediction( root_category=root_category, sub_category=sub_category, full_path=f\"{root_category} > {sub_category}\" ) Confidence-aware Classification Handling uncertain predictions gracefully: Python Copy class ConfidenceClassifier(dspy.Module): def __init__(self, categories, confidence_threshold=0.7): super().__init__() self.categories = categories self.confidence_threshold = confidence_threshold self.classify = dspy.Predict( f\"text, categories[{', '.join(categories)}] -> classification, confidence, uncertainty_analysis\" ) self.request_clarification = dspy.Predict(\"text, uncertainty -> clarification_question\") def forward(self, text): result = self.classify(text=text, categories=\", \".join(self.categories)) confidence = float(result.confidence) if confidence < self.confidence_threshold: clarification = self.request_clarification( text=text, uncertainty=result.uncertainty_analysis ) return dspy.Prediction( classification=\"UNCERTAIN\", confidence=confidence, clarification_needed=clarific"}, {"title": "Perspective-Driven Research", "url": "chapters/chapter-06/perspective-driven-research.html", "content": "Chapter 6 Perspective-Driven Research Simulating human research methodologies by exploring topics from multiple, diverse viewpoints to create comprehensive knowledge foundations. Introduction Single-perspective research often leads to biased or incomplete coverage. Perspective-driven research addresses this by systematically exploring a topic from multiple angles‚Äîsuch as historical, economic, ethical, or technical viewpoints. This approach simulates the curiosity-driven exploration of a thorough human researcher. System Architecture A complete perspective-driven research system consists of three main stages: Perspective Generation: Identifying the most relevant lenses through which to view the topic. Guided Questioning: Formulating specific questions for each perspective. Multi-Perspective Retrieval: Gathering and synthesizing information for each viewpoint. 1. Generating Perspectives First, we define a module to brainstorm diverse perspectives. class PerspectiveGenerator(dspy.Module): def __init__(self): super().__init__() self.generate_perspectives = dspy.ChainOfThought(\"topic -> perspectives, rationale\") def forward(self, topic): return self.generate_perspectives(topic=topic) 2. Perspective-Guided Questioning Once perspectives are defined, the system generates targeted questions to drive the search. class PerspectiveQuestionGenerator(dspy.Module): def __init__(self): super().__init__() self.generate_questions = dspy.ChainOfThought(\"topic, perspective -> focused_questions\") Advanced Features Dynamic Perspective Expansion A sophisticated system can dynamically identify gaps in its current understanding and spawn new perspectives on the fly. self.identify_gaps = dspy.ChainOfThought( \"topic, current_perspectives -> missing_perspectives\" ) Cross-Perspective Synthesis Finally, the system must integrate findings, resolving conflicts and highlighting connections between different viewpoints. Practical Example Applying this to a topic like \"The Impact of Social Media on Mental Health\" might yield perspectives such as: Psychological: Effects on self-esteem and anxiety. Sociological: Changes in community dynamics and communication. Technological: Algorithm design and addictive patterns. Each leads to distinct search queries and a richer final summary. Next: Extreme Multilabel Classification"}, {"title": "Framework Comparisons (DSPy vs. LangChain)", "url": "chapters/chapter-06/framework-comparisons-dspy-ecosystem.html", "content": "Chapter 6 Framework Comparisons A deep dive into the differences between DSPy and LangChain. Understand when to use \"Programming over Prompting\" versus \"Workflow Orchestration\". DSPy vs. LangChain The choice between DSPy and LangChain often comes down to philosophy: LangChain focuses on orchestration. It connects LLMs to thousands of tools, APIs, and data sources. It is great for building agents that need to browse the web or query databases out-of-the-box. DSPy focuses on optimization. It treats prompts as compiled bytecode. It is great for building high-quality pipelines where accuracy and reliability are paramount. When to Choose What Scenario Recommended Framework Rapid Prototyping LangChain (due to rich tooling) Complex Reasoning DSPy (automatic optimization) Production Optimization DSPy (metric-driven improvement) Data Integrations LangChain (lots of loaders) Hybrid Architecture You don't have to choose! A powerful pattern is to use LangChain for data loading and DSPy for core logic. # 1. Use LangChain to load data loader = PyPDFLoader(\"data.pdf\") docs = loader.load() # 2. Use DSPy to process it intelligently class Summarizer(dspy.Module): # ... optimized logic ... optimizer = dspy.BootstrapFewShot(...) program = optimizer.compile(Summarizer(), trainset=docs) Next: Multi-Agent RAG Systems"}, {"title": "Multi-Agent RAG Systems", "url": "chapters/chapter-06/multi-agent-rag-systems.html", "content": "Chapter 6 Multi-Agent RAG Systems Designing hierarchical, multi-expert systems where specialized agents collaborate to solve complex, multi-domain problems. The Power of Specialization Single-agent systems often struggle with broad knowledge domains, like medicine, where deep expertise is required in multiple sub-fields (e.g., Cardiology vs. Endocrinology). Multi-Agent RAG solves this by creating a team of specialists. Architecture A typical setup involves: Lead Agent (Orchestrator): Receives the user query and delegates it to the correct expert(s). Expert Agents: Specialized agents with their own dedicated vector stores (e.g., a \"Diabetes Expert\" with access to diabetes research papers). Tool-Based Communication: Agents interact via defined tool signatures. class MultiAgentSystem(dspy.Module): def __init__(self): self.lead_agent = dspy.ReAct( signature=MedicalSignature, tools=[consult_diabetes_expert, consult_cardio_expert] ) def forward(self, question): return self.lead_agent(question=question) Optimization with GEPA We can optimize this entire system using GEPA (Generative Evolving Prompt Architecture). This advanced optimizer uses a \"Teacher\" LLM to critique the performance of individual agents (Student LLMs) and iteratively refine their instructions, leading to significant performance gains (+8% accuracy in medical benchmarks). Next: Exercises"}, {"title": "Retrieval Augmented Guardrails", "url": "chapters/chapter-06/retrieval-augmented-guardrails.html", "content": "Chapter 6 Retrieval Augmented Guardrails Enhancing AI safety in high-stakes domains by evaluating outputs against retrieved historical contexts and similar cases. Why Retrieval for Guardrails? Standard AI guardrails often check outputs against static rules (\"Do not mention X\"). In complex fields like healthcare, this isn't enough. An answer might be safe in one context but dangerous in another. Retrieval-Augmented Guardrails improve safety by retrieving similar past cases (e.g., previous patient messages and clinician responses) to use as a \"ground truth\" reference for evaluating the current output. The Evaluation Pipeline This system acts as a sophisticated judge: Retrieve: Find historically similar interactions. Compare: Use an ErrorClassifier module to check if the new AI response deviates from the clinical standards established in those historical examples. Assess Severity: If an error is found, determine if it's a minor tone issue or a critical safety risk. Implementation class RetrievalAugmentedEvaluator(dspy.Module): def forward(self, patient_message, ai_response): # 1. Get context similar_cases = self.retriever(query=patient_message, k=3) # 2. Check for errors against that context classification = self.error_classifier( message=patient_message, response=ai_response, context=similar_cases ) return classification Performance Gains Adding retrieval to the guardrail system resulted in: 50% higher concordance with human reviewers compared to non-retrieval baselines. 42% better identification of safety concerns. Next: GraphRAG Tutorial"}, {"title": "IR Model Training from Scratch", "url": "chapters/chapter-06/ir-model-training-scratch.html", "content": "Chapter 6 IR Model Training from Scratch A comprehensive guide to building, training, and optimizing Information Retrieval models from scratch using DSPy, even with minimal training data. Introduction Information Retrieval (IR) is the science of finding relevant material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections. Traditionally, training effective IR models like dense retrievers requires massive datasets of query-document pairs. DSPy changes this equation, allowing us to bootstrap effective IR systems with minimal signals. The Zero-to-IR Framework We can break down the process of training an IR model from scratch using minimal data into four phases: Initialization: Selecting the architecture (Sparse, Dense, Hybrid). Data Processing: Converting raw text and limited relevance judgments into training examples. Training Strategy: Applying prompt optimization or meta-learning. Calibration: Post-processing scores for better ranking. Example: Dense Retriever Component A dense retriever maps queries and documents to a shared vector space. class DenseRetriever(dspy.Module): def __init__(self): self.query_encoder = dspy.Predict(\"query -> query_vector\") self.document_encoder = dspy.Predict(\"document -> document_vector\") def forward(self, query, document): q_vec = self.query_encoder(query=query).query_vector d_vec = self.document_encoder(document=document).document_vector return self.calculate_similarity(q_vec, d_vec) Advanced Techniques Self-Supervised Pre-training: Generate synthetic queries for documents to create a massive \"fake\" training set before using real labels. Active Learning: Iteratively select the most confusing query-document pairs for human annotation to maximize data efficiency. Cross-Lingual Transfer: Use models trained on high-resource languages to bootstrap retrieval in low-resource languages. Next: LingVarBench Healthcare"}, {"title": "Real-World Applications", "url": "chapters/chapter-06/index.html", "content": "Chapter 6 Real-World Applications Moving from theory to practice: Building RAG systems, Agents, and Production Pipelines. ~2 hours read Introduction In the previous chapters, we mastered the core building blocks of DSPy: Signatures, Modules, Metrics, and Optimizers. Now, we apply these tools to solve complex, real-world problems. This chapter focuses on the most common and high-value applications of Large Language Models, starting with Retrieval-Augmented Generation (RAG) and moving into autonomous agents, precise classification tasks, and specialized research workflows. Learning Objectives Build Robust RAG Systems: Create pipelines that can search, filter, and synthesize information from large document bases. Master Multi-Hop Reasoning: Design systems that can answer complex questions requiring information from multiple disparate sources. Develop Intelligent Agents: Create agents that can use tools and plan dynamic sequences of actions. Implement Complex Classification: Handle difficult categorization tasks with high precision using DSPy optimizers. Explore Advanced Architectures: Learn about Multi-Agent RAG, GraphRAG, and Perspective-Driven Research. Chapter Roadmap 01 RAG Systems The foundation of modern LLM apps: combining search with generation. 02 Multi-Hop Search Solving complex queries that require multiple steps of gathering info. 03 Intelligent Agents Building autonomous systems that can use tools and make decisions. 04 Advanced Systems Exploring Multi-Agent architectures, GraphRAG, and more. Next: RAG Systems"}, {"title": "Exercises", "url": "chapters/chapter-06/exercises.html", "content": "Chapter 6 ¬∑ Section 8 Exercises Apply your knowledge to build comprehensive real-world applications with DSPy. ~30 min read Overview These exercises provide hands-on practice building complete, production-ready applications with DSPy. You'll work with all the concepts learned in previous chapters‚Äîsignatures, modules, evaluation, and optimization‚Äîto solve real-world problems. Exercise 1: Customer Support RAG System Objective: Create a complete RAG system for customer support that can answer questions about product documentation and policies. Tasks Complete the CustomerSupportRAG class implementation. Add modules for question understanding and answer generation. Implement the create_support_rag function. Create training data and optimize with BootstrapFewShot. Test with support-related questions. Python Copy import dspy class CustomerSupportRAG(dspy.Module): def __init__(self): super().__init__() self.retrieve = dspy.Retrieve(k=5) # TODO: Add modules def forward(self, question): # TODO: Implement RAG pipeline pass Exercise 2: Multi-hop Research Assistant Objective: Build a system that can answer complex research questions by gathering information from multiple sources. Tasks Implement multi-hop search logic. Add modules for connecting information across documents. Create a comprehensive evaluation metric. Optimize with MIPRO for complex reasoning. Python Copy class ResearchAssistant(dspy.Module): def __init__(self): super().__init__() self.retrieve = dspy.Retrieve(k=5) # TODO: Add modules for multi-hop reasoning def forward(self, research_question): # TODO: Implement multi-hop search and synthesis pass Exercise 3: Multi-label Document Classifier Objective: Build a sophisticated classifier that can assign multiple labels to documents based on their content. Tasks Implement multi-label classification logic. Handle label dependencies (some labels co-occur). Create appropriate training data and evaluation metrics. Optimize with appropriate DSPy optimizer. Exercise 4: Contract Information Extractor Objective: Build an entity extraction system specifically designed for legal contracts. Tasks Identify contract-specific entity types. Implement extraction for parties, dates, amounts, obligations. Add validation to ensure extracted info is accurate. Create a relationship extractor for contract clauses. Exercise 5: Autonomous Customer Service Agent Objective: Create an intelligent agent that can handle customer service interactions from start to finish. Tasks Implement intent classification for customer messages. Add modules for handling different types of requests and escalation logic. Add memory to maintain conversation context. Optimize with real conversation data. Exercise 6: Code Review Assistant Objective: Build an automated code review assistant that can analyze code for bugs, security issues, and style violations. Tasks Implement analysis for different code quality aspects. Detect common bugs, anti-patterns, and security vulnerabilities. Ensure adherence to coding standards. Generate a comprehensive review report. Next: Solutions"}, {"title": "LingVarBench & Synthetic Data", "url": "chapters/chapter-06/lingvarbench-healthcare-synthetic-data.html", "content": "Chapter 6 LingVarBench A framework for generating high-quality, privacy-compliant synthetic healthcare transcripts using stochastic introspective optimization. The Privacy Challenge Medical NLP is hindered by the lack of publicly available data due to stringent privacy regulations (HIPAA). LingVarBench addresses this by generating synthetic patient-doctor conversations that are indistinguishable from real ones but contain zero real patient data. Architecture The system uses a two-stage process: Generation Pipeline: A DSPy module creates a conversation based on a medical topic, injecting specific entities (diseases, medications) while ensuring natural flow. SIMBA Optimizer: The Stochastic Introspective Mini-Batch Ascent optimizer evolves the prompt itself to maximize linguistic diversity and medical accuracy. SIMBA Optimization SIMBA treats the prompt as an organism in an evolutionary algorithm. class SIMBAOptimizer: def optimize_prompt(self, base_prompt, evaluation_data): population = self._initialize_population(base_prompt) for generation in range(self.generations): # Evaluate -> Select -> Mutate -> Repeat fitness_scores = [self._evaluate(p) for p in population] population = self._create_next_generation(population, fitness_scores) return best_prompt Real-World Impact Models trained purely on LingVarBench synthetic data achieved >90% accuracy when tested on real, private healthcare datasets. This \"synthetic-to-real\" transfer learning proves that high-quality synthetic data can effectively substitute for sensitive real-world data. Next: Scientific Figure Captions"}, {"title": "Solutions", "url": "chapters/chapter-06/solutions.html", "content": "Chapter 6 ¬∑ Section 9 Solutions Complete solutions for real-world application exercises. 7 Solutions Solution 1 ‚≠ê‚≠ê Intermediate Customer Support RAG System import dspy class CustomerSupportRAG(dspy.Module): def __init__(self, knowledge_base=None): super().__init__() # In a real scenario, knowledge_base would be indexed in a vector DB # Here we simulate retrieval self.knowledge_base = knowledge_base or [] self.retrieve = dspy.Retrieve(k=3) self.generate_answer = dspy.ChainOfThought( \"question, context -> answer, sources, confidence\" ) def forward(self, question): # 1. Retrieve relevant info # context = self.retrieve(question) # Simulating retrieval for this example: context = [doc for doc in self.knowledge_base if any(kw in doc.lower() for kw in question.lower().split())] context_str = \"\\n\".join(context) if context else \"No relevant documents found.\" # 2. Generate answer prediction = self.generate_answer( question=question, context=context_str ) return dspy.Prediction( answer=prediction.answer, reasoning=prediction.rationale, sources=context, confidence=prediction.confidence ) # Example Usage kb = [ \"Product returns must be initiated within 30 days of purchase.\", \"Free shipping is available for orders over $50.\", \"Customer support is available 24/7 via phone and chat.\" ] rag = CustomerSupportRAG(knowledge_base=kb) result = rag(\"What is the return policy?\") print(f\"Answer: {result.answer}\") print(f\"Confidence: {result.confidence}\") Solution 2 ‚≠ê‚≠ê‚≠ê Advanced Multi-hop Research Assistant class ResearchAssistant(dspy.Module): def __init__(self): super().__init__() self.retrieve = dspy.Retrieve(k=3) self.generate_subquestions = dspy.Predict( \"question -> subquestions\" ) self.synthesize = dspy.ChainOfThought( \"question, findings -> answer\" ) def forward(self, research_question): # 1. Break down complex question plan = self.generate_subquestions(question=research_question) subquestions = plan.subquestions.split(\"\\n\") findings = [] for subq in subquestions: # 2. Retrieve info for each step docs = self.retrieve(subq).passages findings.append(f\"Q: {subq}\\nFound: {docs}\") # 3. Synthesize final answer result = self.synthesize( question=research_question, findings=\"\\n\\n\".join(findings) ) return dspy.Prediction( answer=result.answer, reasoning=result.rationale ) Solution 3 ‚≠ê‚≠ê Intermediate Multi-label Document Classifier class MultiLabelClassifier(dspy.Module): def __init__(self, possible_labels): super().__init__() self.labels = \", \".join(possible_labels) self.classify = dspy.Predict( f\"document, possible_labels[{self.labels}] -> applied_labels, confidence_scores\" ) def forward(self, document): prediction = self.classify( document=document, possible_labels=self.labels ) # Parse labels (assuming comma-separated output) labels = [l.strip() for l in prediction.applied_labels.split(',')] return dspy.Prediction( labels=labels, scores=prediction.confidence_scores ) Solution 4 ‚≠ê‚≠ê‚≠ê Advanced Contract Information Extractor class ContractExtractor(dspy.Module): def __init__(self): super().__init__() self.extract_parties = dspy.Predict(\"contract -> parties\") self.extract_dates = dspy.Predict(\"contract -> effective_date, termination_date\") self.extract_terms = dspy.Predict(\"contract -> payment_terms, obligations\") def forward(self, contract_text): parties = self.extract_parties(contract=contract_text) dates = self.extract_dates(contract=contract_text) terms = self.extract_terms(contract=contract_text) return dspy.Prediction( parties=parties.parties, dates={ \"effective\": dates.effective_date, \"termination\": dates.termination_date }, terms={ \"payment\": terms.payment_terms, \"obligations\": terms.obligations } ) Solution 5 ‚≠ê‚≠ê‚≠ê Advanced Autonomous Customer Service Agent class CustomerServiceAgent(dspy.Module): def __init__(self): super().__init__() self.classify_intent = dspy.Predict(\"message -> intent\") self.handle_inquiry = dspy.ChainOfThought(\"message, history -> response, action\") self.escalate = dspy.Predict(\"message, reason -> department\") self.history = [] def forward(self, customer_message): # 1. Identify Intent intent = self.classify_intent(message=customer_message).intent # 2. Add to history self.history.append(f\"User: {customer_message}\") if \"urgent\" in intent.lower() or \"complaint\" in intent.lower(): # Escalate dept = self.escalate(message=customer_message, reason=intent) response = f\"I am escalating this issue to {dept.department}.\" action = \"ESCALATE\" else: # Handle normally result = self.handle_inquiry( message=customer_message, history=\"\\n\".join(self.history[-5:]) ) response = result.response action = result.action self.history.append(f\"Agent: {response}\") return dspy.Prediction( response=response, action=action, intent=intent ) Solution 6 ‚≠ê‚≠ê Intermediate Code Review Assistant class CodeReviewAssistant(dspy.Module): def __init__(self): super().__init__() self.analyze = dspy.ChainOfThought( \"code, language -> bugs, security_issues, style_suggestions, overall_score\" ) def forward(self, code, language=\"python\"): review = self.ana"}, {"title": "Intelligent Agents", "url": "chapters/chapter-06/intelligent-agents.html", "content": "Chapter 6 ¬∑ Section 6 Intelligent Agents Explore the frontier of AI application development by building autonomous agents that perceive, plan, and act. ~20 min read Introduction Intelligent agents are autonomous systems capable of perceiving their environment, reasoning about goals, and taking actions to achieve them. DSPy provides a structured way to implement the core loops of perception, planning, and execution that define agency. Understanding Intelligent Agents Core Components Perception: Understanding the current state (e.g., reading user input, checking database). Planning: Breaking down goals into actionable steps. Decision Making: Choosing the best action from available options. Memory: Retaining context from past interactions. Execution: Performing actions (e.g., API calls, database updates). Agent Types Reactive: Responds immediately to inputs without deep planning. Proactive: Plans ahead to achieve long-term goals. Collaborative: Works with other agents or humans. Learning: Improves strategies based on feedback. Building Agents with DSPy Basic Reactive Agent A simple agent that perceives, decides, and acts: Python Copy import dspy class ReactiveAgent(dspy.Module): def __init__(self, name, capabilities): super().__init__() self.name = name self.capabilities = capabilities self.perceive = dspy.Predict(\"input -> perceived_state\") self.decide = dspy.Predict(\"state, capabilities -> action, reasoning\") self.memory = {} def forward(self, input_text): # Perception perception = self.perceive(input=input_text) current_state = perception.perceived_state # Decision decision = self.decide( state=current_state, capabilities=\", \".join(self.capabilities) ) return dspy.Prediction( agent_name=self.name, perceived_state=current_state, action=decision.action, reasoning=decision.reasoning ) Proactive Agent with Planning An agent that creates and executes multi-step plans: Python Copy class ProactiveAgent(dspy.Module): def __init__(self, name, goals, tools): super().__init__() self.name = name self.goals = goals self.tools = tools self.understand_context = dspy.Predict(\"input -> context, user_intent\") self.create_plan = dspy.ChainOfThought(\"context, intent, goals, tools -> plan\") self.execute_step = dspy.Predict(\"plan, current_step, tools -> action, next_step\") self.current_plan = None self.current_step = 0 def forward(self, input_text): # Understand context understanding = self.understand_context(input=input_text) # Create/Update Plan if not self.current_plan or \"new\" in understanding.user_intent: planning = self.create_plan( context=understanding.context, intent=understanding.user_intent, goals=\", \".join(self.goals), tools=\", \".join(self.tools) ) self.current_plan = planning.plan self.current_step = 0 # Execute execution = self.execute_step( plan=self.current_plan, current_step=str(self.current_step), tools=\", \".join(self.tools) ) if execution.next_step: self.current_step = int(execution.next_step) return dspy.Prediction( action=execution.action, plan=self.current_plan, step=self.current_step ) Real-World Application: Customer Service Agent A practical agent handling customer inquiries, knowledge base search, and escalation: Python Copy class CustomerServiceAgent(dspy.Module): def __init__(self, company_name, knowledge_base): super().__init__() self.company_name = company_name self.knowledge_base = knowledge_base self.classify_intent = dspy.Predict(\"customer_message -> intent, urgency, sentiment\") self.search_knowledge = dspy.Retrieve(k=3) self.generate_response = dspy.ChainOfThought( \"intent, sentiment, knowledge, company_policy -> response, action_needed\" ) self.escalate = dspy.Predict(\"issue, customer_details -> escalation_reason, department\") def forward(self, customer_message): # 1. Classify Intent classification = self.classify_intent(customer_message=customer_message) # 2. Retrieve Knowledge relevant_kb = self.search_knowledge( query=f\"{classification.intent} {customer_message}\" ) # 3. Generate Response response = self.generate_response( intent=classification.intent, sentiment=classification.sentiment, knowledge=\"\\n\".join(relevant_kb.passages), company_policy=self.knowledge_base.get(\"policies\", \"\") ) # 4. Check for Escalation final_action = response.action_needed if \"escalate\" in str(response.action_needed).lower(): escalation = self.escalate( issue=customer_message, customer_details=\"Session Context...\" ) final_action = f\"Escalated to {escalation.department}\" return dspy.Prediction( response=response.response, action=final_action ) Best Practices Clear Goals: Define explicit goals and alignment checks for agents. Error Handling: Implement robust fallback mechanisms for failed actions or API calls. Continuous Learning: Allow agents to store feedback and adjust strategies over time (e.g., using a memory module). Safety: Constrain agent actions to prevent unintended consequences. Next: Code Generation"}, {"title": "Scientific Figure Caption Generation", "url": "chapters/chapter-06/scientific-figure-caption-generation.html", "content": "Chapter 6 Scientific Figure Captioning A two-stage pipeline for generating scientifically accurate and stylistically consistent figure captions using context-aware optimization. The Challenge Writing captions for scientific figures is hard. The caption must be technically precise and match the specific writing style of the paper (or journal). Generic image captioning models fail here because they lack context and domain knowledge. Two-Stage Pipeline We solve this with a dual-stage approach: Context-Aware Generation: A module retrieves relevant text from the paper (e.g., surrounding the figure reference) to understand what the figure shows. Stylistic Refinement: A second module rewrites basic captions to match the target author's style using few-shot examples from their previous work. Stage 1: Category-Specific Optimization We use MIPROv2 to optimize prompts specifically for different figure types (e.g., bar charts, scatter plots, diagrams). def optimize_for_category(training_data, category): # Filter data for just this figure type (e.g. \"bar_graph\") category_data = [x for x in data if x.category == category] # Compile a specialized module optimizer = dspy.MIPROv2(metric=caption_quality_metric) return optimizer.compile(CategoryCaptionModule(), trainset=category_data) Results This specialized pipeline achieved significant metric improvements: ROUGE-1 Recall: +8.3% Style Consistency: ~45% improvement in BLEU scores against author profile examples. Next: Retrieval-Augmented Guardrails"}, {"title": "Databricks Integration", "url": "chapters/chapter-08/databricks-integration.html", "content": "Chapter 8 ¬∑ Case Study 10 Databricks Platform Integration A deep dive into how Databricks integrated DSPy to streamline enterprise AI development. ~20 min read Integration Goals Databricks aimed to provide native platform support for DSPy, enabling users to seamlessly connect to their Foundation Model APIs and Vector Search endpoints. Native Configuration Setting up DSPy to work with Databricks is straightforward, involving just a few lines of configuration code. Python Copy def configure_databricks_dspy(): workspace = WorkspaceClient() lm = dspy.Databricks( model=\"databricks-dbrx-instruct\", api_base=workspace.config.host, api_token=workspace.config.token ) rm = dspy.DatabricksRM( endpoint_name=\"vector_search_endpoint\", index_name=\"document_index\" ) dspy.settings.configure(lm=lm, rm=rm) Unified RAG Implementation With native integration, building RAG pipelines becomes easier. The `DatabricksRAG` module demonstrates how to connect Vector Search retrieval with DBRX generation. Results 15x Faster Development (hours vs. days) 33% Lower Latency for deployments Seamless MLflow Integration for tracking Continue to Behavioral Simulation"}, {"title": "Assertion-Driven Apps", "url": "chapters/chapter-08/assertion-apps.html", "content": "Chapter 8 ¬∑ Case Study 7 Assertion-Driven Apps Applying DSPy Assertions to enforce quality, compliance, and accuracy in medical, legal, and financial domains. ~25 min read Introduction In critical domains like healthcare and law, AI outputs must be strictly accurate. DSPy Assertions allow developers to enforce these constraints programmatically. Case Study: Medical Report Generator We implement a system where assertions validate format, content completeness, and medical accuracy such as consistent patient age. Python Copy class MedicalReportGenerator(dspy.Module): def generate_with_assertions(self, patient_data, initial_report): format_asserted = dspy.Assert( self.base_generator, validation_fn=self.validate_medical_format ) return format_asserted(**patient_data) Validation Logic We define specific validation functions that check for required sections (Header, Assessment, etc.) and ensure clinical terminology is used correctly. Case Study: Legal Document Analyzer Assertions are used to ensure legal risk assessments include all critical clauses like \"liability\" and \"termination\", ensuring no clause is missed during analysis. Continue to JetBlue Optimization"}, {"title": "JetBlue Optimization", "url": "chapters/chapter-08/jetblue-optimization.html", "content": "Chapter 8 ¬∑ Case Study 8 Databricks & JetBlue Optimization A real-world example of optimizing customer service chatbots for speed and accuracy using DSPy on Databricks. ~15 min read Business Challenge JetBlue needed to reduce manual prompt engineering time and improve the response latency of their customer support chatbots. Solution Architecture Using DSPy, they built a multi-stage RAG pipeline. A key component was the automated optimization of retrieval queries and answers. The Pipeline Python Copy class JetBlueRAGPipeline(dspy.Module): def __init__(self): self.generate_query = ChainOfThought(\"context, question -> search_query\") self.generate_answer = Predict(\"context, question -> answer\") def forward(self, question, context=None): query = self.generate_query(context=context, question=question).search_query passages = self.retrieve(query).passages return self.generate_answer(context=passages, question=question) Optimization with MIPROv2 Automated prompt optimization was the game changer. By defining an evaluation metric, they could let DSPy's optimizer find the best prompts, reducing engineering time from hours to zero. Key Results 2x Faster response time (2.4s to 1.2s) 100% Reduction in manual prompt engineering 17% Accuracy Gain (72% to 89%) Continue to Replit Code Repair"}, {"title": "Automated Data Analysis", "url": "chapters/chapter-08/data-analysis.html", "content": "Chapter 8 ¬∑ Case Study 5 Automated Data Analysis Creating a system that processes data streams, generates insights, and creates reports from natural language queries. ~20 min read Introduction Financial and enterprise sectors need systems that can ingest massive amounts of data and provide immediate, actionable intelligence. DSPy helps bridge the gap between raw data and natural language insights. System Architecture The pipeline includes data ingestion, feature engineering, the DSPy analytics engine, and a reporting layer. Data Analyzer The core module interprets requests and executes analysis plans. Python Copy class DataAnalyzer(dspy.Module): def __init__(self): self.analyze = dspy.ChainOfThought(DataAnalysisSignature) def forward(self, request, data): data_summary = self._summarize_data(data) analysis = self.analyze(query=request.query, summary=data_summary) return analysis Natural Language Querying Translating user questions into executable analysis plans is a perfect use case for DSPy's ChainOfThought. Python Copy class NLQueryInterface(dspy.Module): def process_query(self, query, catalog): translation = self.translate(nl_query=query, available_data=catalog) return self.executor.analyze(translation) Anomaly Detection Combining statistical methods (Z-score) with AI pattern recognition allows for robust anomaly detection. Continue to STORM Writing Assistant"}, {"title": "Behavioral Simulation Automation", "url": "chapters/chapter-08/behavioral-simulation.html", "content": "Chapter 8 ¬∑ Case Study 11 Behavioral Simulation Automation Transforming leadership assessment by automating behavioral scoring with DSPy, reducing turnaround time from days to seconds. ~20 min read Business Challenge DDI (Development Dimensions International) needed to scale their leadership assessments. Human scoring was accurate but slow (24-48 hours) and expensive. DSPy Optimization Pipeline They built a pipeline that breaks down the assessment into analysis, scoring, and report generation steps. Behavioral Assessment Pipeline Python Copy class BehavioralAssessmentPipeline(dspy.Module): def __init__(self): self.response_analyzer = ChainOfThought(\"question, response -> analysis\") self.scorer = Predict(\"analysis, criteria -> scores\") self.report_generator = ChainOfThought(\"scores, framework -> report\") def forward(self, question, response, framework): analysis = self.response_analyzer(question, response, framework) scores = self.scorer(analysis, framework) return self.report_generator(scores, framework) Prompt Optimization Using `BootstrapFewShot`, they optimized prompts against expert human scores. This increased the recall score from 0.43 to 0.98. Impact 17,000x Faster Delivery (seconds vs. days) 95% Cost Reduction 95% Scoring Agreement with experts Continue to Medical Report Gen"}, {"title": "Enterprise RAG", "url": "chapters/chapter-08/enterprise-rag.html", "content": "Chapter 8 ¬∑ Case Study 1 Enterprise RAG System Building a secure, scalable, and multi-lingual RAG system for corporate knowledge management. ~25 min read Problem Definition A multinational corporation needs a unified solution to help employees quick-find information across thousands of internal documents, including policies, legal contracts, and product specs. Key Requirements Accurate Retrieval: Precisions matters. Security: Respect document access permissions (ACLs). Scalability: Handle millions of documents. Multilingual: Content in 15+ languages. System Design The architecture consists of an ingestion pipeline (OCR, chunking), a hybrid retrieval system (Vector + Keyword), and a DSPy-powered generation layer. Document Indexer The indexer handles text extraction, language detection, and semantic chunking. Python Copy class DocumentIndexer(dspy.Module): def forward(self, document: Dict) -> List[DocumentChunk]: text = self._extract_text(document) language = self._detect_language(text) chunks = self._create_chunks(text, language) return [DocumentChunk(content=c, language=language) for c in chunks] Hybrid Retrieval Combining vector search for semantics and keyword search for specific terms is crucial for enterprise accuracy. Python Copy class HybridRetriever(dspy.Module): def forward(self, query): vec_res = self._vector_search(query) kw_res = self._keyword_search(query) return self._combine_and_rerank(vec_res, kw_res) RAG Generator The generation module synthesizes answers and verifies them against the source context to minimize hallucinations. Python Copy class RAGGenerator(dspy.Module): def __init__(self): self.generate = dspy.Predict(GenerateAnswerSignature) self.verify = dspy.ChainOfThought(VerifyAnswerSignature) def forward(self, question, context): ans = self.generate(question=question, context=context) final = self.verify(answer=ans.answer, context=context) return final Continue to Healthcare Clinical Notes"}, {"title": "STORM Writing Assistant", "url": "chapters/chapter-08/storm-assistant.html", "content": "Chapter 8 ¬∑ Case Study 6 STORM Writing Assistant A deep dive into building an AI system for comprehensive, multi-perspective article generation. ~30 min read Overview STORM (Synthesis of Topic Outlines through Retrieval and Multi-perspective questioning) simulates the human research process. It gathers information from various angles, synthesizes it, and generates well-structured, cited articles. System Architecture The system operates in two main phases: Pre-writing (Research & Outlining) and Writing (Drafting & Refining). Research Synthesizer This component identifies connections and resolves contradictions between different sources. Python Copy class ResearchSynthesizer(dspy.Module): def forward(self, topic, research_data): connections = self.identify_connections(research_data) synthesis = self.create_synthesis(topic, research_data, connections) return synthesis Human-AI Collaboration STORM is designed to assist, not replace. The Human Review Interface allows users to answer questions generated by the AI to refine the article's direction. Python Copy class HumanReviewInterface(dspy.Module): def generate_review_prompts(self, article, topic): questions = self.generate_review_questions(article, topic) return questions def process_human_feedback(self, feedback): return self.summarize_feedback(feedback) Continue to Assertion-Driven Apps"}, {"title": "Customer Support Chatbot", "url": "chapters/chapter-08/customer-support.html", "content": "Chapter 8 ¬∑ Case Study 3 Customer Support Chatbot Developing a high-performance, intent-aware AI chatbot capable of handling 50,000+ daily inquiries. ~30 min read Business Challenge An e-commerce giant needs to automate 50,000+ daily inquiries while maintaining >90% CSAT. The system must handle multi-turn conversations, recognize intents accurately, and escalate when necessary. System Design The solution involves an input processor, an NLU engine for intent classification, a dialogue manager for state tracking, and a dynamic response generator. Intent Classification We use ChainOfThought to determine the user's intent and extract relevant entities like Order IDs. Python Copy class IntentClassifier(dspy.Module): def __init__(self): self.classify = dspy.ChainOfThought(IntentClassifierSignature) def forward(self, message, history): return self.classify(message=message, conversation_history=history) Dialogue Management Managing the state of the conversation is key. The Dialogue Manager decides the next action: respond, perform a task (like checking order status), or escalate. Python Copy class DialogueManager(dspy.Module): def forward(self, state, message, intent_result): # Determine next action based on state and intent # execute backend actions if needed pass Knowledge Integration The bot queries multiple sources (FAQ, Product DB, Vector Store) to provide accurate answers to general inquiries. Continue to AI Code Assistant"}, {"title": "Replit Code Repair", "url": "chapters/chapter-08/replit-repair.html", "content": "Chapter 8 ¬∑ Case Study 9 Replit Code Repair Using DSPy to fix code bugs automatically by generating synthetic training data for LSP diagnostics. ~20 min read Business Challenge Replit faced a situation where only 10% of code errors identified by their Language Server Protocol (LSP) had automated fixes. This left developers to manually debug huge numbers of errors. Data Pipeline Architecture Replit utilized DSPy to synthesize code fixes. The pipeline analyzes the error, synthesizes a fix, and then verifies it. Code Repair Pipeline Python Copy class CodeRepairPipeline(dspy.Module): def __init__(self): self.diagnostic_analyzer = ChainOfThought(\"code, error -> analysis\") self.fix_synthesizer = ChainOfThought(\"code, analysis -> diff\") self.fix_verifier = Predict(\"code, diff -> valid\") def forward(self, code_file, error_line, error_message): analysis = self.diagnostic_analyzer(code_file, error_line, error_message) diff = self.fix_synthesizer(code_file, analysis) verification = self.fix_verifier(code_file, diff) return verification Synthetic Data Generation Using this pipeline, they generated a dataset of over 100,000 synthetic fixes to train a smaller 7B parameter model that could run efficiently in production. Impact 35% Automated Fix Rate (up from 10%) 50,000 Daily Fixes suggested to users 68% User Acceptance Rate Continue to Databricks Integration"}, {"title": "Medical Report Generation", "url": "chapters/chapter-08/medical-report.html", "content": "Chapter 8 ¬∑ Case Study 12 Medical Report Generation Breaking down technical jargon into patient-friendly consultations using DSPy for structured extraction and generation. ~20 min read Business Challenge Medical reports are notoriously difficult for patients to understand. Salomatic needed a way to translate complex doctor notes and lab results into clear, actionable advice. Multi-Stage Extraction Pipeline They used DSPy to create a rigorous pipeline that extracts, validates, and interprets medical data. The Pipeline Python Copy class MedicalReportPipeline(dspy.Module): def __init__(self): self.extract_labs = ChainOfThought(\"notes, results -> panels\") self.extract_diagnoses = ChainOfThought(\"notes, history -> diagnoses\") self.generate_consultation = ChainOfThought( \"profile, labs, diagnoses -> consultation\" ) def forward(self, notes, results, patient): panels = self.extract_labs(notes, results) diagnoses = self.extract_diagnoses(notes, patient) return self.generate_consultation(patient, panels, diagnoses) Observability with Langtrace Integration with Langtrace provided deep visibility into where data extraction might be failing, allowing them to pinpoint issues and improve accuracy rapidly. Results 87.5% Reduction in manual corrections 90% Faster report generation 50x Capacity Increase Continue to Exercises"}, {"title": "Chapter 8: Case Studies", "url": "chapters/chapter-08/index.html", "content": "Chapter 8 Case Studies Comprehensive case studies demonstrating real-world applications of DSPy in production environments. ~10 min read Overview This chapter presents comprehensive case studies that demonstrate real-world applications of DSPy in production environments. Each case study explores a complete implementation, from initial problem definition through to deployment, showcasing best practices and advanced techniques. Learning Objectives After completing this chapter, you will be able to: Apply DSPy concepts to solve real business problems Design and implement end-to-end AI applications Optimize performance and manage production deployments Handle common challenges and edge cases in production Scale DSPy applications for enterprise use Case Studies Covered We will cover a wide range of domains including: Enterprise RAG Systems: Knowledge management at scale. Healthcare: Clinical note processing and report generation. Customer Support: Intelligent chatbots. Coding Assistants: AI-powered development tools. Data Analysis: Automated insights pipelines. Writing Assistants: The STORM architecture for articles. And more, including jetBlue's optimization and Databricks integration. Start Reading: Enterprise RAG System"}, {"title": "Chapter 8 Exercises", "url": "chapters/chapter-08/exercises.html", "content": "Chapter 8 Exercises Put your knowledge to the test. These exercises will challenge you to apply DSPy concepts to real-world scenarios. 1. Building a Mini-RAG System Objective: Create a simplified version of the enterprise RAG system for a personal knowledge base. Requirements: Document ingestion from PDF files Vector storage using ChromaDB Retrieval and answer generation Basic citation support Python Copy # Step 3: Answer Generation with DSPy class RAGAnswerSignature(dspy.Signature): \"\"\"Generate answer from retrieved context.\"\"\" context = dspy.InputField(desc=\"Retrieved document chunks\") question = dspy.InputField(desc=\"User question\") answer = dspy.OutputField(desc=\"Answer based on context\") sources = dspy.OutputField(desc=\"Source information\") class RAGAnswerer(dspy.Module): def __init__(self): super().__init__() self.generate = dspy.Predict(RAGAnswerSignature) def forward(self, question: str, retrieved_docs: List[Dict]): context = \"\\n\\n\".join([doc['content'] for doc in retrieved_docs]) return self.generate(context=context, question=question) 2. STORM Writing Assistant Implementation Objective: Build a simplified version of the STORM writing assistant for generating articles. Requirements: Multi-perspective research simulation Outline generation from research Section-by-section content generation Basic citation integration Python Copy class ContentGenerator(dspy.Module): def __init__(self): super().__init__() self.generate_content = dspy.Predict( \"section_title, research_data, word_count -> content\" ) self.add_citations = dspy.Predict( \"content, research_data -> cited_content\" ) View Solutions"}, {"title": "AI Code Assistant", "url": "chapters/chapter-08/code-assistant.html", "content": "Chapter 8 ¬∑ Case Study 4 AI Code Assistant Enhancing developer productivity with an AI assistant that understands, generates, documents, and tests code. ~25 min read Overview Building an AI code assistant requires more than just a large language model. It needs context awareness, deep understanding of syntax, and the ability to verify its own outputs. System Architecture The system comprises four main modules: Code Analysis, Code Generation, Documentation, and Test Generation. Code Analyzer This module parses the Abstract Syntax Tree (AST) to understand the structure of the code before performing semantic analysis. Python Copy class CodeAnalyzer(dspy.Module): def forward(self, code, file_path): language = self._detect_language(file_path) parsed = self._parse_code(code, language) analysis = self.analyze(code=code[:1000], language=language.value) return CodeAnalysisResult(language=language, ast_tree=parsed) Code Generator The generator takes a natural language prompt and existing context to produce code, which is then refined and validated. Python Copy class CodeGenerator(dspy.Module): def forward(self, prompt, context, style): # Generate initial draft gen = self.generate(prompt=prompt, context=context) # Self-correction loop refined = self.refine(code=gen.code, errors=self._validate(gen.code)) return refined Automated Testing A unique feature of this assistant is its ability to generate unit tests for the code it produces, ensuring correctness. Python Copy class TestGenerator(dspy.Module): def forward(self, code, language=\"python\"): # Generate pytest/unittest code tests = self.generate(code=code, framework=\"pytest\") return tests Continue to Automated Data Analysis"}, {"title": "Chapter 8 Solutions", "url": "chapters/chapter-08/solutions.html", "content": "Chapter 8 Solutions Example implementations and solutions for the Chapter 8 exercises. 1. Building a Mini-RAG System Here is a complete implementation of the Mini-RAG system, including document chunking, vector storage with ChromaDB, and a DSPy-based answerer. Python Copy import dspy import chromadb from typing import List, Dict from sentence_transformers import SentenceTransformer # 1. Document Processing def chunk_document(text: str, chunk_size: int = 1000) -> List[str]: return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)] # 2. Vector Storage class SimpleRAG: def __init__(self): self.chroma_client = chromadb.Client() self.collection = self.chroma_client.create_collection(\"documents\") self.embedder = SentenceTransformer('all-MiniLM-L6-v2') def add_documents(self, chunks: List[str], metadata: List[Dict]): embeddings = self.embedder.encode(chunks).tolist() ids = [str(i) for i in range(len(chunks))] self.collection.add( documents=chunks, embeddings=embeddings, metadatas=metadata, ids=ids ) def query(self, query: str, n_results: int = 3) -> Dict: query_embedding = self.embedder.encode([query]).tolist() return self.collection.query( query_embeddings=query_embedding, n_results=n_results ) # 3. DSPy Answerer class RAGAnswerSignature(dspy.Signature): \"\"\"Generate answer from retrieved context.\"\"\" context = dspy.InputField(desc=\"Retrieved document chunks\") question = dspy.InputField(desc=\"User question\") answer = dspy.OutputField(desc=\"Answer based on context\") sources = dspy.OutputField(desc=\"Source information\") class RAGAnswerer(dspy.Module): def __init__(self): super().__init__() self.generate = dspy.Predict(RAGAnswerSignature) def forward(self, question: str, retrieved_docs: List[Dict]): # Extract content from ChromaDB results # Note: ChromaDB returns a dict with 'documents' as a list of lists docs_content = retrieved_docs['documents'][0] context = \"\\n\\n\".join(docs_content) return self.generate( context=context, question=question ) 2. STORM Writing Assistant This solution implements a simplified STORM pipeline that generates a multi-section article with citations based on simulated research findings. Python Copy import dspy from typing import Dict, List class ContentGenerator(dspy.Module): def __init__(self): super().__init__() self.generate_content = dspy.Predict( \"section_title, research_data, word_count -> content\" ) self.add_citations = dspy.Predict( \"content, research_data -> cited_content\" ) def generate_section(self, section_title: str, research_data: Dict, word_count: int = 300) -> Dict: # Flatten findings for context research_text = \"\" for perspective, data in research_data.items(): research_text += f\"\\nPerspective: {perspective}\\n\" research_text += \"\\n\".join(data['findings']) # Generate proper content content_result = self.generate_content( section_title=section_title, research_data=research_text, word_count=str(word_count) ) # Enhance with citations cited_result = self.add_citations( content=content_result.content, research_data=research_text ) return { 'title': section_title, 'content': cited_result.cited_content, 'word_count': len(cited_result.cited_content.split()) } Continue to Chapter 09"}, {"title": "Healthcare Clinical Notes", "url": "chapters/chapter-08/healthcare-notes.html", "content": "Chapter 8 ¬∑ Case Study 2 Healthcare Clinical Notes Extracting structured insights and identifying risks from unstructured medical narratives. ~20 min read Introduction Analysis of unstructured clinical notes is a critical task in healthcare. DSPy can automate the extraction of symptoms, medications, and diagnoses, as well as flag potential risks. System Architecture The system breaks down clinical note analysis into modular tasks: Entity Extraction, Risk Analysis, Summarization, and Recommendations. Medical Entity Extraction Using ChainOfThought to identify and structure entities creates a robust foundation for downstream tasks. Python Copy class MedicalEntityExtractor(dspy.Module): def __init__(self): self.extract = dspy.ChainOfThought( \"clinical_note -> medical_entities, patient_symptoms, diagnoses, medications, vitals\" ) def forward(self, clinical_note): with dspy.context(medical_context=True): return self.extract(clinical_note=clinical_note) Clinical Risk Analysis Once entities are extracted, a separate module assesses risks and checks for drug interactions. Python Copy class ClinicalRiskAnalyzer(dspy.Module): def __init__(self): self.analyze_risk = dspy.Predict( \"entities, symptoms, meds -> risk_factors, alert_level\" ) Optimization with MIPRO Medical vernacular is complex. Optimizing prompts with MIPRO using a labeled dataset of clinical notes significantly improves F1 scores on entity extraction. Continue to Customer Support Chatbot"}, {"title": "Additional Resources", "url": "chapters/chapter-09/resources.html", "content": "Chapter 9 Additional Resources A curated collection of documentation, papers, and tools to help you master DSPy. Official DSPy Resources GitHub Repository - Source code and docs. Documentation - Official guides and tutorials. Examples - Practical use cases. Academic Papers Deep dive into the research behind DSPy: \"DSPy: Compiling Language Model Calls into State-of-the-Art Retrievers\" (2023) - Read on ArXiv \"Optimizing Language Models for Reasoning\" (2024) - Read on ArXiv Community & Support Stanford NLP Discord - Chat with developers and users. GitHub Discussions - Ask questions and share ideas. Twitter/X - Updates and announcements. Continue to Glossary"}, {"title": "Community Resources", "url": "chapters/chapter-09/community.html", "content": "Chapter 9 Community Resources Learn from the community's experience, insights, and real-world production stories. Developer Perspectives Isaac Miller: \"Why I Bet on DSPy\" Highlights DSPy as a tool to make LLMs more effective by treating them as \"next-token predictors\" rather than reasoning engines, using optimizers to guide them. Jina AI: \"Not Your Average Prompt Engineering\" Discusses the separation of logic from text and the dual role of metrics as both loss functions and evaluation tools. Key Community Insights From Prompting to Programming: Transition from manual tuning to systematic, algorithmic optimization. Metrics are Central: A well-defined metric is crucial for optimization success. Start Small: Begin with simple tasks and scale complexity gradually. Common Challenges \"Bootstrapped 0 full traces\": Often caused by metrics that never return True or improper module signatures. Debug by testing your metric on mock data. Conclusion The DSPy community is growing rapidly. Join the Discord, read the blogs, and contribute back to help advance the field of systematic AI programming. Back to Home"}, {"title": "API Reference Quick Guide", "url": "chapters/chapter-09/api-reference.html", "content": "Chapter 9 API Reference A quick lookup guide for DSPy's core classes, methods, and patterns. Initialization dspy.configure() Configure the global settings for DSPy. Python Copy dspy.configure(lm=lm, rm=rm) Signatures dspy.Signature Define the input/output structure of a task. Python Copy class QA(dspy.Signature): \"\"\"Answer questions.\"\"\" question = dspy.InputField() answer = dspy.OutputField() Inline Signatures: \"question -> answer\" Modules dspy.Predict Basic module for using a signature. pred = dspy.Predict(\"input -> output\") dspy.ChainOfThought Adds step-by-step reasoning. cot = dspy.ChainOfThought(\"input -> output\") dspy.ReAct Agent loop for reasoning and acting. agent = dspy.ReAct(signature, tools=[...]) Optimizers dspy.BootstrapFewShot Optimizes by selecting effective few-shot examples. opt = dspy.BootstrapFewShot(metric=metric_fn) dspy.MIPRO Optimizes instructions and examples. opt = dspy.MIPRO(metric=metric_fn) Evaluation dspy.Evaluate Run evaluation on a dataset. Python Copy evaluator = dspy.Evaluate(devset=devset, metric=metric_fn) score = evaluator(program) Continue to Troubleshooting"}, {"title": "Glossary", "url": "chapters/chapter-09/glossary.html", "content": "Chapter 9 Glossary Definitions of key terms used in DSPy and LLM development. Adapter A component that connects DSPy to external tools or APIs. BootstrapFewShot An optimization technique in DSPy that automatically discovers and selects effective examples to include in the prompt. ChainOfThought A reasoning technique where the model generates a sequence of intermediate steps before arriving at the final answer. Compilation The process of optimizing a DSPy program by training its parameters (like instructions and examples) against a metric. Context Information provided to the language model to help it answer a query, often retrieved from external documents. Demonstration An example input-output pair included in the prompt to guide the model's behavior (few-shot learning). DSPy Declarative Self-improving Python. A framework for programming with language models as composable modules. Evaluation The process of measuring the quality of a system's outputs using defined metrics and a test dataset. Hallucination A phenomenon where a language model generates incorrect or nonsensical information that appears plausible. Metric A function that takes an example and a prediction (and optionally specific trace information) and returns a score indicating quality. Module A building block in DSPy (like dspy.Predict or dspy.ChainOfThought) that encapsulates a transformation from input to output. Optimizer An algorithm (often called a Teleprompter) that tunes the parameters of a DSPy program (prompts, examples) to maximize a metric. Predictor A module that uses a language model to predict outputs based on inputs and a defined signature. RAG (Retrieval-Augmented Generation) A pattern where relevant documents are retrieved and fed to the LLM to ground its responses in specific data. Signature A declarative specification of the input/output behavior of a DSPy module (e.g., \"question -> answer\"). Teleprompter The older term for an Optimizer in DSPy; responsible for automatically generating and selecting effective prompts. Continue to Community Resources"}, {"title": "Chapter 9: Appendices", "url": "chapters/chapter-09/index.html", "content": "Chapter 9 Appendices Your comprehensive reference guide for DSPy, including API details, troubleshooting, and resources. Overview Welcome to the Appendices chapter - your comprehensive reference guide for the entire DSPy journey. This chapter consolidates essential reference materials that you'll return to throughout your DSPy career. What You'll Find API Reference Quick Guide: Concise reference of classes and methods. Troubleshooting Guide: Solutions to common issues and debugging. Additional Resources: Curated links to docs, papers, and tools. Glossary: Definitions of key terms. Learning Objectives Quickly look up DSPy API methods and classes. Diagnose and resolve common DSPy errors. Find and leverage community resources. Understand specialized terminology. Start Reading: API Reference"}, {"title": "Troubleshooting Guide", "url": "chapters/chapter-09/troubleshooting.html", "content": "Chapter 9 Troubleshooting Guide Diagnostic steps and solutions for common DSPy issues. Installation & Setup Issue: ModuleNotFoundError: No module named 'dspy' Solution: Install the package using pip install dspy-ai (not just 'dspy'). Ensure you are in the correct virtual environment. Issue: Authentication/API Errors Ensure your API keys are correctly set. For OpenAI: Python Copy dspy.configure(api_key=\"...\", model=\"gpt-4\") Or use environment variables like OPENAI_API_KEY. Runtime Errors Timeout Errors Network issues or slow model responses. Try increasing the timeout: dspy.OpenAI(..., request_timeout=60) Rate Limit Errors Implement retry logic or backoff. Use the dspy.settings to configure global retry behavior if available, or wrap calls with retry decorators. Evaluation & Optimization Optimization Fails / No Improvement Check your metric function logic. It must accurately reflect quality. Ensure you have enough high-quality examples in your training set. Try a different optimizer (e.g., switch from BootstrapFewShot to MIPRO). Debugging Tips Enable verbose logging to see exactly what is being sent to the LM: dspy.settings.configure(trace=True) Inspect the last call: lm.inspect_history(n=1) Continue to Additional Resources"}, {"title": "Module Composition", "url": "chapters/chapter-03/module-composition.html", "content": "Chapter 3 ¬∑ Section 5 Module Composition Combine modules into sophisticated pipelines that handle complex, multi-step workflows. ~15 min read üîó The Power of Composition Module composition is where DSPy really shines. By nesting modules inside custom modules, you can build complex systems that are: üì¶ Modular Each component can be developed, tested, and optimized independently. üîß Maintainable Changes to one module don't break others if interfaces are maintained. ‚ö° Optimizable DSPy optimizers can tune all sub-modules together automatically. üèóÔ∏è Nesting Custom Modules Custom modules can contain other custom modules, creating a hierarchy: import dspy # Level 1: Basic modules with signatures class Summarize(dspy.Signature): \"\"\"Create a summary.\"\"\" text: str = dspy.InputField() summary: str = dspy.OutputField() class ExtractTopics(dspy.Signature): \"\"\"Extract main topics.\"\"\" text: str = dspy.InputField() topics: list[str] = dspy.OutputField() # Level 2: Combine into a custom module class ContentAnalyzer(dspy.Module): def __init__(self): super().__init__() self.summarizer = dspy.Predict(Summarize) self.topic_extractor = dspy.Predict(ExtractTopics) def forward(self, document: str): summary = self.summarizer(text=document) topics = self.topic_extractor(text=document) return dspy.Prediction( summary=summary.summary, topics=topics.topics ) # Level 3: Use ContentAnalyzer in another module class DocumentProcessor(dspy.Module): def __init__(self): super().__init__() self.analyzer = ContentAnalyzer() # Nested custom module! self.classifier = dspy.ChainOfThought(ClassifyDocument) def forward(self, document: str): # Use the nested module analysis = self.analyzer(document=document) # Use its outputs classification = self.classifier( summary=analysis.summary, topics=str(analysis.topics) ) return dspy.Prediction( summary=analysis.summary, topics=analysis.topics, category=classification.category ) üìä Common Pipeline Patterns Sequential Pipeline Each step's output feeds into the next: class SequentialPipeline(dspy.Module): def __init__(self): super().__init__() self.step1 = dspy.Predict(Step1Sig) self.step2 = dspy.Predict(Step2Sig) self.step3 = dspy.Predict(Step3Sig) def forward(self, input_data: str): # A ‚Üí B ‚Üí C result1 = self.step1(data=input_data) result2 = self.step2(data=result1.output) result3 = self.step3(data=result2.output) return result3 Parallel Pipeline Run multiple analyses on the same input, then combine: class ParallelPipeline(dspy.Module): def __init__(self): super().__init__() self.sentiment = dspy.Predict(SentimentSig) self.topics = dspy.Predict(TopicsSig) self.entities = dspy.Predict(EntitiesSig) self.combiner = dspy.ChainOfThought(CombineSig) def forward(self, text: str): # Run in parallel (conceptually) sent_result = self.sentiment(text=text) topic_result = self.topics(text=text) entity_result = self.entities(text=text) # Combine results combined = self.combiner( sentiment=sent_result.sentiment, topics=str(topic_result.topics), entities=str(entity_result.entities) ) return dspy.Prediction( sentiment=sent_result.sentiment, topics=topic_result.topics, entities=entity_result.entities, synthesis=combined.synthesis ) Branching Pipeline Choose different paths based on conditions: class BranchingPipeline(dspy.Module): def __init__(self): super().__init__() self.classifier = dspy.Predict(ClassifySig) self.tech_handler = dspy.ChainOfThought(TechSig) self.general_handler = dspy.Predict(GeneralSig) def forward(self, query: str): # Classify first classification = self.classifier(query=query) # Branch based on result if classification.category == \"technical\": result = self.tech_handler(query=query) return dspy.Prediction( answer=result.answer, reasoning=result.rationale, path=\"technical\" ) else: result = self.general_handler(query=query) return dspy.Prediction( answer=result.answer, path=\"general\" ) üîÑ Managing Data Flow Careful data flow design is crucial for complex pipelines: class DocumentQA(dspy.Module): \"\"\"Answer questions about a document using organized data flow.\"\"\" def __init__(self): super().__init__() # Preprocessing self.chunker = dspy.Predict(ChunkDocument) self.summarize_chunk = dspy.Predict(SummarizeChunk) # Core QA self.find_relevant = dspy.ChainOfThought(FindRelevant) self.answer = dspy.ChainOfThought(AnswerFromContext) def forward(self, document: str, question: str): # Step 1: Break into chunks chunks_result = self.chunker(document=document) chunks = chunks_result.chunks # Step 2: Summarize each chunk (for context) chunk_summaries = [] for chunk in chunks: summary = self.summarize_chunk(chunk=chunk) chunk_summaries.append({ \"text\": chunk, \"summary\": summary.summary }) # Step 3: Find relevant chunks summaries_text = \"\\n\".join([c[\"summary\"] for c in chunk_summaries]) relevant = self.find_relevant( question=question, chunk_summaries=summaries_text ) # Step 4: Get relevant chunk texts relevant_indices = relevant.relevant_indices context = \"\\n\\n\".join([ chunk_summaries[i][\"text\"] for i in relevant_indices ]) # "}, {"title": "Creating Custom Modules", "url": "chapters/chapter-03/custom-modules.html", "content": "Chapter 3 ¬∑ Section 4 Creating Custom Modules Build your own modules by subclassing dspy.Module‚Äîthe key to creating powerful, reusable LM components. ~15 min read üèóÔ∏è Why Create Custom Modules? While built-in modules are powerful, custom modules let you: üîó Chain Multiple Steps Combine multiple LM calls with custom logic between them. üîÑ Add Control Flow Include if/else, loops, and error handling around LM calls. üì¶ Encapsulate Complexity Hide implementation details behind a clean interface. ‚ôªÔ∏è Enable Reuse Create modules you can use across different projects. üìê Basic Module Structure Every custom module follows this pattern: import dspy class MyModule(dspy.Module): def __init__(self): super().__init__() # Initialize sub-modules here self.step1 = dspy.Predict(SomeSignature) self.step2 = dspy.ChainOfThought(AnotherSignature) def forward(self, **inputs): # Define the execution logic result1 = self.step1(**inputs) result2 = self.step2(data=result1.output) return result2 1Ô∏è‚É£ Inherit from dspy.Module This enables DSPy's parameter tracking and optimization. 2Ô∏è‚É£ Define __init__ Create all sub-modules as instance attributes. Call super().__init__(). 3Ô∏è‚É£ Define forward Implement the execution logic. This is called when you invoke the module. üí° Important: When you call my_module(inputs), it actually calls my_module.forward(inputs). This is similar to PyTorch! üí° Simple Example: Multi-Step QA Let's create a module that first researches a topic, then answers a question: import dspy # Define the signatures class GenerateSearchQuery(dspy.Signature): \"\"\"Generate a search query to find information for answering the question.\"\"\" question: str = dspy.InputField() search_query: str = dspy.OutputField() class AnswerWithContext(dspy.Signature): \"\"\"Answer the question using the provided context.\"\"\" context: str = dspy.InputField() question: str = dspy.InputField() answer: str = dspy.OutputField() # Create the custom module class ResearchQA(dspy.Module): def __init__(self): super().__init__() self.generate_query = dspy.Predict(GenerateSearchQuery) self.answer = dspy.ChainOfThought(AnswerWithContext) def forward(self, question: str, knowledge_base: str): # Step 1: Generate a search query query_result = self.generate_query(question=question) # Step 2: Simulate searching (in real app, would search knowledge_base) # For demo, we just use the knowledge_base directly context = knowledge_base # Step 3: Answer with context answer_result = self.answer( context=context, question=question ) return dspy.Prediction( search_query=query_result.search_query, answer=answer_result.answer, rationale=answer_result.rationale ) # Usage qa = ResearchQA() result = qa( question=\"What are the benefits of DSPy?\", knowledge_base=\"DSPy is a framework that makes LM programming more reliable and maintainable...\" ) print(result.answer) üì¶ Returning Results with dspy.Prediction Use dspy.Prediction to return structured results from your modules: class AnalysisPipeline(dspy.Module): def __init__(self): super().__init__() self.summarize = dspy.Predict(Summarize) self.analyze = dspy.ChainOfThought(Analyze) def forward(self, document: str): summary = self.summarize(text=document) analysis = self.analyze(text=document) # Return multiple outputs in a Prediction return dspy.Prediction( summary=summary.summary, analysis=analysis.conclusion, confidence=analysis.confidence, # You can add computed values too word_count=len(document.split()) ) # Access all fields result = pipeline(document=\"...\") print(result.summary) print(result.analysis) print(result.confidence) print(result.word_count) üîÑ Adding Control Flow Custom modules can include any Python logic: class SmartClassifier(dspy.Module): def __init__(self): super().__init__() self.quick_classify = dspy.Predict(QuickClassify) self.deep_analyze = dspy.ChainOfThought(DeepAnalyze) def forward(self, text: str, use_deep_analysis: bool = False): # Conditional logic if use_deep_analysis or len(text) > 1000: result = self.deep_analyze(text=text) return dspy.Prediction( category=result.category, confidence=result.confidence, reasoning=result.rationale, method=\"deep\" ) else: result = self.quick_classify(text=text) return dspy.Prediction( category=result.category, confidence=0.8, # Default confidence for quick reasoning=\"Quick classification\", method=\"quick\" ) Loops and Iteration class BatchProcessor(dspy.Module): def __init__(self): super().__init__() self.process_item = dspy.Predict(ProcessItem) def forward(self, items: list[str]): results = [] # Process each item for item in items: result = self.process_item(item=item) results.append(result.output) return dspy.Prediction( results=results, count=len(results) ) ‚ö†Ô∏è Error Handling Handle failures gracefully in your modules: class RobustTranslator(dspy.Module): def __init__(self): super().__init__() self.translate = dspy.Predict(Translate) self.fallback = dspy.Predict(SimpleTranslate) def forward(self, text: str, target_language: str): try: # Try primary translation result = self.translate( "}, {"title": "Built-in Modules", "url": "chapters/chapter-03/built-in-modules.html", "content": "Chapter 3 ¬∑ Section 3 Built-in Modules DSPy provides several powerful built-in modules, each adding unique capabilities to your LM applications. ~20 min read ‚ö° dspy.Predict The simplest module‚Äîmakes a single LM call based on your signature. import dspy class Translate(dspy.Signature): \"\"\"Translate the text to the target language.\"\"\" text: str = dspy.InputField() language: str = dspy.InputField() translation: str = dspy.OutputField() # Create and use translator = dspy.Predict(Translate) result = translator(text=\"Hello, world!\", language=\"Spanish\") print(result.translation) # ¬°Hola, mundo! ‚úÖ When to Use Simple, straightforward tasks where reasoning isn't needed. Translation, classification, simple Q&A. ‚ö†Ô∏è Limitations May struggle with complex reasoning, multi-step problems, or math. üí≠ dspy.ChainOfThought Automatically adds a \"reasoning\" step before generating the final answer. This dramatically improves accuracy on complex tasks. class MathProblem(dspy.Signature): \"\"\"Solve the math word problem.\"\"\" problem: str = dspy.InputField() answer: str = dspy.OutputField() # ChainOfThought adds reasoning automatically solver = dspy.ChainOfThought(MathProblem) result = solver(problem=\"\"\" A train travels 120 miles at 60 mph, then 80 miles at 40 mph. What is the total time for the journey? \"\"\") # Access the reasoning print(f\"Reasoning: {result.rationale}\") # First leg: 120 miles √∑ 60 mph = 2 hours # Second leg: 80 miles √∑ 40 mph = 2 hours # Total: 2 + 2 = 4 hours print(f\"Answer: {result.answer}\") # 4 hours üí° Key feature: ChainOfThought automatically adds a rationale output field‚Äîyou don't need to define it in your signature! ‚úÖ When to Use Complex reasoning, math problems, multi-step analysis, decision-making. üìà Performance Often 20-40% more accurate than Predict on reasoning tasks, but uses more tokens. üí° dspy.ChainOfThoughtWithHint Like ChainOfThought, but accepts a hint to guide the reasoning process. class ComplexAnalysis(dspy.Signature): \"\"\"Analyze the situation and provide recommendations.\"\"\" situation: str = dspy.InputField() recommendation: str = dspy.OutputField() # Create with hint capability analyzer = dspy.ChainOfThoughtWithHint(ComplexAnalysis) result = analyzer( situation=\"Our startup is growing fast but running low on runway.\", hint=\"Consider both short-term survival and long-term growth options.\" ) print(result.rationale) # Shows reasoning influenced by hint print(result.recommendation) ‚úÖ When to Use When you want to guide the LM's reasoning direction without fully specifying the approach. üíª dspy.ProgramOfThought Generates Python code to solve the problem, then executes it for precise calculations. class Calculation(dspy.Signature): \"\"\"Calculate the result.\"\"\" problem: str = dspy.InputField() answer: float = dspy.OutputField() # ProgramOfThought generates and runs code calculator = dspy.ProgramOfThought(Calculation) result = calculator(problem=\"\"\" Calculate compound interest on $10,000 at 5% annual rate compounded monthly for 3 years. \"\"\") print(result.answer) # 11,614.72 (precise calculation) ‚ö†Ô∏è Security note: ProgramOfThought executes generated code. Use with caution and only with trusted inputs. ‚úÖ When to Use Mathematical calculations, data processing, algorithmic problems where precision matters. üìä Advantage 100% accurate for calculations (no LM hallucination on the math itself). üõ†Ô∏è dspy.ReAct Implements the ReAct (Reasoning + Acting) pattern‚Äîthe LM can use tools and observe results in a loop. import dspy # Define tools the agent can use def search_web(query: str) -> str: \"\"\"Search the web for information.\"\"\" # In reality, call a search API return f\"Search results for: {query}\" def calculate(expression: str) -> str: \"\"\"Evaluate a mathematical expression.\"\"\" return str(eval(expression)) # Create ReAct agent with tools class ResearchTask(dspy.Signature): \"\"\"Research the topic and provide a comprehensive answer.\"\"\" question: str = dspy.InputField() answer: str = dspy.OutputField() researcher = dspy.ReAct( ResearchTask, tools=[search_web, calculate] ) result = researcher(question=\"What is the population of Tokyo and how does it compare to New York?\") print(result.answer) üîÑ How It Works The LM reasons about what to do ‚Üí calls a tool ‚Üí observes the result ‚Üí reasons again ‚Üí repeats until done. ‚úÖ When to Use Tasks requiring external information, API calls, web search, database queries, or multi-step tool use. üìä Module Comparison Module LM Calls Reasoning Tools Token Usage Predict 1 ‚ùå ‚ùå Low ChainOfThought 1 ‚úÖ ‚ùå Medium ProgramOfThought 1+ ‚úÖ (via code) ‚ùå Medium ReAct Multiple ‚úÖ ‚úÖ High üéØ Choosing the Right Module ‚ùì Is it a simple, direct task? Use dspy.Predict ‚Äî fast and token-efficient. ‚ùì Does it require reasoning or analysis? Use dspy.ChainOfThought ‚Äî shows its work. ‚ùì Does it involve precise calculations? Use dspy.ProgramOfThought ‚Äî generates and runs code. ‚ùì Does it need external tools or data? Use dspy.ReAct ‚Äî can search, call APIs, etc. ‚ùì Is it a complex multi-step workflow? Create a custom dspy.Module ‚Äî next"}, {"title": "What Are Modules?", "url": "chapters/chapter-03/what-are-modules.html", "content": "Chapter 3 ¬∑ Section 2 What Are Modules? Modules are the executable components that bring signatures to life‚Äîthey make the actual LM calls and can be composed into complex systems. ~10 min read Module: The Execution Engine A Module in DSPy is an executable component that takes a signature and adds behavior‚Äîlike making LM calls, adding reasoning steps, or using tools. üí° Think of It Like a Function If a signature is like a function declaration, a module is like the implementation: # In regular Python: def calculate_tax(income: float, rate: float) -> float: # Declaration return income * rate # Implementation # In DSPy: class TaxCalculation(dspy.Signature): # Declaration \"\"\"Calculate the estimated tax.\"\"\" income: float = dspy.InputField() tax_rate: float = dspy.InputField() tax_amount: float = dspy.OutputField() calculator = dspy.Predict(TaxCalculation) # Implementation The module wraps the signature and provides the execution logic‚Äîhow to actually call the LM and handle the response. üîß What Modules Actually Do When you call a module, several things happen behind the scenes: 1Ô∏è‚É£ Construct the Prompt The module uses the signature to build a prompt from the docstring, field names, and descriptions. 2Ô∏è‚É£ Add Module-Specific Behavior Depending on the module type, it may add reasoning steps, tool calls, or multiple attempts. 3Ô∏è‚É£ Call the Language Model The complete prompt is sent to the configured LM (GPT-4, Claude, Llama, etc.). 4Ô∏è‚É£ Parse and Return Results The LM's response is parsed according to the output fields and returned as a structured object. üìù Basic Example Here's the simplest possible module usage: import dspy # 1. Configure LM lm = dspy.LM(\"openai/gpt-4o-mini\") dspy.configure(lm=lm) # 2. Define a signature class Summarize(dspy.Signature): \"\"\"Create a brief summary of the text.\"\"\" text: str = dspy.InputField() summary: str = dspy.OutputField() # 3. Create a module from the signature summarizer = dspy.Predict(Summarize) # 4. Call the module result = summarizer(text=\"DSPy is a framework for programming language models...\") # 5. Access the output print(result.summary) üí° Key insight: The module is callable! Just pass the input fields as keyword arguments and access outputs as attributes. üóÇÔ∏è Types of Modules DSPy provides several built-in modules, each adding different behavior: Module Behavior Best For dspy.Predict Direct LM call, no additional steps Simple tasks, fast responses dspy.ChainOfThought Adds reasoning before answering Complex reasoning, math, analysis dspy.ProgramOfThought Generates and executes code Computational tasks, calculations dspy.ReAct Uses tools with observation loops Tool use, external APIs, search dspy.Module Base class for custom modules Complex pipelines, custom logic üîÑ Same Signature, Different Modules One powerful aspect of DSPy is that you can use the same signature with different modules to get different behaviors: class MathProblem(dspy.Signature): \"\"\"Solve the math problem.\"\"\" problem: str = dspy.InputField() answer: str = dspy.OutputField() # Same signature, different modules: # Quick answer (might make mistakes) quick_solver = dspy.Predict(MathProblem) # With reasoning (more accurate) reasoning_solver = dspy.ChainOfThought(MathProblem) # With code execution (precise calculations) code_solver = dspy.ProgramOfThought(MathProblem) Comparing the Results problem = \"What is 847 * 293?\" # Quick answer result1 = quick_solver(problem=problem) print(f\"Predict: {result1.answer}\") # With reasoning result2 = reasoning_solver(problem=problem) print(f\"Reasoning: {result2.rationale}\") # Shows work print(f\"Answer: {result2.answer}\") # With code result3 = code_solver(problem=problem) print(f\"Code: {result3.answer}\") # 248,171 (exact) üèõÔ∏è The Module Hierarchy All DSPy modules inherit from dspy.Module: dspy.Module ‚îú‚îÄ‚îÄ dspy.Predict # Basic prediction ‚îú‚îÄ‚îÄ dspy.ChainOfThought # Adds reasoning ‚îú‚îÄ‚îÄ dspy.ChainOfThoughtWithHint # CoT with hints ‚îú‚îÄ‚îÄ dspy.ProgramOfThought # Code generation ‚îú‚îÄ‚îÄ dspy.ReAct # Tool use ‚îú‚îÄ‚îÄ dspy.MultiChainComparison # Multiple reasoning chains ‚îî‚îÄ‚îÄ YourCustomModule # Your own modules! Because they all share the same base class, they can: ‚úÖ Be Composed Together Use multiple modules in sequence within a custom module. ‚úÖ Track Parameters DSPy automatically tracks all sub-modules for optimization. ‚úÖ Be Optimized Optimizers can improve prompts across all modules uniformly. üìù Key Takeaways Modules execute signatures ‚Äî they're the \"how\" to signatures' \"what\" Different modules add different behaviors (reasoning, code, tools) Same signature, different modules = different capabilities Modules are callable ‚Äî pass inputs, get outputs All modules inherit from dspy.Module and can be composed Continue to Built-in Modules"}, {"title": "Chapter 3: Modules", "url": "chapters/chapter-03/index.html", "content": "Chapter 3 Modules Modules are the building blocks that bring signatures to life‚Äîthey implement the actual LM calls and can be composed into complex pipelines. 7 Sections ~2-3 hours ‚≠ê‚≠ê Intermediate üß© Building with Modules From simple predictors to complex multi-step pipelines What You'll Learn By the end of this chapter, you will: ‚úÖ Understand the role of modules in DSPy ‚úÖ Use all built-in modules (Predict, ChainOfThought, ReAct, etc.) ‚úÖ Create custom modules by subclassing dspy.Module ‚úÖ Compose modules into multi-step pipelines ‚úÖ Handle state and data flow between modules ‚úÖ Build production-ready modular systems Chapter Overview This chapter takes you from basic module usage to building sophisticated LM applications: üìñ What Are Modules? Understand modules as the \"how\" to signatures' \"what\"‚Äîthe executable components. üîß Built-in Modules Master Predict, ChainOfThought, ProgramOfThought, ReAct, and more. üèóÔ∏è Creating Custom Modules Build your own modules with __init__ and forward methods. üîó Module Composition Combine modules into pipelines and handle complex workflows. üèãÔ∏è Exercises Build real-world modules and pipelines hands-on. Modules vs. Signatures Understanding the relationship between signatures and modules is key: üìù Signatures # WHAT to do (specification) class Summarize(dspy.Signature): \"\"\"Summarize the text.\"\"\" text: str = dspy.InputField() summary: str = dspy.OutputField() Defines inputs/outputs Declarative contract No execution logic üß© Modules # HOW to do it (implementation) summarize = dspy.Predict(Summarize) # Or with reasoning summarize = dspy.ChainOfThought(Summarize) result = summarize(text=\"...\") Executes the signature Adds behavior (CoT, etc.) Can be composed Key Concepts Preview ‚ö° dspy.Predict Basic module that executes a signature with a single LM call. üí≠ dspy.ChainOfThought Adds step-by-step reasoning before generating the final answer. üõ†Ô∏è dspy.ReAct Enables tool use with an action-observation loop. üîÑ dspy.Module Base class for creating custom modules with complex logic. Prerequisites Before starting this chapter, ensure you have: ‚úÖ Completed Chapter 1 (DSPy Fundamentals) ‚úÖ Completed Chapter 2 (Signatures) ‚úÖ Python classes understanding (init, methods, inheritance) ‚úÖ Working DSPy environment with API keys üí° Note: This chapter builds heavily on Signatures from Chapter 2. Make sure you're comfortable creating class-based signatures. üöÄ Let's Build! Ready to turn your signatures into powerful, composable modules? Let's start! Continue to What Are Modules?"}, {"title": "Exercises", "url": "chapters/chapter-03/exercises.html", "content": "Chapter 3 ¬∑ Section 6 Exercises Build custom modules and compose pipelines for real-world tasks. 5 Exercises ~1-2 hours üèãÔ∏è Put Your Module Skills to Work These exercises progress from basic module usage to building sophisticated multi-step pipelines. Apply what you've learned! üí° Tip: Start by sketching out your module structure on paper before coding. Think about what signatures you need and how data flows between steps. Exercise 1 ‚≠ê Beginner Comparing Built-in Modules Task: Use the same signature with different built-in modules and compare the results. Requirements: Create a MathWordProblem signature with problem input and answer output Use it with dspy.Predict and dspy.ChainOfThought Test with: \"A store has 234 apples. If 67 are sold and 45 more are delivered, how many apples are there now?\" Compare the outputs and note differences Questions to Answer: Which module gave the correct answer? What additional output did ChainOfThought provide? When would you use each module? Exercise 2 ‚≠ê Beginner Your First Custom Module Task: Create a custom module that performs two-step summarization. Requirements: Create a TwoStepSummarizer module that: First extracts key points from a document Then synthesizes those points into a summary Use dspy.Prediction to return both key_points and final_summary Test with a paragraph about any topic of your choice # Your module should work like this: summarizer = TwoStepSummarizer() result = summarizer(document=\"Your long document here...\") print(result.key_points) # List of key points print(result.final_summary) # Coherent summary Exercise 3 ‚≠ê‚≠ê Intermediate Branching Pipeline Task: Build a module that routes requests to different handlers based on classification. Requirements: Create a SmartAssistant module that: First classifies the request type (question, creative, technical, casual) Routes to appropriate handler: Questions ‚Üí ChainOfThought for reasoned answers Creative ‚Üí Predict with creative tone Technical ‚Üí ChainOfThought with technical focus Casual ‚Üí Simple Predict Return the response along with which path was taken Test Cases: \"Write a short poem about clouds\" \"Why is the sky blue?\" \"Explain how a compiler works\" \"Hey, what's up?\" Exercise 4 ‚≠ê‚≠ê Intermediate Multi-Step Analysis Pipeline Task: Build a news article analyzer with multiple parallel analyses. Requirements: Create a NewsAnalyzer module that processes an article through: Sentiment analysis Key entity extraction Topic classification Bias detection Combine all analyses into a comprehensive report Return structured output with all components Test with this article: Tech giant XYZ Corp announced record profits today, exceeding analyst expectations by 15%. CEO Jane Smith attributed the success to their new AI product line, which has seen rapid adoption across enterprise customers. Critics argue the company's market dominance raises antitrust concerns, while investors remain bullish on the stock's future performance. Exercise 5 ‚≠ê‚≠ê‚≠ê Advanced Complete Application: Study Assistant Task: Build a comprehensive study assistant that helps students learn from text materials. Requirements: Create a StudyAssistant module with these capabilities: 1Ô∏è‚É£ summarize() Create a concise summary of the material 2Ô∏è‚É£ generate_questions() Create N study questions with answers 3Ô∏è‚É£ explain_concept() Explain a specific concept from the material 4Ô∏è‚É£ create_flashcards() Generate flashcard pairs (front: term/question, back: definition/answer) 5Ô∏è‚É£ full_study_guide() Run all analyses and compile a complete study guide Bonus: Add error handling for edge cases (empty text, very short content, etc.) üí° Exercise Tips üìê Plan Your Signatures First Define all the signatures you need before writing the module üß™ Test Each Step Verify each sub-module works before composing them together üîç Print Intermediate Results Debug by printing what each step returns üì¶ Use dspy.Prediction Return structured results for clean interfaces üéâ Ready to Compare? Check your solutions against ours! View Solutions"}, {"title": "Solutions", "url": "chapters/chapter-03/solutions.html", "content": "Chapter 3 ¬∑ Section 7 Solutions Complete solutions with explanations for all Chapter 3 exercises. 5 Solutions ‚ö†Ô∏è Spoiler Alert! Try to complete the exercises on your own before viewing these solutions. Solution 1 ‚≠ê Beginner Comparing Built-in Modules import dspy from dotenv import load_dotenv load_dotenv() lm = dspy.LM(\"openai/gpt-4o-mini\") dspy.configure(lm=lm) # Define the signature class MathWordProblem(dspy.Signature): \"\"\"Solve the math word problem step by step.\"\"\" problem: str = dspy.InputField() answer: str = dspy.OutputField() # Create modules with the same signature simple_solver = dspy.Predict(MathWordProblem) reasoning_solver = dspy.ChainOfThought(MathWordProblem) # The test problem problem = \"\"\" A store has 234 apples. If 67 are sold and 45 more are delivered, how many apples are there now? \"\"\" # Compare results print(\"=\" * 60) print(\"PREDICT (Simple)\") print(\"=\" * 60) result1 = simple_solver(problem=problem) print(f\"Answer: {result1.answer}\") print(\"\\n\" + \"=\" * 60) print(\"CHAIN OF THOUGHT (With Reasoning)\") print(\"=\" * 60) result2 = reasoning_solver(problem=problem) print(f\"Reasoning: {result2.rationale}\") print(f\"Answer: {result2.answer}\") # Correct answer: 234 - 67 + 45 = 212 Expected Output: PREDICT (Simple) Answer: 212 apples CHAIN OF THOUGHT (With Reasoning) Reasoning: Let me solve this step by step: 1. Starting apples: 234 2. Sold: 234 - 67 = 167 3. Delivered: 167 + 45 = 212 Answer: 212 apples üí° Key Insight ChainOfThought automatically adds the rationale field, showing its work. For math problems, both may get the right answer, but CoT is more reliable for complex problems. Solution 2 ‚≠ê Beginner Your First Custom Module import dspy from dotenv import load_dotenv load_dotenv() lm = dspy.LM(\"openai/gpt-4o-mini\") dspy.configure(lm=lm) # Signatures for the two steps class ExtractKeyPoints(dspy.Signature): \"\"\"Extract the key points from the document.\"\"\" document: str = dspy.InputField() key_points: list[str] = dspy.OutputField( desc=\"List of 3-5 key points from the document\" ) class SynthesizeSummary(dspy.Signature): \"\"\"Create a coherent summary from key points.\"\"\" key_points: str = dspy.InputField(desc=\"Key points to synthesize\") summary: str = dspy.OutputField( desc=\"Coherent 2-3 sentence summary\" ) # The custom module class TwoStepSummarizer(dspy.Module): def __init__(self): super().__init__() self.extract = dspy.Predict(ExtractKeyPoints) self.synthesize = dspy.Predict(SynthesizeSummary) def forward(self, document: str): # Step 1: Extract key points extraction = self.extract(document=document) # Step 2: Synthesize into summary points_text = \"\\n\".join([f\"- {p}\" for p in extraction.key_points]) synthesis = self.synthesize(key_points=points_text) return dspy.Prediction( key_points=extraction.key_points, final_summary=synthesis.summary ) # Test it summarizer = TwoStepSummarizer() document = \"\"\" Machine learning is a subset of artificial intelligence that enables systems to learn from data without being explicitly programmed. It uses algorithms to identify patterns and make decisions. There are three main types: supervised learning, unsupervised learning, and reinforcement learning. ML is used in many applications including image recognition, natural language processing, and recommendation systems. The field has grown rapidly due to increased computing power and data availability. \"\"\" result = summarizer(document=document) print(\"Key Points:\") for i, point in enumerate(result.key_points, 1): print(f\" {i}. {point}\") print(f\"\\nFinal Summary: {result.final_summary}\") üí° Key Insight The two-step approach often produces better summaries than a single-step approach because it forces structured analysis first. Solution 3 ‚≠ê‚≠ê Intermediate Branching Pipeline import dspy from typing import Literal from dotenv import load_dotenv load_dotenv() lm = dspy.LM(\"openai/gpt-4o-mini\") dspy.configure(lm=lm) # Signatures class ClassifyRequest(dspy.Signature): \"\"\"Classify the type of user request.\"\"\" request: str = dspy.InputField() category: Literal[\"question\", \"creative\", \"technical\", \"casual\"] = dspy.OutputField() class AnswerQuestion(dspy.Signature): \"\"\"Answer the question with clear reasoning.\"\"\" question: str = dspy.InputField() answer: str = dspy.OutputField() class CreativeResponse(dspy.Signature): \"\"\"Generate creative content as requested.\"\"\" request: str = dspy.InputField() response: str = dspy.OutputField(desc=\"Creative, engaging response\") class TechnicalExplanation(dspy.Signature): \"\"\"Provide a technical explanation.\"\"\" topic: str = dspy.InputField() explanation: str = dspy.OutputField(desc=\"Clear technical explanation\") class CasualReply(dspy.Signature): \"\"\"Reply in a casual, friendly manner.\"\"\" message: str = dspy.InputField() reply: str = dspy.OutputField() # The branching module class SmartAssistant(dspy.Module): def __init__(self): super().__init__() self.classifier = dspy.Predict(ClassifyRequest) # Different handlers for different request types self.question_handler = dspy.ChainOfThought(AnswerQuesti"}, {"title": "Demonstration Optimization", "url": "chapters/chapter-05/demonstration-optimization.html", "content": "Chapter 5 Demonstration Optimization Strategies for selecting, generating, and evaluating the best sets of demonstrations to guide your DSPy modules. Introduction Demonstrations (few-shot examples) are crucial for guiding language models. The quality, diversity, and relevance of these examples can dramatically impact performance. In DSPy, we can programmatically select and optimize demonstration sets. Demonstration Effectiveness Factors Relevance: Similarity to the current input. Diversity: Coverage of different problem types and edge cases. Quality: Correctness and clarity of the examples. Consistency: Alignment with the task instructions. Selection Algorithms 1. Similarity-based Selection Selects demonstrations that are semantically similar to the input using embeddings (e.g., SentenceTransformers). class SimilarityBasedSelector: def select(self, query, candidates, k=5): query_emb = self.encoder.encode(query) candidate_embs = self.encoder.encode(candidates) # Select top-k by cosine similarity 2. Diversity-aware Selection Selects a set of demonstrations that maximizes coverage of the problem space, avoiding redundancy. class DiversityAwareSelector: def select(self, candidates, k=5): # Greedy selection to maximize diversity metric # e.g., using localized feature distance 3. Learned Selection Trains a small model (or policy) to predict which demonstrations will yield the best performance for a given input. Demonstration Generation Bootstrap Generation Using the model itself to generate new examples based on a few seed examples. This is the core of DSPy's `BootstrapFewShot`. Synthetic Generation Creating examples from structured templates to ensure specific patterns are covered. Utility Functions & Metrics To optimize demonstrations, we need to measure their utility. Performance Utility: Direct impact on validation set accuracy. Information Utility: Entropy and mutual information provided by the set. Coverage Utility: How well the set covers the feature space of expected inputs. Next: Multistage Architectures"}, {"title": "Complex Pipeline Optimization", "url": "chapters/chapter-05/complex-pipeline-optimization.html", "content": "Chapter 5 Complex Pipeline Optimization Master advanced optimization strategies for complex, multi-stage, and hierarchical DSPy pipelines. Introduction Complex pipelines often involve intricate dependencies, loop structures, and conditional logic. Standard optimization techniques may fail to capture these nuances. This section covers advanced strategies like hierarchical optimization and coordinated tuning. Hierarchical Optimization Optimizing at different levels of abstraction allows for more efficient search. Global Level ‚îú‚îÄ‚îÄ Sub-pipeline Level ‚îÇ ‚îú‚îÄ‚îÄ Stage Level ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ Instructions ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Demonstrations class HierarchicalOptimizer: def optimize(self, pipeline): # 1. Optimize high-level structure/params self.optimize_global(pipeline) # 2. Optimize sub-pipelines for sub in pipeline.sub_pipelines: self.optimize_sub(sub) # 3. Optimize individual stages for stage in pipeline.stages: self.optimize_stage(stage) Stage-wise Tuning with Coordination Optimizing stages independently can lead to sub-optimal global results if dependencies are ignored. Coordinated tuning ensures that changes in one stage are compatible with downstream stages. class CoordinatedOptimizer: def optimize_stage(self, stage, context): # Optimize stage while respecting constraints from other stages # and checking compatibility with downstream inputs pass Resource-Aware Optimization Balancing performance with cost and latency is crucial for production systems. Multi-objective Optimization: optimizing for a weighted sum of accuracy, cost, and speed. Dynamic Scaling: adjusting the complexity of the pipeline based on current load or difficulty. Next: Instruction-Demo Interactions"}, {"title": "InPars+ Synthetic Data IR", "url": "chapters/chapter-05/inpars-plus-synthetic-data-ir.html", "content": "Chapter 5 InPars+: Synthetic Data for IR Generate high-quality synthetic queries for information retrieval systems. Key Innovations CPO Fine-tuning: Improves generator quality with Contrastive Preference Optimization DSPy Dynamic Optimization: Real-time prompt adaptation based on retrieval performance 60% Reduced Filtering: Higher initial query quality Neural IR Integration: Works seamlessly with neural re-rankers CPO Query Generator class CPOQueryGenerator(dspy.Module): def __init__(self, model_name=\"mistralai/Mistral-7B\"): super().__init__() self.query_generator = dspy.Predict( \"\"\"Generate diverse, relevant search queries based on the document. Document: {document} Generate {num_queries} unique queries that would retrieve this document.\"\"\" ) def generate_queries(self, document, num_queries=5): result = self.query_generator(document=document, num_queries=num_queries) queries = self._parse_queries(result.queries) return self._ensure_diversity(queries) Next: CustomMIPROv2"}, {"title": "CustomMIPROv2", "url": "chapters/chapter-05/custom-mipro-enhanced-optimization.html", "content": "Chapter 5 CustomMIPROv2 Enhanced multi-stage prompt optimization for production workloads. Key Enhancements Two-Stage Optimization: Separates constraint extraction from instruction generation Explicit Constraints: Users can provide domain-specific rules Mini-Batch Evaluation: Efficient evaluation using representative subsets Context-Aware: Better handling of long conversations Two-Stage Process class CustomMIPROv2: def __init__(self, teacher_model=\"gpt-4\", student_model=\"gpt-4o-mini\"): # Stage 1: Extract constraints from demonstrations self.constraint_extractor = dspy.Predict( \"Analyze task demonstrations and extract key constraints.\" ) # Stage 2: Generate instructions based on constraints self.instruction_generator = dspy.ChainOfThought( \"Generate optimized instruction based on constraints and examples.\" ) def compile(self, program, trainset, valset, metric, tips=None): # Extract constraints from training data constraints = self._extract_constraints(trainset) # Generate optimized instructions return self._generate_optimized_program(program, constraints, tips) Next: Automatic Prompt Optimization"}, {"title": "KNNFewShot: Similarity-Based Selection", "url": "chapters/chapter-05/knn-fewshot.html", "content": "Chapter 5 KNNFewShot Optimization Dynamically select the most relevant demonstrations for each query using K-Nearest Neighbors. Introduction KNNFewShot is a dynamic DSPy optimizer that uses the K-Nearest Neighbors algorithm to select the most relevant examples for each query. Unlike BootstrapFewShot which selects a static set of examples that work well on average, KNNFewShot tailors the context to every single input. Core Concept Embed: Convert all training examples to vector embeddings. Query: Embed the incoming user query. Search: Find the K most similar training examples in the vector space. Select: Use those specific examples as the few-shot demonstrations for that query. ‚ú® Key Advantage: KNNFewShot is highly effective when your task is too broad to be covered by a single set of 5-10 fixed examples (e.g., Open Domain QA, broad classification). üíª Basic Usage import dspy from dspy.teleprompt import KNNFewShot # 1. Define Program & Data class QAModule(dspy.Module): def __init__(self): super().__init__() self.generate = dspy.Predict(\"question, context -> answer\") def forward(self, question): return self.generate(question=question) trainset = [ dspy.Example(question=\"Capital of France?\", answer=\"Paris\", topic=\"geography\"), dspy.Example(question=\"Who wrote Hamlet?\", answer=\"Shakespeare\", topic=\"literature\"), # ... Assume 100+ diverse examples ] # 2. Configure Optimizer # 'k' is the number of neighbors to retrieve optimizer = KNNFewShot(k=3) # 3. Compile # KNNFewShot doesn't \"train\" in the traditional sense; # it indexes your training set. compiled_qa = optimizer.compile(QAModule(), trainset=trainset) # 4. Infer # For \"Capital of Germany?\", it will likely retrieve the \"Capital of France\" example result = compiled_qa(question=\"What is the capital of Germany?\") print(result.answer) ‚öôÔ∏è Advanced Configuration Custom Similarity Metrics By default, KNNFewShot uses standard embeddings (like OpenAI's). You can define custom similarity logic. from sentence_transformers import SentenceTransformer import numpy as np # Custom encoder encoder = SentenceTransformer('all-MiniLM-L6-v2') def custom_similarity(query, example): q_emb = encoder.encode(query) ex_emb = encoder.encode(example.question) return np.dot(q_emb, ex_emb) # Simplistic dot product optimizer = KNNFewShot( k=5, similarity_fn=custom_similarity, vectorizer=encoder.encode ) Parameters Parameter Type Description k int Number of neighbors to find (default: 3) vectorizer Callable Function to turn examples into vectors embedding_model str Model name (e.g., \"text-embedding-ada-002\") ‚ú® Best Practices & Pitfalls 1. Choose the Right 'k' Too few samples (k=1) might lead to overfitting on a single similar example. Too many (k=20) might dilute the context or exceed standard token limits. A range of 3-7 is usually optimal. 2. Data Hygiene Your retrieval is only as good as your database. Ensure your trainset is: Clean: No typos or bad formatting. Diverse: Covers the full search space. Normalized: Consistent capitalization and spacing improves similarity matching. 3. Avoid Self-Selection If your training set contains the exact query you are testing on (during evaluation), KNN might retrieve the answer directly. Ensure your validation/test sets are distinct from the indexed training set. Next: Fine-tuning"}, {"title": "Instruction-Demonstration Interactions", "url": "chapters/chapter-05/instruction-demonstration-interactions.html", "content": "Chapter 5 Instruction-Demonstration Interactions Analyze and optimize the interplay between instructions and demonstrations to create synergistic prompts. Introduction Instructions and demonstrations are the key drivers of LLM performance. When optimized in isolation, we miss synergistic effects or accidentally introduce redundancy. This section explores how to optimize them jointly. Theoretical Foundations Interaction Framework Performance can be modeled as: Response = f(Model, I, D, Query) Where I is the instruction and D is the demonstration set. Interactions can be: Complementary: They provide different, enhancing information. Redundant: They repeat the same rules, which can waste tokens. Contradictory: They give conflicting signals (e.g., \"be concise\" but uses long examples). Empirical Analysis Using controlled experiments to detect synergy or conflict. class InteractionAnalyzer: def analyze(self, instructions, demonstrations, test_set): # Grid search over instructions x demonstrations # to find optimal pairs and interaction effects pass Optimization Strategies Redundancy Reduction If demonstrations clearly show a pattern, the instruction can be shortened. Conversely, a strong instruction may reduce the need for many examples. Joint Optimization Iteratively refining both components to maximize the interaction term. class InteractionAwareOptimizer: def optimize(self, pipeline): # Jointly update instruction text and demonstration selection pass Next: Prompts as Hyperparameters"}, {"title": "Fine-Tuning Small LMs", "url": "chapters/chapter-05/finetuning.html", "content": "Chapter 5 Fine-Tuning Small LMs Adapt small, efficient models like Llama 3 or Mistral to perform specific tasks as well as giant models. Introduction While powerful models like GPT-4 are excellent for prototyping, they are often too slow, expensive, or private for production. Fine-tuning allows you to take a smaller model (like Mistral-7B or Llama-3-8B) and specialize it on your specific task, often achieving comparable performance at a fraction of the cost. ü§î When to Fine-Tune? Scenario Fine-Tuning is... Why? High Volume / Real-time ‚úÖ Excellent Low latency, low per-token cost. Specialized Domain ‚úÖ Excellent Models learn jargon and formats better than via prompting. Complex Reasoning ‚ö†Ô∏è Challenging Small models struggle with deep reasoning even after fine-tuning. Frequent Updates ‚ùå Poor Retraining is slow compared to updating a prompt. üõ†Ô∏è Setting Up QLoRA QLoRA (Quantized Low-Rank Adaptation) is the industry standard for efficient fine-tuning. It allows you to fine-tune a 7B model on a single consumer GPU. # Prerequisites !pip install torch transformers datasets accelerate peft bitsandbytes from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training from transformers import AutoModelForCausalLM, AutoTokenizer def setup_qlora(model_name=\"mistralai/Mistral-7B-v0.1\"): # Load model in 4-bit to save memory model = AutoModelForCausalLM.from_pretrained( model_name, load_in_4bit=True, device_map=\"auto\" ) # Configure LoRA lora_config = LoraConfig( r=16, # Rank: Higher = more parameters to train lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], # Target attention layers lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\" ) model = prepare_model_for_kbit_training(model) model = get_peft_model(model, lora_config) return model üîå Integration with DSPy Once you have a fine-tuned model, using it in DSPy is straightforward. You simply wrap it in a `dspy.LM` class. class FineTunedLLM(dspy.LM): def __init__(self, model, tokenizer): self.model = model self.tokenizer = tokenizer def __call__(self, prompt, **kwargs): inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") outputs = self.model.generate(**inputs, max_new_tokens=200) return [self.tokenizer.decode(outputs[0], skip_special_tokens=True)] # Usage ft_model = FineTunedLLM(my_finetuned_model, my_tokenizer) dspy.configure(lm=ft_model) ‚ú® Synergy: Fine-Tuning + Prompt Opt The most powerful systems typically combine both approaches: Step 1: Fine-Tune the model to understand the basic format and domain constraints. Step 2: Prompt Optimize (using BootstrapFewShot or MiPRO) to find the best few-shot examples that steer this new model. üöÄ Result: Research shows this combination can achieve 2-26x improvements over baseline, unlocking capabilities neither method can achieve alone. Next: Choosing Optimizers"}, {"title": "Bayesian Optimization", "url": "chapters/chapter-05/bayesian-optimization.html", "content": "Chapter 5 Bayesian Optimization Navigate the prompt space intelligently using Gaussian Processes to balance exploration and exploitation. Introduction Bayesian Optimization (BO) is a powerful global optimization technique that excels at optimizing expensive black-box functions with few evaluations. In the context of DSPy, BO provides a principled approach to efficiently search for optimal prompt configurations. Core Components Search Space: The domain of possible configurations (e.g., instruction styles, temperature). Surrogate Model: A probabilistic model (usually a Gaussian Process) approximating the performance landscape. Acquisition Function: guiding the selection of next points. # Defining a search space for Bayesian Optimization def _define_search_space(self): return { \"instruction_length\": {\"type\": \"discrete\", \"values\": [10, 20, 30]}, \"instruction_style\": {\"type\": \"categorical\", \"values\": [\"direct\", \"detailed\"]}, \"temperature\": {\"type\": \"continuous\", \"bounds\": [0.0, 1.0]}, } Gaussian Process Surrogate The Gaussian Process (GP) models both the expected performance and the uncertainty across the search space. This allows the optimizer to identify regions that are likely to be high-performing (high mean) or are unexplored (high uncertainty). # Example of fitting a GP surrogate self.surrogate_model.fit(self.X_observed, self.y_observed) mean, std = self.surrogate_model.predict(new_configs, return_std=True) Acquisition Functions Acquisition functions determine where to sample next by balancing: Exploitation: Sampling where the model predicts high value. Exploration: Sampling where the model is uncertain. Common functions include Expected Improvement (EI) and Upper Confidence Bound (UCB). Practical Implementation Implementing a full Bayesian Optimization pipeline involves defining the task, the metric, and iteratively updating the model. # Running the optimization loop for iteration in range(max_iterations): # Fit surrogate self.surrogate_model.fit(X_observed, y_observed) # Select next point using acquisition function next_config = self._select_next_configuration() # Evaluate score = self._evaluate_configuration(next_config) self._add_observation(next_config, score) Next: Comprehensive Examples"}, {"title": "Prompts as Hyperparameters", "url": "chapters/chapter-05/prompts-as-hyperparameters.html", "content": "Chapter 5 Prompts as Hyperparameters Treat prompts as trainable hyperparameters optimized through automated search. The Hyperparameter Framework In traditional ML, hyperparameters like learning rate are tuned for optimal performance. In DSPy, prompts themselves become hyperparameters. This transforms prompt engineering from an art into a systematic optimization problem. Types of prompt hyperparameters include: instruction templates, few-shot example selection, formatting patterns, task decomposition, and reasoning steps. Auto-Optimization Architecture from dataclasses import dataclass from typing import List, Dict, Any @dataclass class PromptHyperparameters: instruction_template: str example_selection_strategy: str formatting_pattern: str reasoning_guidance: str task_decomposition: List[str] class PromptHyperparameterOptimizer: def __init__(self, base_program, metric_fn, search_space): self.base_program = base_program self.metric_fn = metric_fn self.search_space = search_space def optimize(self, trainset, valset, num_iterations=50): best_params, best_score = None, 0.0 for _ in range(num_iterations): current_params = self._sample_hyperparameters() program = self._apply_hyperparameters(current_params) score = self._evaluate(program, valset) if score > best_score: best_score = score best_params = current_params return best_params Next: Minimal Data Pipelines"}, {"title": "Multistage Optimization Theory", "url": "chapters/chapter-05/multistage-optimization-theory.html", "content": "Chapter 5 Multistage Optimization Theory Explore the theoretical foundations and mathematical frameworks for optimizing cascaded language model programs. Introduction Multi-stage optimization addresses the challenge of optimizing programs with multiple interconnected modules. Each stage's output becomes the input for subsequent stages, introducing dependencies and potential error propagation that simple optimization strategies often overlook. Theoretical Foundations The Optimization Problem The goal is to find parameters $\\theta$ that maximize the expected performance of a program $P$ composed of stages $f_1, ..., f_K$: P(x; Œ∏) = f_K(...f_1(x; Œ∏_1)...; Œ∏_K) Optimization Landscapes Optimizing these programs is difficult due to: Non-convexity: Many local optima exist, especially with discrete prompt parameters. Curse of Dimensionality: The search space grows exponentially with the number of stages. Stage-wise Dependencies: Changes in early stages ripple through the entire pipeline. Optimization Frameworks 1. Coordinate Descent Optimizes one stage at a time while keeping others fixed. It is simple and effective but may get stuck in local optima. def coordinate_descent(program, trainset): for round in range(num_rounds): for stage in program.stages: stage.optimize(trainset) # Optimize while others are frozen 2. Decomposition Breaks the problem into independent subproblems. This simplifies optimization but ignores cross-stage interactions. 3. End-to-End Differentiable Uses gradients to optimize all stages simultaneously. This requires differentiable components (e.g., soft prompts) but can find better global solutions. Convergence & Emperical Analysis Different algorithms offer different trade-offs in terms of convergence speed and solution quality. Algorithm Convergence Rate Sample Efficiency Coordinate Descent O(1/t) Low Bayesian Optimization O(‚àö(n)) High Gradient-based O(1/t¬≤) Highest Case Study: HotpotQA In Multi-hop Question Answering, joint optimization strategies like MIPRO significantly outperform independent stage optimization by accounting for error propagation. Next: Instruction Tuning Frameworks"}, {"title": "Automatic Prompt Optimization", "url": "chapters/chapter-05/automatic-prompt-optimization-research.html", "content": "Chapter 5 Automatic Prompt Optimization When AI outperforms human prompt engineers. Key Research Findings VMware research (2024) demonstrated several surprising insights: LLM-Generated Prompts Outperform Human Ones: Automatic optimizers created prompts that humans would likely reject, yet they performed better \"Positive Thinking\" Prompts Are Suboptimal: Manual additions like \"This will be fun!\" provide minimal benefit Open Source Models Can Self-Optimize: Even 7B parameter models (Mistral-7B) effectively optimize prompts with just 100 test samples DSPy Implementation class AutomaticPromptOptimizer: def __init__(self, base_model=\"gpt-3.5-turbo\", optimizer_model=\"mixtral-8x7b\"): self.base_lm = dspy.OpenAI(model=base_model) self.optimizer_lm = dspy.HFClientVLLM(model=optimizer_model) def discover_eccentric_prompts(self, task_description, examples): \"\"\"Generate unexpected but effective prompts based on research insights.\"\"\" prompt_generator = dspy.ChainOfThought( \"task_description, examples -> creative_system_prompt, persona_prompt\" ) return prompt_generator(task_description=task_description, examples=examples) Next: Exercises"}, {"title": "CoPA Compiler Method", "url": "chapters/chapter-05/copa-compiler-method.html", "content": "Chapter 5 COPA Compiler Optimization Learn to implement the COPA framework: treating compilation (demonstrations) and prompt optimization (instructions) as a unified, iterative process. Introduction COPA (Compiler and Prompt Optimization) is a synergistic framework that alternates between optimizing model parameters (via demonstration selection/compilation) and optimizing prompt instructions. Instead of choosing either good examples or good instructions, COPA optimizes both in a coordinated loop. üîÑ The Cycle: COPA works by treating the optimization space as unified. It iterates between a \"Compilation Phase\" (finding better examples) and a \"Prompt Phase\" (finding better descriptions), transferring insights between the two. Core Principles Unified Optimization Space: Prompts and demonstrations are optimized together. Iterative Refinement: Phase 1 (Compilation) informs Phase 2 (Content/Prompt), and vice versa. Knowledge Transfer: Insights from compilation (e.g., \"these examples work best\") help the prompt optimizer (e.g., \"therefore, we should emphasize this rule\"). üíª Implementation Strategy Implementing COPA involves creating a custom optimizer class that orchestrates the two phases. class COPAOptimizer: def optimize(self, program, trainset, valset): # Iterative Loop for i in range(self.max_iterations): # Phase 1: Compilation (Demonstrations) # Uses BootstrapFewShot to find best examples if i % 2 == 0: print(\"Phase 1: Compilation\") program = self.compilation_phase(program, trainset) # Phase 2: Prompt Optimization (Instructions) # Uses MIPRO to refine instructions based on current examples else: print(\"Phase 2: Prompt Optimization\") program = self.prompt_phase(program, trainset) # Check for convergence... return program ‚ú® Best Practices Start with Compilation: It's usually better to anchor the model with good examples first before refining the instructions. Balanced Iterations: 2-3 full cycles (4-6 total phases) are usually sufficient for convergence. Use Constraints: Applying constraints (as learned in the previous section) helps stabilize the alternating optimization steps. Next: CoPA Method"}, {"title": "Reflective Prompt Evolution", "url": "chapters/chapter-05/reflective-prompt-evolution.html", "content": "Chapter 5 Reflective Prompt Evolution (RPE) Discover how RPE uses evolutionary algorithms and self-reflection to evolve superior prompts without gradients. Introduction Reflective Prompt Evolution (RPE) is an innovative optimizer that treats prompt engineering as an evolutionary process. Unlike standard gradient-based optimization, RPE maintains a population of prompt candidates and evolves them using mutation and selection, guided by the language model's own ability to \"reflect\" on what's working and what isn't. üß¨ Core Concept: RPE simulates \"Survival of the Fittest\" for prompts. The fittest prompts (those that perform best on your metric) survive and reproduce (mutate) to form the next generation. üöÄ What Makes RPE Special? Population-Based: Explores multiple directions simultaneously, avoiding local optima better than single-path greedy search. Self-Reflection: Uses the LM to critique its own prompts (\"Why did I fail this example?\") to generate smarter mutations. Gradient-Free: Works with any black-box LM API, as it doesn't require access to model weights or gradients. üíª Basic Usage import dspy from dspy.teleprompter import ReflectivePromptEvolution # 1. Define your module (e.g., ChainOfThought) class Reasoner(dspy.Module): def __init__(self): self.prog = dspy.ChainOfThought(\"question -> answer\") def forward(self, question): return self.prog(question=question) # 2. Configure RPE optimizer = ReflectivePromptEvolution( metric=your_metric_function, population_size=10, # Keep 10 active candidates generations=5, # Evolve for 5 rounds mutation_rate=0.3, # 30% chance to mutate a prompt selection_pressure=0.5 # Keep top 50% performers ) # 3. Compile optimized_reasoner = optimizer.compile( Reasoner(), trainset=train_examples, valset=val_examples ) ‚öôÔ∏è The Evolution Process Initialization: Create an initial population of diverse prompts (e.g., using different instruction styles). Evaluation (Fitness): Test all candidates on the training set and assign a fitness score based on your metric. Reflection: For lower-performing prompts, ask the LM to analyze why they failed. \"Analyze this prompt's performance. Which instructions were unclear? What reasoning was missing?\" Mutation: Create new candidates by applying changes suggested by the reflection (e.g., \"Add a step to check for negative numbers\"). Selection: Keep the best prompts from the current pool and the new mutations. Repeat: Continue for N generations or until convergence. üß™ Advanced Tactics Diversity Maintenance To prevent the population from becoming too similar (converging too early), RPE can enforce diversity constraints. It calculates the cosine similarity between prompt embeddings and penalizes or removes redundant candidates. Custom Mutations You can define domain-specific mutation operators. For a coding task, you might add a mutation that specifically inserts \"Check for edge cases\" instructions. class CustomMutationOperator: def domain_specific_mutation(self, prompt, domain): if domain == \"code\": return prompt + \"\\nEnsure time complexity is O(n).\" return prompt ü§î When to Use RPE? Scenario RPE Suitability Reason Complex Reasoning ‚úÖ High Evolution finds creative reasoning paths humans miss. Simple Classification ‚ö†Ô∏è Medium Overkill; BootstrapFewShot is faster and sufficient. Black-Box APIs ‚úÖ High No gradients needed, efficient use of API calls via reflection. Next: CoPA Compiler Method"}, {"title": "Choosing Optimizers", "url": "chapters/chapter-05/choosing-optimizers.html", "content": "Chapter 5 ¬∑ Section 6 Choosing Optimizers Decision guide, trade-offs, and optimization synergy analysis. ~15 min read üìã Quick Reference Guide Optimizer Best For Data Speed Performance None (Baseline) Simple tasks None Fastest Baseline BootstrapFewShot General improvement 10-100 Fast Good KNNFewShot Context-sensitive 100+ Medium Good MIPRO Maximum performance 20-200 Slow Excellent Fine-Tuning Production, cost-sensitive 1000+ Very Slow Excellent üéØ Decision Framework Step 1: Analyze Your Constraints üìä Data Constraints How many examples? What quality and diversity? ‚è±Ô∏è Time Budget Minutes, hours, or days for optimization? üéØ Performance Target What accuracy improvement do you need? üîß Task Complexity Simple classification or complex reasoning? üì¶ Use Case Recommendations Use Case 1: Quick Prototype üöÄ Scenario: Building an MVP with 50 examples and 2 days deadline Recommendation: BootstrapFewShot with max_bootstrapped_demos=8 optimizer = BootstrapFewShot( metric=answer_accuracy, max_bootstrapped_demos=8, max_labeled_demos=4 ) prototype = optimizer.compile(SupportBot(), trainset=examples) Use Case 2: Enterprise RAG System üè¢ Scenario: 10,000 examples, high accuracy (95%+) required Recommendation: MIPRO with auto=\"heavy\", consider fine-tuning for cost # Stage 1: Quick baseline baseline = BootstrapFewShot(metric=f1_score).compile( LegalRAG(), trainset=trainset[:1000] ) # Stage 2: Advanced optimization optimizer = MIPRO(metric=weighted_metric, auto=\"heavy\") optimized = optimizer.compile(LegalRAG(), trainset=trainset) Use Case 3: Real-time Classification ‚ö° Scenario: 1000+ requests/sec, <100ms latency Recommendation: KNNFewShot with caching, or fine-tuned small model optimizer = KNNFewShot( k=3, similarity_fn=semantic_similarity, cache_embeddings=True # Speed optimization ) classifier = optimizer.compile(ContentModerator(), trainset=examples) üìà Expected Performance Patterns Optimizer Accuracy Gain Compile Time Best For Baseline 0% < 1s Quick testing BootstrapFewShot 5-15% 1-5 min Most tasks KNNFewShot 5-12% 1-2 min Context tasks MIPRO 10-25% 5-30 min Complex tasks Fine-Tuning 15-30% 1-4 hrs Production üîÑ Progressive Optimization Strategy Start simple and progressively add optimization: def progressive_optimization(program, trainset, valset): \"\"\"Start simple and progressively add optimization.\"\"\" stages = [ {\"name\": \"Baseline\", \"optimizer\": None}, {\"name\": \"BootstrapFewShot\", \"optimizer\": BootstrapFewShot(metric=accuracy_metric), \"config\": {\"max_bootstrapped_demos\": 4}}, {\"name\": \"KNNFewShot\", \"optimizer\": KNNFewShot(k=3)}, {\"name\": \"MIPRO\", \"optimizer\": MIPRO(metric=accuracy_metric, auto=\"medium\")}, ] best_program = program best_score = 0 for stage in stages: print(f\"\\n=== Stage: {stage['name']} ===\") if stage['optimizer']: compiled = stage['optimizer'].compile( best_program, trainset=trainset, **stage.get('config', {}) ) else: compiled = program score = evaluate(compiled, valset) print(f\"Score: {score:.3f}\") if score > best_score: best_score = score best_program = compiled print(\"‚úì New best model!\") return best_program üìê Optimization Order Effects When combining strategies, order matters significantly: ‚úÖ Optimal order: Fine-tuning ‚Üí Prompt Optimization This achieves 3.5x improvement beyond individual approaches! ‚ùå Suboptimal order: Prompt Optimization ‚Üí Fine-tuning Only achieves 1.8x improvement (prompts don't transfer well) # OPTIMAL ORDER: Fine-tune first finetuned = finetune(base_model, trainset) dspy.settings.configure(lm=finetuned) optimizer = MIPRO(metric=accuracy, auto=\"medium\") compiled = optimizer.compile(program, trainset=trainset) # Result: 3.5x improvement! üîó Synergy Quantification Combined optimization achieves synergistic effects: Task Baseline FT Only PO Only Combined Synergy MultiHopQA 12% 28% 20% 45% 3.5x GSM8K Math 11% 32% 22% 55% 2.8x AQuA 9% 35% 28% 69% 3.4x üí° Key insight: Combined optimization exceeds the sum of individual improvements‚Äîthis is synergy! üå≤ Quick Decision Tree Starting optimization? ‚îÇ ‚îú‚îÄ‚îÄ Have < 20 examples? ‚îÇ ‚îî‚îÄ‚îÄ Use: BootstrapFewShot (or no optimization) ‚îÇ ‚îú‚îÄ‚îÄ Have 20-100 examples? ‚îÇ ‚îî‚îÄ‚îÄ Need max performance? ‚Üí MIPRO ‚îÇ ‚îî‚îÄ‚îÄ Need speed? ‚Üí BootstrapFewShot ‚îÇ ‚îú‚îÄ‚îÄ Have 100+ examples? ‚îÇ ‚îî‚îÄ‚îÄ Context-sensitive task? ‚Üí KNNFewShot ‚îÇ ‚îî‚îÄ‚îÄ Complex reasoning? ‚Üí MIPRO ‚îÇ ‚îî‚îÄ‚îÄ Have 1000+ examples AND production needs? ‚îî‚îÄ‚îÄ Consider: Fine-tuning + MIPRO üìù Key Takeaways Start with BootstrapFewShot‚Äîit's fast and effective for most tasks Use MIPRO when maximum performance is critical KNNFewShot excels at context-sensitive tasks with large datasets Order matters: Fine-tune first, then prompt optimize Combined optimization achieves synergistic (3x+) improvements Next: Constraint-Driven Optimization"}, {"title": "CoPA Method", "url": "chapters/chapter-05/copa-method.html", "content": "Chapter 5 CoPA Method Combined Fine-Tuning and Prompt Optimization: Discover how to achieve 2-26x performance gains by synergizing model weight updates with prompt optimization. Introduction COPA (Compiler and Prompt Optimization Algorithm) represents the cutting edge of DSPy optimization by combining two powerful techniques: fine-tuning and prompt optimization. While each technique individually provides significant improvements, COPA demonstrates that combining them creates synergistic effects that exceed additive improvements, often achieving 2-26x performance gains. The Joint Optimization Problem Traditional DSPy optimization operates at a single level: either you fine-tune model weights OR you optimize prompts. COPA treats this as a two-level parameter problem: Level 1 - Weights (W): Model parameters modified through fine-tuning. Level 2 - Prompts (P): Instructions and demonstrations optimized by DSPy. # The joint optimization objective: # argmax_{W, P} E[metric(program(W, P), examples)] üíª Implementing COPA A basic COPA implementation optimizes in two stages: first fine-tuning the base model, then applying prompt optimization. class COPAOptimizer: def optimize(self, program, trainset, valset, finetune_data=None): \"\"\" Two-stage optimization: 1. Fine-tune the base model 2. Apply prompt optimization to the fine-tuned model \"\"\" # Stage 1: Fine-tuning print(\"Stage 1: Fine-tuning base model...\") finetuned_model = self._finetune( trainset if finetune_data is None else finetune_data ) # Configure DSPy to use fine-tuned model finetuned_lm = self._create_dspy_lm(finetuned_model) dspy.settings.configure(lm=finetuned_lm) # Stage 2: Prompt optimization print(\"Stage 2: Applying prompt optimization...\") if self.prompt_optimizer == \"mipro\": optimizer = MIPRO( metric=self.metric, num_candidates=15, auto=\"medium\" ) else: optimizer = BootstrapFewShot( metric=self.metric, max_bootstrapped_demos=8 ) compiled_program = optimizer.compile( program, trainset=trainset, valset=valset ) return compiled_program, finetuned_model Monte Carlo Methods COPA can use Monte Carlo methods to efficiently explore the vast space of possible prompt configurations. class MonteCarloPromptExplorer: def explore(self, program, prompt_templates, demo_pool, metric, trainset): results = [] for _ in range(self.num_samples): # Sample instruction and demonstrations instruction = np.random.choice(prompt_templates) demos = np.random.choice(demo_pool, size=num_demos, replace=False) # Configure and evaluate config = {\"instruction\": instruction, \"demonstrations\": demos} score = self._evaluate_config(program, config, metric, trainset) results.append({\"config\": config, \"score\": score}) return max(results, key=lambda x: x[\"score\"]) Bayesian Optimization For even more efficiency, Bayesian optimization uses a Gaussian Process surrogate model to intelligently search the prompt configuration space. # Bayesian optimization loop for iteration in range(n_iterations): # Fit surrogate model surrogate = self._fit_surrogate() # Find next point using acquisition function (e.g., Expected Improvement) next_config = self._maximize_acquisition(surrogate, prompt_space) # Evaluate and update observations score = self._evaluate(program, next_config, metric, valset) self.observed_configs.append(next_config) self.observed_scores.append(score) üöÄ Performance Benchmarks Combining fine-tuning and prompt optimization yields impressive results: Model Baseline Fine-Tuning Only Prompt Opt Only COPA Improvement Llama-7B 12.3% 28.5% 19.7% 45.2% 3.7x Mistral-7B 18.7% 35.2% 31.4% 62.8% 3.4x Next: Joint Optimization"}, {"title": "Minimal Data Pipelines", "url": "chapters/chapter-05/minimal-data-pipelines.html", "content": "Chapter 5 Minimal Data Pipelines Training sophisticated models with as few as 10 labeled examples. The Challenge of Limited Data Many real-world scenarios require training with severely limited labeled data. DSPy provides a comprehensive framework for building minimal data training pipelines that combine multiple optimization strategies. Core principles: Data Efficiency, Strategy Diversity, Robust Validation, Confidence Awareness, and Adaptability. Pipeline Architecture class MinimalDataTrainingPipeline: def __init__(self, config): self.config = config # num_examples, task_type, strategies def execute_pipeline(self, base_program, examples): # Stage 1: Data Analysis data_analysis = self._analyze_training_data(examples) # Stage 2: Strategic Data Augmentation augmented_data = self._augment_data(examples) # Stage 3: Multi-Strategy Optimization optimized = self._optimize(base_program, augmented_data) # Stage 4: Robust Validation validated = self._validate(optimized) return validated Next: GEPA Optimization"}, {"title": "State-Space Prompt Optimization", "url": "chapters/chapter-05/state-space-prompt-optimization.html", "content": "Chapter 5 State-Space Prompt Optimization Model prompt optimization as a classical AI graph search problem. The Prompt Space as a Graph This approach models prompts as nodes in a graph where: States: Individual prompt strings Edges: Transformation operations (make concise, add examples, etc.) Heuristic: Performance score on a dev set Goal: Find the prompt with maximum performance PromptNode Structure @dataclass class PromptNode: prompt_text: str parent: Optional['PromptNode'] = None operator_used: Optional[str] = None score: Optional[float] = None def get_path(self) -> List[str]: \"\"\"Get the sequence of operators used to reach this node.\"\"\" path = [] node = self while node.parent is not None: path.append(node.operator_used) node = node.parent return list(reversed(path)) Next: InPars+ Synthetic Data"}, {"title": "Joint Optimization", "url": "chapters/chapter-05/joint-optimization.html", "content": "Chapter 5 Joint Optimization Unleash the power of synergy by optimizing model weights and prompts simultaneously. Introduction Joint optimization in DSPy represents a paradigm shift from treating fine-tuning and prompt optimization as separate processes. Instead, it recognizes that these two optimization dimensions are deeply interconnected and can be optimized together to achieve superior performance. Theoretical Foundations Why Joint Optimization Matters Traditional sequential optimization (fine-tune then prompt-optimize) often gets stuck in suboptimal local minima. Joint optimization addresses this by: Simultaneous Exploration: Exploring the combined space of parameters and prompts. Coordinated Updates: Ensuring parameter and prompt updates complement each other. Global Optimum Seeking: Working toward a true global optimum across both dimensions. Mathematical Framework The objective is to maximize the joint likelihood: L(Œ∏, p) = Œ£_i log P(y_i | x_i; Œ∏, p) + Œª1 * R1(Œ∏) + Œª2 * R2(p) Where Œ∏ represents model parameters and p represent prompts. Joint Optimization Strategies 1. Alternating Optimization The most common approach where parameters and prompts are optimized in alternating phases: class AlternatingJointOptimizer(JointOptimizationFramework): def optimize(self, train_data, val_data, num_epochs=10): for epoch in range(num_epochs): # Phase 1: Parameter optimization self._optimize_parameters(train_data, val_data, steps=5) # Phase 2: Prompt optimization self._optimize_prompts(train_data, val_data, steps=1) # Evaluate combined performance combined_metric = self._evaluate(val_data) 2. Simultaneous Gradient-Based Optimization For soft prompts that can be optimized with gradients alongside model weights: class SimultaneousJointOptimizer(JointOptimizationFramework): def optimize(self, train_data, val_data): # Forward pass with both parameter and prompt gradients outputs = self.forward(batch) loss = self.compute_joint_loss(outputs, batch) # Backward pass self.param_optimizer.zero_grad() self.prompt_optimizer.zero_grad() loss.backward() # Update both self.param_optimizer.step() self.prompt_optimizer.step() This is Experimental It is important to note that full joint optimization (simultaneous updates) is an advanced and often experimental technique. In many practical DSPy workflows, alternating or \"Coordinate Descent\" style optimization (like COPA) is more stable and easier to implement. DSPy Joint Optimizer Example class DSPyJointOptimizer(dspy.Module): def optimize(self, trainset, valset, metric=None): # Initialize optimization state state = OptimizationState( model=self.base_model, prompts=self._initialize_prompts(), trainset=trainset ) # Run optimization best_state = self.coordinator.optimize(state) return best_state.model, best_state.prompts Advanced Techniques Curriculum Joint Optimization: Gradually increasing the complexity of data or the \"freedom\" of the optimization parameters over time. Meta-Learning: Using meta-learning to find good initializations for both prompts and weights that adapt quickly to new tasks. Next: Monte Carlo Optimization"}, {"title": "Constraint-Driven Optimization", "url": "chapters/chapter-05/constraint-driven-optimization.html", "content": "Chapter 5 Constraint-Driven Optimization Learn to incorporate runtime constraints and validation directly into your optimization process for robust, high-quality DSPy systems. üìã Prerequisites Previous Section: Choosing Optimizers Chapter 3: Assertions Module (for understanding constraints) Concept: Basic optimization theory Level: Advanced Introduction Constraint-driven optimization extends DSPy's standard optimization by ensuring that your compiled programs not only maximize a metric (like accuracy) but also adhere to strict requirement boundaries (like format, length, or safety). üí° The Shift: Instead of just asking \"Did it get the right answer?\", we ask \"Did it get the right answer and follow the rules?\" Core Concepts 1. Constraint-Aware Metrics The simplest way to enforce constraints is to embed them into your metric function. If a prediction violates a hard constraint, it receives a score of 0, regardless of the content's quality. def constrained_metric(example, pred, trace=None): # 1. Check Hard Constraints (Format, Type) if not validate_format(pred): return 0.0 # Immediate failure # 2. Check Soft Constraints (Style, Length) penalty = 0.0 if len(pred.output) > 500: penalty += 0.1 # 3. Calculate Base Score base_score = answer_f1_score(example, pred) return max(0.0, base_score - penalty) 2. Hard vs. Soft Constraints Type Description Example Handling Strategy Hard Must pass for the output to be valid. Valid JSON syntax, specific output schema. Return 0 in metric if failed. Use Assertions. Soft Preferences that improve quality. Conciseness, tone, specific vocabulary. Apply scalar penalties to the metric score. üöÄ Optimization Strategies 1. Constraint-Guided Example Selection When using BootstrapFewShot, you can ensure that only examples satisfying your constraints are selected as demonstrations. 2. Progressive Constraint Enforcement For difficult constraints, it helps to start leniently and tighten requirements over time. This prevents the optimizer from getting stuck early on when few candidates satisfy all rules. # Concept: Gradual tightening levels = [ [validate_format], # Level 1: Just format [validate_format, validate_length], # Level 2: + Length [validate_format, validate_length, validate_style] # Level 3: All ] for constraints in levels: # Optimizing loop... pass Advanced Techniques Constraint Transfer Learning If you have successfully optimized constraints for one task (e.g., \"Always output JSON\"), you can transfer those learned patterns (instructions or demonstrations) to a new, related task. RAG System Optimization For RAG systems, constraints often involve citation handling and hallucination prevention. constraints = { 'min_evidence': 2, # Cite at least 2 sources 'max_hallucination': 0.1 # <10% hallucinated content } üìä Monitoring and Analysis It's crucial to track Violation Rates during optimization. If 90% of your candidates are failing a specific constraint, you may need to: Relax the constraint temporarily. Improve the base prompt instructions. Provide better manually-written demonstrations that satisfy the constraint. Next: Reflective Prompt Evolution"}, {"title": "GEPA Optimization", "url": "chapters/chapter-05/gepa-genetic-pareto-optimization.html", "content": "Chapter 5 GEPA: Genetic-Pareto Optimization Multi-objective prompt optimization using genetic algorithms and Pareto fronts. Key Concepts GEPA merges genetic algorithms (selection, crossover, mutation, elitism) with Pareto optimization (multi-objective trade-offs). Its distinguishing feature is the use of natural language reflections to guide the evolutionary process. Basic Usage from gepa import GEPAOptimizer optimizer = GEPAOptimizer( population_size=20, generations=10, mutation_rate=0.2, crossover_rate=0.7, objectives=[\"accuracy\", \"clarity\", \"completeness\"], reflection_model=\"gpt-4\" ) compiled = optimizer.compile( program=program, trainset=training_data, valset=validation_data ) Next: State-Space Optimization"}, {"title": "The Compilation Concept", "url": "chapters/chapter-05/compilation-concept.html", "content": "Chapter 5 ¬∑ Section 2 The Compilation Concept Transform high-level program specifications into optimized prompts and weights. ~10 min read üîß What is DSPy Compilation? DSPy compilation transforms your high-level program into optimized prompts and weights. Unlike traditional compilation that converts source code to machine code, DSPy compilation optimizes the language model interactions within your program. The compilation process includes: ‚úèÔ∏è Automatic Prompt Engineering Crafting optimal prompts for your specific task üìã Example Selection Choosing the best demonstrations for few-shot learning ‚öôÔ∏è Weight Tuning Optimizing module parameters for better performance üîó Pipeline Optimization Improving the overall program structure üîÑ The Compilation Pipeline import dspy from dspy.teleprompt import BootstrapFewShot # Before compilation: High-level specification class QASystem(dspy.Module): def __init__(self): super().__init__() self.generate_answer = dspy.ChainOfThought(\"question -> answer\") def forward(self, question): return self.generate_answer(question=question) # Define metric def answer_exact_match(example, pred, trace=None): return example.answer.lower() == pred.answer.lower() # After compilation: Optimized prompts and weights optimized_qa = BootstrapFewShot(metric=answer_exact_match).compile( QASystem(), trainset=train_data ) ‚öôÔ∏è How Compilation Works 1Ô∏è‚É£ Program Specification You define the high-level structure using DSPy modules 2Ô∏è‚É£ Training Data Provide examples of inputs and desired outputs 3Ô∏è‚É£ Optimization Metric Define how to measure performance (accuracy, F1, etc.) 4Ô∏è‚É£ Compilation DSPy automatically optimizes using the specified optimizer 5Ô∏è‚É£ Evaluation Test the compiled program on held-out data üì¶ Types of Compilation Prompt Compilation Optimizes the natural language instructions: Rewrites instructions for clarity Adds relevant context Formats examples optimally Example Compilation Selects and orders training examples: Chooses diverse examples Orders by difficulty or relevance Balances different types of cases Weight Compilation Optimizes module parameters: Adjusts confidence thresholds Tunes generation parameters Optimizes module interactions üìä Compilation vs Traditional Programming Traditional Programming DSPy Compilation Source code ‚Üí Machine code High-level LM program ‚Üí Optimized prompts Static optimization Dynamic optimization based on data One-time compilation Iterative improvement possible Hardware-specific Task and data-specific Manual optimization required Automatic optimization üéØ When to Use Compilation ‚úÖ Use compilation when: You have training data available Performance is critical Task is complex or nuanced You want consistent results Manual prompt engineering is time-consuming ‚ö†Ô∏è Skip compilation when: Task is very simple No training data available One-off tasks Rapid prototyping needed üí° Compilation Best Practices Start Simple # Start with this simple_classifier = dspy.Predict(\"text -> category\") # Then compile for better performance optimized = BootstrapFewShot(metric=accuracy).compile( simple_classifier, trainset=data ) Use Sufficient Training Data # Minimum 10-20 examples for basic tasks # 50-100+ examples for complex tasks # Diversity in examples is crucial Choose the Right Metric # For classification: accuracy, F1 # For generation: ROUGE, BLEU # For QA: exact match, F1 # Custom metrics for domain-specific tasks Validate Properly # Split data properly train_data, val_data = train_test_split(all_data, test_size=0.2) # Compile on training data compiled_program = optimizer.compile(program, trainset=train_data) # Evaluate on validation data results = evaluate(compiled_program, val_data) üìù Key Takeaways DSPy compilation automatically optimizes LM interactions Transforms high-level programs into optimized prompts and parameters Process is data-driven and reproducible Different types: prompts, examples, and weights Proper validation is essential for success Next: BootstrapFewShot"}, {"title": "Monte Carlo Optimization", "url": "chapters/chapter-05/monte-carlo-optimization.html", "content": "Chapter 5 Monte Carlo Optimization Harness stochastic optimization to explore complex prompt and parameter spaces efficiently. Introduction Monte Carlo methods provide powerful stochastic optimization techniques that excel in complex, non-convex optimization spaces typical of language model systems. Unlike gradient-based methods, Monte Carlo techniques work with any black-box evaluation function, making them particularly suitable for diverse prompt optimization tasks. Fundamentals Monte Carlo optimization relies on four key steps: Random Exploration: Sampling points from the search space. Evaluation: Assessing the quality of each sample. Adaptive Sampling: Focusing exploration on promising regions. Convergence: Gradually narrowing down to optimal solutions. Optimization Strategies 1. Random Search The simplest approach, sampling random solutions until a good one is found. While basic, it effectively establishes a baseline. class RandomSearchMonteCarlo(MonteCarloOptimizer): def optimize(self): for iteration in range(self.max_iterations): solution = self._sample_solution() score = self.evaluation_fn(solution) if score > self.best_score: self.best_score = score self.best_solution = solution 2. Simulated Annealing A more sophisticated method that allows \"uphill\" moves (accepting worse solutions) early on to escape local optima, gradually \"cooling\" to focus on the best region. class SimulatedAnnealingMonteCarlo(MonteCarloOptimizer): def optimize(self): current_solution = self._sample_solution() for iteration in range(self.max_iterations): neighbor = self._generate_neighbor(current_solution) delta = self.evaluation_fn(neighbor) - self.evaluation_fn(current_solution) # Acceptance probability calculation if delta > 0 or random.random() < np.exp(delta / self.temperature): current_solution = neighbor self.temperature *= self.cooling_rate # Cool down Advanced Methods Cross-Entropy Method An evolutionary-like approach where a population is sampled, the top performers (\"elite\") are selected, and the sampling distribution is updated to match the elite. Particle Swarm Optimization Simulates a swarm of particles moving through the search space. Each particle adjusts its trajectory based on its own best known position and the swarm's best known position. Monte Carlo for Prompt Optimization Monte Carlo methods are excellent for finding the best combination of instruction templates, few-shot examples, and formatting styles. def define_prompt_search_space(): return { \"instruction\": { \"type\": \"string_template\", \"templates\": [\"Answer concisely.\", \"Be detailed.\", \"Think step-by-step.\"] }, \"temperature\": { \"type\": \"continuous\", \"min\": 0.0, \"max\": 1.0 }, \"max_examples\": { \"type\": \"integer\", \"min\": 0, \"max\": 5 } } Next: Bayesian Optimization"}, {"title": "Chapter 5: Optimizers", "url": "chapters/chapter-05/index.html", "content": "Chapter 5 Optimizers Learn how DSPy optimizers systematically improve your prompts and modules. üöÄ Chapter Overview Optimizers are the heart of DSPy. They take your declarative modules and systematically improve them by tuning prompts and updating weights. In this chapter, we'll explore the full range of DSPy optimizers, from basic few-shot selectors to advanced evolutionary algorithms and fine-tuning strategies. You'll learn how to treat prompt engineering as a programmatic optimization problem, allowing you to achieve higher performance with less manual effort. üìö What You'll Learn üîß Core Optimizers Master BootstrapFewShot, COPRO, and MiPRO for automatic prompt improvement. üß† Model Tuning Learn how to fine-tune small language models to perform like larger ones. üî¨ Advanced Techniques Explore cutting-edge methods like Bayesian Optimization, Monte Carlo methods, and Genetic Algorithms. üìê Theoretical Foundations Understand the theory behind multi-stage optimization and complex pipeline architectures. üìã Chapter Structure This chapter is organized into several key areas: Getting Started: Compilation Concept, BootstrapFewShot Instruction Optimization: COPRO, MiPRO, Automatic Prompt Optimization Example Selection: KnnFewShot, Demonstration Optimization Model Optimization: Fine-tuning, Joint Optimization Advanced Methods: Monte Carlo, Bayesian, Genetic Algorithms (GEPA), Reflective Prompt Evolution Architectures & Frameworks: CoPA, Multistage Theory, Instruction Tuning Frameworks Next: Compilation Concept"}, {"title": "Comprehensive Examples", "url": "chapters/chapter-05/comprehensive-examples.html", "content": "Chapter 5 Comprehensive Examples Apply advanced optimization techniques to real-world scenarios, from Enterprise RAG to Multi-Language Code Generation. Introduction This section brings together all the optimization techniques we've explored‚ÄîCOPA, joint optimization, Monte Carlo methods, and Bayesian optimization‚Äîthrough comprehensive, real-world examples. Example 1: Enterprise RAG System Optimization We optimize a complex RAG system that needs to answer domain-specific questions accurately while maintaining consistency with company guidelines. Implementation class EnterpriseRAGSystem(dspy.Module): def forward(self, question, domain=None): # 1. Process Query enhanced_query = self.query_processor(question, domain) # 2. Retrieve & Rerank docs = self.retriever(enhanced_query).passages ranked_docs = self.reranker(enhanced_query, docs) # 3. Generate Answer answer = self.generator( question=question, context=ranked_docs, instruction=self._build_instruction() ) return answer Multi-Objective Optimization We use Bayesian Optimization to balance multiple objectives like accuracy, latency, and cost. optimizer = MultiObjectiveBayesianOptimizer( objectives=[\"accuracy\", \"latency\", \"cost\"], preference_weights={\"accuracy\": 0.5, \"latency\": 0.3, \"cost\": 0.2} ) Example 2: Multi-Language Code Generation Optimizing a system that generates code in Python, JavaScript, Java, and C++, using Joint Optimization to transfer knowledge between languages. class JointCodeOptimizer: def optimize(self): # Phase 1: Fine-tune individual language models for lang in languages: self._fine_tune_language_model(lang) # Phase 2: Joint prompt optimization self._optimize_prompts_jointly(shared_knowledge) return optimized_system Next: Multistage Optimization Theory"}, {"title": "Exercises", "url": "chapters/chapter-05/exercises.html", "content": "Chapter 5 ¬∑ Section 7 Exercises Practice DSPy optimization with hands-on exercises. 5 Exercises Exercise 1 ‚≠ê Beginner Basic BootstrapFewShot Optimization Objective: Learn to use BootstrapFewShot to improve a simple QA system. Problem You have a basic question-answering system that needs improvement. Use BootstrapFewShot to optimize it with provided training data. import dspy from dspy.teleprompt import BootstrapFewShot class BasicQA(dspy.Module): def __init__(self): super().__init__() self.generate_answer = dspy.Predict(\"question -> answer\") def forward(self, question): return self.generate_answer(question=question) # Training data trainset = [ dspy.Example(question=\"What is 2+2?\", answer=\"4\").with_inputs(\"question\"), dspy.Example(question=\"Capital of France?\", answer=\"Paris\").with_inputs(\"question\"), dspy.Example(question=\"Who wrote Romeo and Juliet?\", answer=\"Shakespeare\").with_inputs(\"question\"), dspy.Example(question=\"What is H2O?\", answer=\"Water\").with_inputs(\"question\"), dspy.Example(question=\"How many continents?\", answer=\"7\").with_inputs(\"question\"), ] # Test data testset = [ dspy.Example(question=\"What is 3+3?\", answer=\"6\").with_inputs(\"question\"), dspy.Example(question=\"Capital of Spain?\", answer=\"Madrid\").with_inputs(\"question\"), ] # TODO: Implement this function def bootstrap_optimize(program, trainset, max_demos=4): \"\"\"Optimize the program using BootstrapFewShot.\"\"\" pass Tasks 1. Define an exact match metric 2. Create a BootstrapFewShot optimizer 3. Compile the program with training data 4. Evaluate on test data and compare with baseline Exercise 2 ‚≠ê‚≠ê Intermediate KNNFewShot for Context-Aware Selection Objective: Implement KNNFewShot to select relevant examples dynamically based on query similarity. Problem Build a context-aware classifier that selects different examples based on the input text's topic. import dspy from dspy.teleprompt import KNNFewShot class TopicClassifier(dspy.Module): def __init__(self): super().__init__() self.classify = dspy.Predict(\"text -> topic\") def forward(self, text): return self.classify(text=text) # Diverse training data trainset = [ dspy.Example( text=\"The company's stock increased after earnings\", topic=\"finance\" ).with_inputs(\"text\"), dspy.Example( text=\"New study reveals vaccine effectiveness\", topic=\"healthcare\" ).with_inputs(\"text\"), dspy.Example( text=\"Court ruled in favor of the plaintiff\", topic=\"legal\" ).with_inputs(\"text\"), dspy.Example( text=\"Quarterback threw a touchdown pass\", topic=\"sports\" ).with_inputs(\"text\"), ] # TODO: Implement these functions def create_knn_optimizer(k=3): \"\"\"Create KNNFewShot optimizer.\"\"\" pass def evaluate_classifier(classifier, testset): \"\"\"Evaluate classifier accuracy.\"\"\" pass Tasks 1. Create KNNFewShot optimizer with k=3 2. Compile the classifier 3. Test with domain-specific queries 4. Observe how different examples are selected Exercise 3 ‚≠ê‚≠ê Intermediate MIPRO for Complex Reasoning Objective: Use MIPRO to optimize a Chain of Thought program for mathematical reasoning. Problem Improve a mathematical problem solver that requires step-by-step reasoning. import dspy from dspy.teleprompt import MIPRO class MathSolver(dspy.Module): def __init__(self): super().__init__() self.solve = dspy.ChainOfThought(\"problem -> answer\") def forward(self, problem): result = self.solve(problem=problem) return dspy.Prediction( steps=result.rationale, answer=result.answer ) # Math problems trainset = [ dspy.Example( problem=\"A rope is 12m long. Cut into 3 equal pieces. Length of each?\", answer=\"4 meters\" ).with_inputs(\"problem\"), dspy.Example( problem=\"Train travels 60km in 1 hour. How far in 3 hours?\", answer=\"180 km\" ).with_inputs(\"problem\"), dspy.Example( problem=\"Box has 8 rows of 5 apples each. Total apples?\", answer=\"40 apples\" ).with_inputs(\"problem\"), ] # TODO: Implement these functions def create_math_metric(): \"\"\"Create metric for math problems.\"\"\" pass def mipro_optimize(program, trainset, num_candidates=10): \"\"\"Optimize using MIPRO.\"\"\" pass Tasks 1. Create a comprehensive metric for math problems 2. Configure MIPRO with appropriate parameters 3. Optimize the math solver 4. Analyze the improved reasoning Exercise 4 ‚≠ê‚≠ê‚≠ê Advanced Optimizer Comparison Benchmark Objective: Compare different optimizers on the same task to understand trade-offs. Problem Build a sentiment analyzer and optimize with BootstrapFewShot, KNNFewShot, and MIPRO. Compare results. import dspy import time from dspy.teleprompt import BootstrapFewShot, KNNFewShot, MIPRO class SentimentAnalyzer(dspy.Module): def __init__(self): super().__init__() self.analyze = dspy.Predict(\"text -> sentiment, confidence\") def forward(self, text): return self.analyze(text=text) trainset = [ dspy.Example(text=\"I love this!\", sentiment=\"positive\", confidence=\"high\").with_inputs(\"text\"), dspy.Example(text=\"Terrible quality.\", sentiment=\"negative\", confidence=\"high\").with_inputs(\"text\"), dspy.Example(text=\"Works as expected.\", sentiment=\"neutral\", confidence=\"medium\").with_inputs(\"text\"), #"}, {"title": "Instruction Tuning Frameworks", "url": "chapters/chapter-05/instruction-tuning-frameworks.html", "content": "Chapter 5 Instruction Tuning Frameworks Master instruction tuning principles and methodologies, from template design to automatic optimization in DSPy. Introduction Instruction tuning improves language model performance by training models to follow natural language instructions. In DSPy, this extends to automatically discovering and refining the instructions that guide each module in a multi-stage program. Foundations Instruction tuning emphasizes learning from task descriptions (what to do) rather than just input-output pairs. This enables: Generalization: Handling new tasks described in natural language. Zero-shot Capability: Performing tasks without needing examples. Better Instruction Following: Adhering to complex constraints. Methodologies 1. Supervised Instruction Fine-tuning Training on datasets formatted as instructions. The model learns to predict the output given an instruction and input. 2. RLHF (Reinforcement Learning from Human Feedback) Using human preferences to fine-tune the model, rewarding responses that better follow instructions or align with human intent. 3. Dynamic Template Generation Using an LLM to generate and refine instruction templates based on task descriptions. class InstructionTemplateGenerator: def generate_template(self, task): prompt = f\"Generate an effective instruction template for: {task}\" return self.llm.generate(prompt) Automatic Instruction Optimization We can automate the search for optimal instructions using various algorithms. Evolutionary Optimization Evolving a population of instructions by mutating and combining them, selecting the best performers based on a validation set. class EvolutionaryInstructionOptimizer: def optimize(self, task, examples): population = self._initialize_population(task) for generation in range(generations): fitness = [self._evaluate(inst) for inst in population] population = self._evolve(population, fitness) return best_instruction DSPy Integration DSPy provides tools to tune instructions for specific modules within a pipeline. class DSPyInstructionTuner: def tune_module_instruction(self, module_class, signature, trainset): candidates = self._generate_candidates(module_class, signature) best_instruction, score = self._evaluate_candidates(candidates, trainset) return best_instruction Best Practices Clarity: Be explicit about what needs to be done. Format Specification: Clearly define the expected output format (e.g., JSON, List). Iterative Refinement: Start simple and add constraints/details based on errors. Next: Demonstration Optimization"}, {"title": "COPRO (Chain-of-Thought Prompt Optimization)", "url": "chapters/chapter-05/copro.html", "content": "Chapter 5 COPRO: Chain-of-Thought Prompt Optimization Discover the best Instructions for your DSPy programs using evolutionary search and cost-aware optimization. üìã Prerequisites Previous Section: BootstrapFewShot (Few-shot optimization) Chapter 4: Evaluation (Metrics and validation) Concept: Evolutionary algorithms (helpful but not required) Knowledge: Understanding of prompt engineering basics Introduction to COPRO COPRO (Chain-of-thought PROmpt optimization) is an advanced DSPy optimizer that uses evolutionary search to discover and refine optimal instructions for your language model programs. Unlike BootstrapFewShot, which focuses on selecting good demonstrations, COPRO specifically targets instruction optimization‚Äîfinding the best way to describe your task to the language model. üí° Core Innovation: Prompts that work well for humans may not be optimal for LMs. COPRO solves this by letting the LM generate, evaluate, and evolve its own instructions. How It Works Generate Candidates: Uses an LM to propose diverse instruction variations. Evaluate: Tests each candidate against your metric on a training set. Evolve: Selects top performers and mutates them to create better versions. Converge: Repeats this process to find the optimal prompt. Cost-Aware Optimization COPRO is designed to be efficient: Adaptive Evaluation: Spends more compute on promising candidates. Early Termination: Stops bad search paths quickly. Budget Management: Respects your constraints on total optimization cost. üíª Basic Usage Simple Classification Example Here is how to use COPRO to optimize a sentiment classifier: import dspy from dspy.teleprompt import COPRO # 1. Define your signature and module class SentimentClassifier(dspy.Signature): \"\"\"Classify text sentiment.\"\"\" text: str = dspy.InputField() sentiment: str = dspy.OutputField(desc=\"positive, negative, or neutral\") classifier = dspy.Predict(SentimentClassifier) # 2. Define training data (20-50 examples recommended) trainset = [ dspy.Example(text=\"I love this!\", sentiment=\"positive\"), dspy.Example(text=\"Terrible service.\", sentiment=\"negative\"), # ... add more examples ] # 3. Define metric def sentiment_accuracy(example, pred, trace=None): return example.sentiment.lower() == pred.sentiment.lower() # 4. Configure COPRO copro = COPRO( metric=sentiment_accuracy, breadth=10, # Candidates per generation depth=3 # Number of generations ) # 5. Compile optimized_classifier = copro.compile(classifier, trainset=trainset) # 6. Usage result = optimized_classifier(text=\"This exceeded all expectations!\") print(result.sentiment) ‚öôÔ∏è Advanced Configuration Fine-tune COPRO's behavior for your specific needs. Parameter Description Default Recommended for Reasoning breadth Candidates per generation 10 15-20 depth Number of generations 3 3-5 init_temperature Creativity for initial candidates 1.4 1.5 prompt_model LM used to generate prompts None (uses task model) GPT-4 / Stronger Model Using a Stronger Teacher Model A common strategy is to use a powerful model (like GPT-4) to generate prompt candidates, but optimize them for a smaller, faster model (like GPT-3.5 or a local model). # Use GPT-4 to propose instructions prompt_generator = dspy.LM(model=\"openai/gpt-4\") # Optimize for GPT-3.5 target_model = dspy.LM(model=\"openai/gpt-3.5-turbo\") dspy.configure(lm=target_model) copro = COPRO( metric=your_metric, breadth=12, depth=4, prompt_model=prompt_generator # Teacher model ) optimized_program = copro.compile(program, trainset=trainset) üÜö COPRO vs. Other Optimizers Feature COPRO BootstrapFewShot MiPRO Primary Target Instructions Demonstrations Both Method Evolutionary Search Bootstrap Sampling Bayesian Optimization Best For Instruction-sensitive tasks, Reasoning Tasks with good examples Complex pipelines, Max performance Speed Medium Fast Slow Recommendation: Use COPRO when you have a tricky reasoning task where the exact wording of the prompt matters significantly, or when you have limited data for demonstrations. üåç Real-World Applications 1. Medical Triage In high-stakes domains like healthcare, instruction precision is critical. COPRO can evolve prompts that strictly adhere to safety protocols. # Metric penalizes dangerous errors heavily def triage_metric(example, pred, trace=None): correct = pred.level == example.level # Critical penalty for under-triaging emergencies if example.level == \"emergency\" and pred.level != \"emergency\": return 0.0 return 1.0 if correct else 0.5 copro = COPRO( metric=triage_metric, breadth=15, depth=5, init_temperature=1.2 # Lower temp for stability ) 2. Legal Analysis For tasks requiring specific vocabulary and structure, COPRO can find the \"magic words\" that align the model with legal standards. ‚ú® Best Practices Diverse Training Data: Ensure your 20-50 training examples cover various edge cases. If they are all simple, COPRO won't learn to handle complexity. Meaningful Metrics: Your metric determines the direction of evolution. A binary (0/1) metric provides less signal than a "}, {"title": "Solutions", "url": "chapters/chapter-05/solutions.html", "content": "Chapter 5 ¬∑ Section 8 Solutions Complete solutions with detailed explanations. 5 Solutions Solution 1 ‚≠ê Beginner Basic BootstrapFewShot Optimization import dspy from dspy.teleprompt import BootstrapFewShot import os from dotenv import load_dotenv # Load environment variables load_dotenv() # Configure DSPy lm = dspy.LM(\"openai/gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\")) dspy.configure(lm=lm) class BasicQA(dspy.Module): def __init__(self): super().__init__() self.generate_answer = dspy.Predict(\"question -> answer\") def forward(self, question): return self.generate_answer(question=question) # Define exact match metric def exact_match(example, pred, trace=None): \"\"\"Check if prediction matches expected answer.\"\"\" return example.answer.lower().strip() == pred.answer.lower().strip() # Training data trainset = [ dspy.Example(question=\"What is 2+2?\", answer=\"4\").with_inputs(\"question\"), dspy.Example(question=\"Capital of France?\", answer=\"Paris\").with_inputs(\"question\"), dspy.Example(question=\"Who wrote Romeo and Juliet?\", answer=\"Shakespeare\").with_inputs(\"question\"), dspy.Example(question=\"What is H2O?\", answer=\"Water\").with_inputs(\"question\"), dspy.Example(question=\"How many continents?\", answer=\"7\").with_inputs(\"question\"), ] # Test data testset = [ dspy.Example(question=\"What is 3+3?\", answer=\"6\").with_inputs(\"question\"), dspy.Example(question=\"Capital of Spain?\", answer=\"Madrid\").with_inputs(\"question\"), ] def bootstrap_optimize(program, trainset, max_demos=4): \"\"\"Optimize the program using BootstrapFewShot.\"\"\" optimizer = BootstrapFewShot( metric=exact_match, max_bootstrapped_demos=max_demos, max_labeled_demos=2 ) return optimizer.compile(program, trainset=trainset) def evaluate(program, testset): \"\"\"Evaluate program on test set.\"\"\" correct = 0 for example in testset: try: pred = program(question=example.question) if exact_match(example, pred): correct += 1 except Exception as e: print(f\"Error: {e}\") return correct / len(testset) if testset else 0 # Run optimization baseline = BasicQA() optimized = bootstrap_optimize(BasicQA(), trainset, max_demos=4) # Evaluate baseline_score = evaluate(baseline, testset) optimized_score = evaluate(optimized, testset) print(f\"Baseline accuracy: {baseline_score:.2%}\") print(f\"Optimized accuracy: {optimized_score:.2%}\") print(f\"Improvement: {optimized_score - baseline_score:+.2%}\") üí° Key Concepts The metric function receives example (ground truth) and pred (prediction). Normalize strings with .lower().strip() for robust matching. Solution 2 ‚≠ê‚≠ê Intermediate KNNFewShot for Context-Aware Selection import dspy from dspy.teleprompt import KNNFewShot import os from dotenv import load_dotenv load_dotenv() lm = dspy.LM(\"openai/gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\")) dspy.configure(lm=lm) class TopicClassifier(dspy.Module): def __init__(self): super().__init__() self.classify = dspy.Predict(\"text -> topic\") def forward(self, text): return self.classify(text=text) # Training data with diverse topics trainset = [ dspy.Example(text=\"Stock price increased after earnings\", topic=\"finance\").with_inputs(\"text\"), dspy.Example(text=\"New study reveals vaccine effectiveness\", topic=\"healthcare\").with_inputs(\"text\"), dspy.Example(text=\"Court ruled in favor of plaintiff\", topic=\"legal\").with_inputs(\"text\"), dspy.Example(text=\"Quarterback threw touchdown pass\", topic=\"sports\").with_inputs(\"text\"), dspy.Example(text=\"New iPhone features improved camera\", topic=\"technology\").with_inputs(\"text\"), dspy.Example(text=\"Merger approved by shareholders\", topic=\"finance\").with_inputs(\"text\"), dspy.Example(text=\"Patient recovered after surgery\", topic=\"healthcare\").with_inputs(\"text\"), dspy.Example(text=\"Jury reached guilty verdict\", topic=\"legal\").with_inputs(\"text\"), ] # Test data testset = [ dspy.Example(text=\"Bitcoin price surged today\", topic=\"finance\").with_inputs(\"text\"), dspy.Example(text=\"The team won the championship\", topic=\"sports\").with_inputs(\"text\"), ] def topic_match(example, pred, trace=None): \"\"\"Check if predicted topic matches expected.\"\"\" return example.topic.lower().strip() == pred.topic.lower().strip() def create_knn_optimizer(k=3): \"\"\"Create KNNFewShot optimizer.\"\"\" return KNNFewShot( k=k, trainset=trainset ) def evaluate_classifier(classifier, testset): \"\"\"Evaluate classifier accuracy.\"\"\" correct = 0 for example in testset: try: pred = classifier(text=example.text) if topic_match(example, pred): correct += 1 print(f\"‚úì '{example.text[:40]}...' -> {pred.topic}\") else: print(f\"‚úó '{example.text[:40]}...' -> {pred.topic} (expected: {example.topic})\") except Exception as e: print(f\"Error: {e}\") return correct / len(testset) if testset else 0 # Create and compile optimizer = create_knn_optimizer(k=3) compiled = optimizer.compile(TopicClassifier(), trainset=trainset) # Evaluate print(\"=== KNNFewShot Results ===\") accuracy = evaluate_classifier(compiled, testset) print(f\"\\nAccuracy: {accuracy:.2%}\") üí° Key Concepts KNNFewShot selects the k most similar examples for each query. For"}, {"title": "BootstrapFewShot", "url": "chapters/chapter-05/bootstrap-fewshot.html", "content": "Chapter 5 ¬∑ Section 3 BootstrapFewShot Automatic few-shot example generation‚Äîno manual annotation required. ~15 min read üéØ Introduction BootstrapFewShot is one of DSPy's most powerful optimizers. It automatically generates and selects high-quality few-shot examples to improve your program's performance. Instead of manually crafting examples, BootstrapFewShot discovers the optimal demonstrations for your specific task. üí° Key innovation: Weak supervision‚Äîtrain models without hand-labeled intermediate steps! üî¨ Weak Supervision and the annotate() Method A key innovation from the Demonstrate-Search-Predict paper is weak supervision. In simple terms, this is like a teacher who doesn't check every line of your scratchpad math, but only grades the final answer. If you get the final answer right using your own method, the teacher says \"Good job, do it that way again.\" In DSPy, \"weak supervision\" allows you to train models without hand-labeled intermediate reasoning steps: # Traditional approach requires manually annotated reasoning traditional_training = [ dspy.Example( question=\"What is 15 * 23?\", reasoning=\"Step 1: 15 * 20 = 300\\nStep 2: 15 * 3 = 45\\nStep 3: 300 + 45 = 345\", answer=\"345\" ), # ... many more with detailed reasoning ] # With weak supervision, you only need: weak_supervision_training = [ dspy.Example(question=\"What is 15 * 23?\", answer=\"345\"), dspy.Example(question=\"What is 12 * 17?\", answer=\"204\"), # ... just input-output pairs! ] # BootstrapFewShot will automatically generate the reasoning! ‚öôÔ∏è How BootstrapFewShot Works 1Ô∏è‚É£ Initial Generation Uses the unoptimized program to generate candidate examples 2Ô∏è‚É£ Quality Filtering Evaluates generated examples using your metric 3Ô∏è‚É£ Example Selection Chooses the best examples based on performance 4Ô∏è‚É£ Iterative Refinement Repeats the process to improve example quality üì¶ Basic Usage import dspy from dspy.teleprompt import BootstrapFewShot # 1. Define your program class SimpleQA(dspy.Module): def __init__(self): super().__init__() self.generate = dspy.Predict(\"question -> answer\") def forward(self, question): return self.generate(question=question) # 2. Define evaluation metric def exact_match(example, pred, trace=None): return example.answer.lower() == pred.answer.lower() # 3. Prepare training data trainset = [ dspy.Example(question=\"What is 2+2?\", answer=\"4\").with_inputs(\"question\"), dspy.Example(question=\"What is the capital of France?\", answer=\"Paris\").with_inputs(\"question\"), dspy.Example(question=\"Who wrote Romeo and Juliet?\", answer=\"William Shakespeare\").with_inputs(\"question\"), ] # 4. Create optimizer and compile optimizer = BootstrapFewShot(metric=exact_match, max_bootstrapped_demos=4) compiled_qa = optimizer.compile(SimpleQA(), trainset=trainset) # 5. Use the compiled program result = compiled_qa(question=\"What is 3+3?\") print(result.answer) # Should be \"6\" üéõÔ∏è BootstrapFewShot Parameters Parameter Type Default Description metric Callable Required Function to evaluate example quality max_bootstrapped_demos int 8 Maximum generated examples max_labeled_demos int 4 Maximum human-labeled examples max_rounds int 2 Number of bootstrap iterations üß† Using with Chain of Thought class CoTQA(dspy.Module): def __init__(self): super().__init__() self.generate = dspy.ChainOfThought(\"question -> answer\") def forward(self, question): result = self.generate(question=question) return dspy.Prediction( answer=result.answer, reasoning=result.rationale # Automatically generated! ) # Bootstrap with Chain of Thought optimizer = BootstrapFewShot( metric=exact_match, max_bootstrapped_demos=8, teacher_settings=dict(lm=dspy.settings.lm) ) compiled_cot = optimizer.compile(CoTQA(), trainset=trainset) ‚ú® The magic: The reasoning steps are automatically generated during bootstrapping‚Äîyou only provide input-output pairs! üìä Defining Metrics Exact Match def exact_match_metric(example, pred, trace=None): \"\"\"Simple exact string match.\"\"\" return str(example.answer).lower() == str(pred.answer).lower() Fuzzy Match def fuzzy_match(example, pred, trace=None): \"\"\"Fuzzy matching with some tolerance.\"\"\" from difflib import SequenceMatcher similarity = SequenceMatcher(None, example.answer, pred.answer).ratio() return similarity > 0.9 F1 Score for QA def qa_f1_metric(example, pred, trace=None): \"\"\"F1 score for QA tasks.\"\"\" from collections import Counter pred_tokens = Counter(str(pred.answer).lower().split()) true_tokens = Counter(str(example.answer).lower().split()) common = pred_tokens & true_tokens precision = len(common) / len(pred_tokens) if pred_tokens else 0 recall = len(common) / len(true_tokens) if true_tokens else 0 if precision + recall == 0: return 0 return 2 * precision * recall / (precision + recall) ‚ú® Benefits of Weak Supervision üí∞ Reduced Annotation Cost No need to write detailed reasoning chains‚Äîonly final answers ‚úÖ Consistent Quality Generated reasoning follows consistent patterns üöÄ Rapid Prototyping Test new tasks with minimal data preparation üéØ Better Coverage Generates diverse r"}, {"title": "MIPRO Optimizer", "url": "chapters/chapter-05/mipro.html", "content": "Chapter 5 ¬∑ Section 4 MIPRO Optimizer Multi-step Instruction and Demonstration Optimization for maximum performance. ~20 min read ‚ö° Introduction MIPRO (Multi-step Instruction and demonstration PRompt Optimization) represents a significant advancement in automated prompt optimization. Unlike simpler approaches that only optimize examples, MIPRO simultaneously optimizes both instructions (prompts) and demonstrations (examples) for each module in a multi-stage pipeline. üìä Research results: MIPRO achieves 63% improvement on HotpotQA (32.0 ‚Üí 52.3 F1) compared to manual prompting! üìà Performance Benchmarks Dataset Task Type Manual MIPRO Improvement HotpotQA Multi-hop QA 32.0 F1 52.3 F1 +63% GSM8K Math Reasoning 28.5% 33.8% +19% CodeAlpaca Code Generation 63.1% 64.8% +3% FEVER Fact Verification 71.2% 78.9% +11% üèóÔ∏è Dual-Component Optimization MIPRO's power comes from jointly optimizing two key elements: üìù Instruction Generation Generates candidate instructions using meta-prompting conditioned on program structure, module signatures, and dataset characteristics üìã Demonstration Selection Selects demonstrations using data-driven selection, utility scoring, and greedy algorithms ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ MIPRO Optimization Loop ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ Meta-Prompt ‚îÇ ‚îÇ Demonstration ‚îÇ ‚îÇ ‚îÇ ‚îÇ Generation ‚îÇ ‚îÇ Selection ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚ñº ‚ñº ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ Candidate Configurations ‚îÇ ‚îÇ ‚îÇ ‚îÇ (Instruction + Demonstration Pairs) ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚ñº ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ Simulated Annealing Search ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚ñº ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ Best Configuration ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò üì¶ Basic Usage import dspy from dspy.teleprompt import MIPRO # Define your program class ReasoningQA(dspy.Module): def __init__(self): super().__init__() self.generate = dspy.ChainOfThought(\"question -> answer\") def forward(self, question): return self.generate(question=question) # Define metric def answer_accuracy(example, pred, trace=None): return example.answer.lower() in pred.answer.lower() # Create MIPRO optimizer optimizer = MIPRO( metric=answer_accuracy, num_candidates=10, # Instruction candidates per module init_temperature=0.7, # Diversity in generation verbose=True ) # Compile with MIPRO compiled = optimizer.compile( ReasoningQA(), trainset=trainset, num_trials=3, # Optimization iterations max_bootstrapped_demos=8 # Max demos per module ) üéõÔ∏è Auto Configuration Modes MIPRO provides convenient auto-configuration presets: # Light mode: Quick optimization for simple tasks # Equivalent to: num_candidates=5, init_temperature=0.8 optimizer = MIPRO(metric=accuracy, auto=\"light\") # Medium mode: Balanced optimization (recommended default) # Equivalent to: num_candidates=10, init_temperature=1.0 optimizer = MIPRO(metric=accuracy, auto=\"medium\") # Heavy mode: Extensive optimization for complex tasks # Equivalent to: num_candidates=20, init_temperature=1.2 optimizer = MIPRO(metric=accuracy, auto=\"heavy\") Parameter Default Range Description num_candidates 10 5-30 Instruction candidates per module init_temperature 1.0 0.5-1.5 Meta-prompt sampling temperature num_trials 3 1-10 Optimization iterations max_bootstrapped_demos 8 0-16 Max demos per module üîó Multi-Stage Pipeline Optimization MIPRO excels at optimizing multi-stage pipelines: class MultiHopQA(dspy.Module): \"\"\"Multi-hop QA pipeline.\"\"\" def __init__(self): super().__init__() # Stage 1: Query generation self.generate_queries = dspy.Predict(\"question -> search_queries\") # Stage 2: Retrieval self.retrieve = dspy.Retrieve(k=5) # Stage 3: Answer generation self.generate_answer = dspy.ChainOfThought(\"question, context -> answer\") def forward(self, question): # Stage 1 queries = self.generate_queries(question=question) # Stage 2 passages = self.retrieve(query=queries.search_queries).passages context = \"\\n\".join(passages[:5]) # Stage 3 answer = self.generate_answer(question=question, context=context) return dspy.Prediction( answer=answer.answer, reasoning=answer.rationale ) # MIPRO optimizes EACH module in the pipeline optimizer = MIPRO( metric=qa_metric, num_candidates=15, # More candidates for multi-stage auto=\"medium\" ) optimized = optimizer.compile( MultiHopQA(), trainset=trainset, num_trials=5, max_bootstrapped_demos=6 # Per module ) üéØ Zero-Shot vs Few-Shot MIPRO research reveals that optimized zero-shot can often beat manual few-shot: # Zero-shot optimization (instructions only) mipro_zeroshot = MIPRO(metric=accuracy, num_candidates=20) zeroshot_compiled = mipro_zeroshot.compile( program, trainset=trainset, max_bootstrapped_demos=0 # No demonstrations! ) # Few-sh"}, {"title": "Multistage Architectures", "url": "chapters/chapter-05/multistage-architectures.html", "content": "Chapter 5 Multistage Architectures Explore architectural patterns for multi-stage DSPy programs, including sequential pipelines, branching logic, and hierarchical structures. Introduction Multi-stage architectures allow us to break down complex tasks into manageable steps. This improves modularity, debuggability, and performance. Architectural Patterns 1. Sequential Pipeline The most common pattern, where data flows linearly through a series of stages. Input ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí Stage 3 ‚Üí Output class SequentialPipeline(dspy.Module): def forward(self, x): for stage in self.stages: x = stage(x) return x 2. Branching Architecture Uses a router to direct the flow of execution based on the input or intermediate results. ‚îå‚îÄ‚Üí Stage A Input ‚Üí Router ‚îÄ‚î§ ‚îî‚îÄ‚Üí Stage B class BranchingPipeline(dspy.Module): def forward(self, x): branch = self.router(x) if branch == 'A': return self.stage_a(x) else: return self.stage_b(x) 3. Iterative Architecture Repeats a process until a stopping condition is met (e.g., quality threshold or max iterations). 4. Hierarchical Architecture Composes pipelines within pipelines to handle very complex tasks. Design Principles Clear Interfaces: Define standard input/output schemas for stages. Error Handling: Implement robust error detection and recovery strategies. Observability: Log intermediate outputs for debugging and optimization. Next: Complex Pipeline Optimization"}, {"title": "How to Use This Book", "url": "chapters/chapter-00/how-to-use.html", "content": "Introduction How to Use This Book Choose your path to DSPy mastery and learn how to navigate this guide effectively. Section 2 of 4 ~8 min read Your Learning Journey Choose your path to DSPy mastery Welcome to Your DSPy Learning Adventure This book is crafted to be your comprehensive companion in mastering DSPy, whether you're taking your first steps in LLM programming or looking to level up your existing skills. Think of this book not just as a manual, but as your personal guide through the exciting landscape of programmatic LM development. The Book's Architecture Part I Foundations Core Concepts & Setup Chapter 1 ‚≠ê Beginner 4-6 hours Part II Core Concepts Building Blocks Chapters 2-3 ‚≠ê‚≠ê Intermediate 8-12 hours Part III Optimization Performance & Tuning Chapters 4-5 ‚≠ê‚≠ê‚≠ê Advanced 10-14 hours Part IV Applications Real-World Use Chapters 6-7 ‚≠ê‚≠ê‚≠ê Advanced 12-16 hours Part V Case Studies Domain Expertise Chapter 8 ‚≠ê‚≠ê‚≠ê‚≠ê Expert 8-12 hours Three Paths to Mastery üå± Path 1: Complete Beginner Perfect for you if: New to the world of LLM programming Want a comprehensive foundation Prefer structured, step-by-step learning Start Here ‚Üí Setup Environment ‚Üí Learn Fundamentals ‚Üí Build Projects ‚Üí Master DSPy! ‚è±Ô∏è 40-60 hours üìÖ 6-8 weeks üöÄ Path 2: Intermediate Developer Ideal for you if: Comfortable with Python and ML concepts Familiar with basic LLM operations Want to leverage DSPy efficiently Quick Review ‚Üí DSPy-Specific ‚Üí Master Optimization ‚Üí Production Ready ‚è±Ô∏è 20-30 hours üìÖ 3-4 weeks üéØ Path 3: Advanced Practitioner Designed for you if: Already comfortable with DSPy basics Solving specific, complex challenges Need best practices and advanced patterns 40% Case Studies 25% Advanced Patterns 20% Optimization 15% Production ‚è±Ô∏è 5-20 hours üìÖ As needed Quick Reference Guide Goal Chapters to Focus On Building RAG Systems 6, 8 (Healthcare & Enterprise cases) Creating Agents 3 (ReAct), 6 (Intelligent Agents) Optimization Mastery 5, relevant case studies Production Deployment 7, 8 (Finance & Legal cases) Every Chapter's Structure üìñ Chapter Start ‚Üì üéØ Overview & Objectives ‚Üí ‚úÖ Prerequisites Check ‚Üí üìö Core Content ‚Üì üíª Practical Examples ‚Üí ‚ú® Best Practices ‚Üí ‚ö†Ô∏è Common Pitfalls ‚Üì üìù Summary ‚Üí üèãÔ∏è Exercises ‚Üí üîó Resources ‚Üì üéâ Chapter Complete Difficulty Navigation ‚≠ê Beginner New concepts, gentle introduction ‚≠ê‚≠ê Intermediate Building on basics, practical skills ‚≠ê‚≠ê‚≠ê Advanced Complex topics, expert techniques ‚≠ê‚≠ê‚≠ê‚≠ê Expert Production-level, cutting-edge We believe in a \"Code-First, Context-Always\" approach. Each chapter is a self-contained module designed to take you from concept to deployment. You won't just learn the syntax; you will learn the Best Practices that make your pipelines efficient and the Common Pitfalls that break production systems. From the initial Overview to the final Summary, every section is engineered to respect your time and accelerate your development workflow. Your Support System üìñ Official DSPy Docs Technical documentation and API reference üíª GitHub Repository Source code and issue tracking üìö This Book's Repository All code examples and solutions Ready to Begin Your Journey? üìö Review Prerequisites ‚Üí ‚öôÔ∏è Complete Setup ‚Üí üöÄ Start Learning üåü Learning DSPy is not just about acquiring a new skill‚Äî it's about joining the forefront of AI development. Every expert was once a beginner. Every master was once a learner. Your DSPy journey starts now! Continue to Prerequisites"}, {"title": "Introduction", "url": "chapters/chapter-00/index-full-backup.html", "content": "Introduction Getting Started Everything you need to know before diving into DSPy: the preface, reading guide, prerequisites, and complete setup instructions. 4 Sections ~20 min read Section 1 Preface Welcome to DSPy: A Practical Guide A comprehensive journey from prompt engineering to prompt programming The Challenge of Traditional Prompt Engineering We have all been there. You discover the power of LLMs, and it feels like magic‚Äîat first. You write a prompt manually, and it works. But then you encounter an edge case. As illustrated above, this kicks off a cycle of Manual Prompt Writing and endless Trial & Error. You tweak a word here, add a 'few-shot' example there, and test it against a handful of inputs. It seems to work, so you push it. But this isn't engineering; it‚Äôs guessing. The result is inevitably Brittle, Hard-to-Maintain Code. When the underlying model updates or your data shifts, your carefully crafted prompt breaks, leaving you with a fragile system that is impossible to scale. ‚ö° Brittleness Small changes break behavior High Impact üìà Scalability Doesn't work for complex pipelines High Impact üîß Maintainability Hard to update and debug High Impact üîÑ Reproducibility Inconsistent results across runs High Impact The DSPy Paradigm Shift DSPy changes everything. Instead of manually tuning prompts, DSPy introduces a programming paradigm for LM-based applications. Think of it as the difference between: ‚ùå Traditional Approach prompt = \"Summarize this text: {text}\" response = llm(prompt) ‚úÖ DSPy Approach class Summarize(dspy.Signature): \"\"\"Summarize a given text.\"\"\" text = dspy.InputField() summary = dspy.OutputField() summarizer = dspy.ChainOfThought(Summarize) Notice the fundamental difference in data types. The Traditional Approach relies on 'string concatenation'‚Äîstuffing variables into a sentence and hoping the LLM understands. It is fragile and hard to test. In contrast, the DSPy Approach treats the LLM interaction as a Function. By defining a class with clear InputField and OutputField, you separate what you want (the signature) from how the prompt is written. This allows DSPy to automatically optimize the prompt in the background while your code remains clean and typed. The Learning Journey Who This Book Is For üå± Complete Beginners New to DSPy and want to learn from scratch? Understand Python basics but haven't worked extensively with LLMs? Want a step-by-step guide with clear explanations? DSPy Basics First Programs Core Concepts Best Practices üöÄ Intermediate Developers Worked with LLMs and prompt engineering before? Understand the basics of AI/ML concepts? Want to learn DSPy's framework for robust applications? Prompt Engineering ‚Üí DSPy Framework üèÜ Advanced Practitioners Already familiar with DSPy's basic concepts? Want to learn optimization techniques and production patterns? Looking for real-world case studies and advanced applications? ‚ö° Advanced Optimization üöÄ Production Deployment üîß Custom Components Regardless of your level, this book provides multiple reading paths so you can start at the right place and progress at your own pace. What Makes This Book Unique üéØ Practical Focus 50+ runnable code examples Real-world case studies from healthcare to finance Production-ready patterns you can use immediately üìö Complete Coverage 9 research papers explained with implementations All DSPy optimizers from BootstrapFewShot to MIPRO End-to-end workflows from concept to deployment üõ†Ô∏è Hands-On Learning 40+ exercises with detailed solutions Progressive complexity ensuring solid foundations Challenge problems for advanced learners The Real-World Impact Industry Applications Domain Challenge DSPy Solution Business Impact üè• Healthcare Clinical Note Analysis Structured Extraction ‚¨ÜÔ∏è 50% Faster Processing üí∞ Finance Risk Assessment Multi-Stage Evaluation ‚¨ÜÔ∏è 40% Better Accuracy ‚öñÔ∏è Legal Contract Review Automated Clause Extraction ‚¨áÔ∏è 80% Time Reduction üî¨ Research Literature Review Knowledge Synthesis ‚¨ÜÔ∏è 3x Coverage üí¨ Support Customer Service Contextual Responses ‚¨ÜÔ∏è 60% Satisfaction Traditional Methods 100% DSPy Optimized 250% Your Learning Toolkit ‚úÖ Complete Code Repository with all examples ‚úÖ Docker Environment for consistency ‚úÖ Jupyter Notebooks for experimentation ‚úÖ Solution Keys for all exercises ‚úÖ Bonus Materials and resources Acknowledgments This book stands on the shoulders of giants. Special thanks to: The DSPy Team at Stanford NLP for creating this revolutionary framework Omar Khattab and contributors for their dedication and vision The Community for feedback, examples, and enthusiasm Early Readers for their valuable insights and suggestions Dustin Ober December 2025 Continue to How to Use This Book Section 2 How to Use This Book Your Learning Journey Choose your path to DSPy mastery Welcome to Your DSPy Learning Adventure This book is crafted to be your comprehensive companion in mastering DSPy, whether you're taking your first steps in LLM programming or looking to level up your existing skills. Think o"}, {"title": "Setup Instructions", "url": "chapters/chapter-00/setup.html", "content": "Introduction Setup Instructions Get your DSPy environment ready in 15-30 minutes with step-by-step setup instructions. Section 4 of 4 ~15 min read üöÄ Setup Instructions Get your DSPy environment ready in 15-30 minutes Your Setup Roadmap 1 Python Check Verify Python 3.9+ 1-2 min 2 Project Setup Create directory 1 min 3 Virtual Env Isolate dependencies 2-3 min 4 Installation Install DSPy 2-5 min 5 API Config Set up keys 3-5 min 6 Testing Verify setup 2-3 min ‚úì Ready! Start learning Step 1: Verify Python Installation Open your terminal and run: python3 --version Expected outputs (any of these is good): Python 3.9.0 ‚úÖ Minimum required Python 3.10.8 ‚úÖ Good choice Python 3.11.5 ‚úÖ Latest stable Python 3.12.0 ‚úÖ Cutting edge üí° Pro Tip: If python3 doesn't work, try python. Different systems use different commands. Need to Install/Upgrade Python? ü™ü Windows Python Installer üçé macOS brew install python üêß Linux sudo apt install python3.11 Step 2: Create Your Project Directory üçéüêß macOS / Linux ü™ü Windows CMD ü™ü PowerShell # Create the directory mkdir ~/dspy-learning # Navigate into it cd ~/dspy-learning # Verify location pwd üéØ Success: You should see your DSPy project directory path! Step 3: Set Up Virtual Environment Why Virtual Environments? üì¶ Isolate Dependencies üõ°Ô∏è Avoid Conflicts üßπ Clean Management Create the environment: python3 -m venv venv Activate the Environment macOS / Linux source venv/bin/activate Windows (CMD) venv\\Scripts\\activate Windows (PowerShell) venv\\Scripts\\Activate.ps1 Verify activation: Your prompt should now show (venv) at the beginning! üö® Important: Always activate your virtual environment before working on DSPy projects! Step 4: Install DSPy and Dependencies 1. Start with fresh pip: pip install --upgrade pip 2. Install DSPy core: pip install dspy-ai What you get: ‚úÖ DSPy framework ‚úÖ Core dependencies ‚úÖ Language model adapters 3. Verify installation: python3 -c \"import dspy; print(f'DSPy version: {dspy.__version__}')\" DSPy version: 2.5.x 4. Install additional dependencies: pip install openai anthropic python-dotenv openai OpenAI API client anthropic Claude API client python-dotenv Environment variable management Step 5: Configure API Access Recommended üí≥ OpenAI ü§ñ Claude Free üè† Local Getting Your OpenAI API Key Visit: platform.openai.com Sign up or log in Navigate to API Keys section Create new secret key Copy your key (starts with sk-) Add to your .env file: OPENAI_API_KEY=sk-your-actual-api-key-here Getting Your Anthropic Claude API Key Visit: console.anthropic.com Sign up or log in Navigate to Settings ‚Üí API Keys Create new API key Copy your key (starts with sk-ant-) Add to your .env file: ANTHROPIC_API_KEY=sk-ant-your-actual-api-key-here üí° Using Claude in DSPy: lm = dspy.LM( model=\"anthropic/claude-3-5-sonnet-20241022\", api_key=os.getenv(\"ANTHROPIC_API_KEY\") ) dspy.configure(lm=lm) Setting Up Local Models with Ollama (Free!) Download Ollama: ollama.ai Install Ollama for your operating system Open terminal and pull a model: Pull a lightweight model (~500MB): ollama pull qwen2.5:0.5b Start Ollama server: ollama serve üí° Using Ollama in DSPy: lm = dspy.LM( model=\"ollama_chat/qwen2.5:0.5b\", api_base=\"http://localhost:11434\" ) dspy.configure(lm=lm) ‚úÖ Completely free ‚úÖ No API keys needed ‚úÖ Runs offline ‚úÖ Data stays private Create a .env file: touch .env üö® Security Warning: Never commit API keys to Git or share them publicly! Step 6: Test Your Setup üß™ The Moment of Truth! Create test_setup.py: \"\"\" ‚ú® DSPy Setup Verification Script ‚ú® Tests your installation and API connectivity \"\"\" import os from dotenv import load_dotenv import dspy # Load environment variables load_dotenv() def test_setup(): print(\"üöÄ DSPy Setup Verification\") print(\"=\" * 50) # Check DSPy print(f\"‚úÖ DSPy version: {dspy.__version__}\") # Check API key api_key = os.getenv(\"OPENAI_API_KEY\") if api_key: print(\"‚úÖ OpenAI API key found\") # Configure and test lm = dspy.LM( model=\"openai/gpt-4o-mini\", api_key=api_key ) dspy.configure(lm=lm) # Simple test class SimpleQA(dspy.Signature): question: str = dspy.InputField() answer: str = dspy.OutputField() predictor = dspy.Predict(SimpleQA) result = predictor(question=\"What is 2 + 2?\") print(f\"‚úÖ API Test: 2 + 2 = {result.answer}\") print(\"=\" * 50) print(\"üéâ SUCCESS! Your DSPy environment is ready!\") else: print(\"‚ö†Ô∏è No API key found. Add to .env file.\") if __name__ == \"__main__\": test_setup() Run the test: python3 test_setup.py Expected Successful Output: üöÄ DSPy Setup Verification ================================================== ‚úÖ DSPy version: 2.5.x ‚úÖ OpenAI API key found ‚úÖ API Test: 2 + 2 = 4 ================================================== üéâ SUCCESS! Your DSPy environment is ready! Troubleshooting Common Issues \"No module named 'dspy'\" Activate venv: source venv/bin/activate \"API key not found\" Create .env file with your key \"Invalid API key\" Verify key in provider dashboard Step 7: Get Book Examples (Optional) Clone the book repository: # Go to your learning directory cd ~/dspy-learning # Clone the b"}, {"title": "Prerequisites", "url": "chapters/chapter-00/prerequisites.html", "content": "Introduction Prerequisites Everything you need before diving into DSPy - Python skills, command line basics, and LLM fundamentals. Section 3 of 4 ~8 min read Prerequisites Everything you need before diving into DSPy Setting the Foundation Before diving into DSPy's powerful capabilities, let's ensure you have the solid foundation needed for a successful learning journey. Think of this as your pre-flight checklist‚Äîwe want to make sure you're equipped for takeoff! üìö Knowledge Python Skills Command Line LLM Basics ‚öôÔ∏è Technical OS Compatibility Python 3.9+ Editor/IDE üîë API Access OpenAI Claude Local Models üêç 1. Python Programming (Foundation Required) Basic Syntax Object-Oriented Modules & Packages Error Handling Data Structures Skill Level ‚úÖ Must Have üí° Helpful Beginner Variables, loops, conditionals List comprehensions Core Functions, classes, inheritance Decorators, generators Practical Import/using modules Package creation Robust Try/except blocks Context managers Experience Level: 6+ months of practical Python programming üìù Self-Assessment Test Can you read and understand this code? class DataProcessor: \"\"\"Process data with error handling and transformation.\"\"\" def __init__(self, data: list[str]): self.data = data self.processed_count = 0 def process(self) -> list[str]: \"\"\"Transform all data items, handling errors gracefully.\"\"\" results = [] for item in self.data: try: transformed = self._transform(item) results.append(transformed) self.processed_count += 1 except ValueError as e: print(f\"‚ö†Ô∏è Skipping '{item}': {e}\") print(f\"‚úÖ Processed {self.processed_count}/{len(self.data)} items\") return results def _transform(self, item: str) -> str: if not item.strip(): raise ValueError(\"Empty string not allowed\") return item.strip().upper() # Usage example processor = DataProcessor([\"hello\", \"world\", \"\", \"DSPy\"]) results = processor.process() ‚úÖ If this feels comfortable, you're ready! Need to Level Up? Python.org Tutorial Official Guide Real Python Practical Tutorials Automate the Boring Stuff Project-Based üñ•Ô∏è 2. Command Line Essentials (Daily Use) üìÅ Navigate Files cd, ls, pwd ‚ñ∂Ô∏è Run Scripts python script.py üì¶ Manage Packages pip install Task Command Description Navigate cd path/ Change directory List contents ls (macOS/Linux) See files/folders Current location pwd Show current path Run Python python script.py Execute Python file Install packages pip install package Add Python packages ü§ñ 3. LLM Fundamentals (Conceptual Understanding) What are they? ChatGPT, Claude, GPT-4 How to use them Prompting & APIs API concepts Tokens & limits üéâ Good News! You don't need to be an LLM expert! Chapter 1 covers everything you need. But having these concepts helps: ‚úÖ Know what ChatGPT/Claude are ‚úÖ Understand you can \"ask\" AI questions ‚úÖ Basic awareness that AI can be accessed via code üíª Technical Setup Requirements Operating System üçé macOS 10.14+ ‚úÖ Native üêß Linux (Ubuntu 20.04+) ‚úÖ Native ü™ü Windows 10/11 ‚úÖ WSL2 recommended Python Version python3 --version Python 3.9.0 ‚úÖ Python 3.10.x ‚úÖ Python 3.11.x ‚úÖ Python 3.12.x ‚úÖ Code Editor VS Code Beginner Friendly, Free PyCharm Community Python-Focused, Free Google Colab No Install Required üîë LLM Access & API Keys üí≥ OpenAI (Recommended for Beginners) Cost ~$0.002 per 1K tokens (GPT-4o-mini) Free Credit $5 for new accounts Models GPT-4o, GPT-4o-mini, GPT-3.5-turbo Sign Up ‚Üí ü§ñ Anthropic Claude (Production-Ready) Cost Competitive with OpenAI Models Claude-3.5-Sonnet, Claude-3-Haiku Best For Production applications Sign Up ‚Üí üè† Local Models (Free & Private) Cost Free (uses your computer) Recommended qwen2.5:0.5b (only 0.5GB!) Tools Ollama, LM Studio Get Ollama ‚Üí üí∞ Budget Expectations Per Chapter $0.50 - $2.00 Total Book $5 - $20 total üí° Use cheap models üí° Cache responses üí° Local development üñ•Ô∏è Hardware Requirements Minimum CPU: Any modern RAM: 4GB Storage: 2GB Recommended CPU: 2+ GHz RAM: 8GB+ Storage: 5GB+ Local Models GPU: 8GB+ VRAM RAM: 16GB+ Storage: 50GB+ üí° Important: DSPy itself is lightweight! Your computer just sends API requests. Heavy hardware is only needed if you want to run large models locally. ‚úàÔ∏è Pre-Flight Checklist ‚òê Python 3.9+ installed and working ‚òê pip package manager available ‚òê Code editor chosen and installed ‚òê Basic Python skills (can write classes and functions) ‚òê Command line basics (can navigate and run scripts) ‚òê API access to at least one LLM provider ‚òê Time allocated for setup (1-2 hours) üåü Remember: Every Expert Was Once a Beginner You don't need to be perfect‚Äîyou just need to start. DSPy is designed to make LLM development accessible, and this book is designed to make DSPy accessible to you. Let's build something amazing together! üöÄ Continue to Setup Instructions"}, {"title": "Preface", "url": "chapters/chapter-00/index.html", "content": "Introduction Preface Welcome to DSPy: A Practical Guide - A comprehensive journey from prompt engineering to prompt programming. Section 1 of 4 ~5 min read Welcome to DSPy: A Practical Guide A comprehensive journey from prompt engineering to prompt programming The Challenge of Traditional Prompt Engineering We have all been there. You discover the power of LLMs, and it feels like magic‚Äîat first. You write a prompt manually, and it works. But then you encounter an edge case. As illustrated above, this kicks off a cycle of Manual Prompt Writing and endless Trial & Error. You tweak a word here, add a 'few-shot' example there, and test it against a handful of inputs. It seems to work, so you push it. But this isn't engineering; it's guessing. The result is inevitably Brittle, Hard-to-Maintain Code. When the underlying model updates or your data shifts, your carefully crafted prompt breaks, leaving you with a fragile system that is impossible to scale. ‚ö° Brittleness Small changes break behavior High Impact üìà Scalability Doesn't work for complex pipelines High Impact üîß Maintainability Hard to update and debug High Impact üîÑ Reproducibility Inconsistent results across runs High Impact The DSPy Paradigm Shift DSPy changes everything. Instead of manually tuning prompts, DSPy introduces a programming paradigm for LM-based applications. Think of it as the difference between: ‚ùå Traditional Approach prompt = \"Summarize this text: {text}\" response = llm(prompt) ‚úÖ DSPy Approach class Summarize(dspy.Signature): \"\"\"Summarize a given text.\"\"\" text = dspy.InputField() summary = dspy.OutputField() summarizer = dspy.ChainOfThought(Summarize) Notice the fundamental difference in data types. The Traditional Approach relies on 'string concatenation'‚Äîstuffing variables into a sentence and hoping the LLM understands. It is fragile and hard to test. In contrast, the DSPy Approach treats the LLM interaction as a Function. By defining a class with clear InputField and OutputField, you separate what you want (the signature) from how the prompt is written. This allows DSPy to automatically optimize the prompt in the background while your code remains clean and typed. The Learning Journey Who This Book Is For üå± Complete Beginners New to DSPy and want to learn from scratch? Understand Python basics but haven't worked extensively with LLMs? Want a step-by-step guide with clear explanations? üöÄ Intermediate Developers Worked with LLMs and prompt engineering before? Understand the basics of AI/ML concepts? Want to learn DSPy's framework for robust applications? üèÜ Advanced Practitioners Already familiar with DSPy's basic concepts? Want to learn optimization techniques and production patterns? Looking for real-world case studies and advanced applications? Regardless of your level, this book provides multiple reading paths so you can start at the right place and progress at your own pace. Acknowledgments This book stands on the shoulders of giants. Special thanks to: The DSPy Team at Stanford NLP for creating this revolutionary framework Omar Khattab and contributors for their dedication and vision The Community for feedback, examples, and enthusiasm Early Readers for their valuable insights and suggestions Dustin Ober December 2025 Credits & Licensing Code Attribution: Code examples and technical concepts in this ebook are adapted from the official DSPy repository and documentation, licensed under the MIT License. Asset Attribution: The DSPy name and logo are property of their respective owners. This ebook is an independent community project and is not officially affiliated with the DSPy development team. All illustrative images and diagrams within this ebook were created with the assistance of Google Gemini. Continue to How to Use This Book"}]