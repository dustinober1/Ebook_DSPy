<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Human-Aligned Evaluation - DSPy: A Practical Guide</title>


        <!-- Custom HTML head -->

        <meta name="description" content="The most comprehensive DSPy guide with complete coverage of 9 research papers, advanced optimization techniques, and production-ready applications">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../assets/custom-837cb9c5.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex-8a4736e7.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-90eb277b.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">DSPy: A Practical Guide</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>
                        <a href="https://github.com/dustinober1/Ebook_DSPy" title="Git repository" aria-label="Git repository">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span>
                        </a>
                        <a href="https://github.com/dustinober1/Ebook_DSPy/edit/main/src/04-evaluation/09-human-aligned-evaluation.md" title="Suggest an edit" aria-label="Suggest an edit" rel="edit">
                            <span class=fa-svg id="git-edit-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M421.7 220.3l-11.3 11.3-22.6 22.6-205 205c-6.6 6.6-14.8 11.5-23.8 14.1L30.8 511c-8.4 2.5-17.5 .2-23.7-6.1S-1.5 489.7 1 481.2L38.7 353.1c2.6-9 7.5-17.2 14.1-23.8l205-205 22.6-22.6 11.3-11.3 33.9 33.9 62.1 62.1 33.9 33.9zM96 353.9l-9.3 9.3c-.9 .9-1.6 2.1-2 3.4l-25.3 86 86-25.3c1.3-.4 2.5-1.1 3.4-2l9.3-9.3H112c-8.8 0-16-7.2-16-16V353.9zM453.3 19.3l39.4 39.4c25 25 25 65.5 0 90.5l-14.5 14.5-22.6 22.6-11.3 11.3-33.9-33.9-62.1-62.1L314.3 67.7l11.3-11.3 22.6-22.6 14.5-14.5c25-25 65.5-25 90.5 0z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="human-aligned-evaluation-capturing-what-really-matters"><a class="header" href="#human-aligned-evaluation-capturing-what-really-matters">Human-Aligned Evaluation: Capturing What Really Matters</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Traditional evaluation metrics like BERTScore, ROUGE, and BLEU often fail to capture what truly matters to human users, especially in complex, nuanced tasks. Human-aligned evaluation focuses on creating evaluation systems that reflect actual human priorities and domain-specific quality requirements.</p>
<p>This section draws on real-world experiences from building evaluation systems for clinical summarization, demonstrating how to bridge the gap between automated metrics and human judgment.</p>
<h2 id="the-limitations-of-standard-metrics"><a class="header" href="#the-limitations-of-standard-metrics">The Limitations of Standard Metrics</a></h2>
<h3 id="why-off-the-shelf-metrics-fail"><a class="header" href="#why-off-the-shelf-metrics-fail">Why Off-the-Shelf Metrics Fail</a></h3>
<pre><code class="language-python"># Example from clinical summarization
standard_metrics = {
    "bert_score": 87.19,  # High semantic similarity
    "rouge_2": 0.82,      # Good n-gram overlap
    # But missed critical clinical details!
}

# Human evaluation revealed:
# - Omitted key diagnoses
# - Missing treatment outcomes
# - Incomplete patient history
</code></pre>
<p><strong>Key Problems:</strong></p>
<ol>
<li><strong>Context-blind</strong>: Metrics don’t understand task-specific requirements</li>
<li><strong>Surface-level</strong>: Focus on lexical overlap, not meaningful content</li>
<li><strong>One-size-fits-all</strong>: Can’t adapt to different use cases or priorities</li>
<li><strong>Poor correlation</strong>: Often weak correlation with actual human judgment</li>
</ol>
<h3 id="the-correlation-crisis"><a class="header" href="#the-correlation-crisis">The Correlation Crisis</a></h3>
<p>Studies have shown concerning correlations between standard metrics and human judgment:</p>
<pre><code class="language-python"># Real-world correlation data from summarization tasks
correlations = {
    "BERTScore": 0.14,      # Almost no correlation
    "ROUGE-2": 0.21,        # Weak correlation
    "BLEU": 0.18,          # Poor correlation
    "Human-aligned LLM": 0.28  # 2x better correlation
}
</code></pre>
<h2 id="building-human-aligned-evaluation-systems"><a class="header" href="#building-human-aligned-evaluation-systems">Building Human-Aligned Evaluation Systems</a></h2>
<h3 id="1-understand-your-quality-dimensions"><a class="header" href="#1-understand-your-quality-dimensions">1. Understand Your Quality Dimensions</a></h3>
<p>First, identify what matters for your specific task:</p>
<pre><code class="language-python">class ClinicalQualityDimensions:
    """Quality dimensions for clinical summarization."""

    FACTUAL_ACCURACY = "Is all information correct?"
    CLINICAL_COMPLETENESS = "Are all critical findings included?"
    CONCISENESS = "Is it appropriately brief?"
    CLINICAL_RELEVANCE = "Is information clinically significant?"
    TEMPORAL_ACCURACY = "Are timelines and sequences correct?"

    @classmethod
    def get_weights(cls):
        """Different weights for different clinical contexts."""
        return {
            "emergency": {
                cls.FACTUAL_ACCURACY: 0.5,
                cls.CLINICAL_COMPLETENESS: 0.3,
                cls.CONCISENESS: 0.1,
                cls.CLINICAL_RELEVANCE: 0.1
            },
            "routine_followup": {
                cls.FACTUAL_ACCURACY: 0.3,
                cls.CLINICAL_COMPLETENESS: 0.2,
                cls.CONCISENESS: 0.3,
                cls.CLINICAL_RELEVANCE: 0.2
            }
        }
</code></pre>
<h3 id="2-collect-granular-human-feedback"><a class="header" href="#2-collect-granular-human-feedback">2. Collect Granular Human Feedback</a></h3>
<p>Use structured interfaces to capture nuanced human judgments:</p>
<pre><code class="language-python">class HumanFeedbackCollector:
    """Collect structured human feedback for evaluation alignment."""

    def __init__(self, quality_dimensions):
        self.dimensions = quality_dimensions
        self.feedback_data = []

    def collect_feedback(self, example, prediction, context):
        """Collect human evaluation with detailed breakdown."""
        feedback = {
            "example_id": example.id,
            "prediction": prediction,
            "context": context,
            "ratings": {},
            "detailed_feedback": {},
            "overall_score": None
        }

        # Rate each dimension
        for dimension in self.dimensions:
            rating = input(f"Rate {dimension} (1-5): ")
            feedback["ratings"][dimension] = int(rating)

            # Collect specific feedback
            detail = input(f"Specific feedback for {dimension}: ")
            feedback["detailed_feedback"][dimension] = detail

        # Overall assessment
        feedback["overall_score"] = int(input("Overall quality (1-5): "))

        self.feedback_data.append(feedback)
        return feedback

    def analyze_patterns(self):
        """Identify common failure patterns from collected feedback."""
        patterns = {}

        for dimension in self.dimensions:
            low_scores = [
                f for f in self.feedback_data
                if f["ratings"][dimension] &lt;= 2
            ]

            if low_scores:
                # Extract common issues from feedback
                issues = [
                    f["detailed_feedback"][dimension]
                    for f in low_scores
                ]
                patterns[dimension] = self._cluster_issues(issues)

        return patterns

    def _cluster_issues(self, issues):
        """Simple clustering of similar issues."""
        # In practice, use NLP clustering techniques
        from collections import Counter

        # Simple keyword-based clustering
        clusters = {}
        for issue in issues:
            keywords = issue.lower().split()[:3]  # First 3 words
            key = " ".join(keywords)
            if key not in clusters:
                clusters[key] = []
            clusters[key].append(issue)

        return clusters
</code></pre>
<h3 id="3-llm-as-a-judge-with-human-guided-prompts"><a class="header" href="#3-llm-as-a-judge-with-human-guided-prompts">3. LLM-as-a-Judge with Human-Guided Prompts</a></h3>
<p>Create judges that encode human priorities:</p>
<pre><code class="language-python">class HumanAlignedLLMJudge(dspy.Module):
    """LLM judge trained on human feedback patterns."""

    def __init__(self, quality_dimensions, weights=None):
        super().__init__()
        self.dimensions = quality_dimensions
        self.weights = weights or {d: 0.25 for d in quality_dimensions}

        # Create evaluation signature
        self.evaluation_signature = dspy.Signature(
            """Evaluate a clinical summary against reference text.

            Quality Dimensions to Assess:
            {dimensions}

            For each dimension:
            1. Rate from 0.0 (poor) to 1.0 (excellent)
            2. Provide specific justification
            3. Note any critical issues

            Reference Summary: {reference}
            Generated Summary: {candidate}
            Context: {context}
            """,
            dspy.InputField(desc="Reference summary"),
            dspy.InputField(desc="Generated summary"),
            dspy.InputField(desc="Additional context"),
            dspy.OutputField(desc="Factual accuracy score"),
            dspy.OutputField(desc="Completeness score"),
            dspy.OutputField(desc="Conciseness score"),
            dspy.OutputField(desc="Overall weighted score"),
            dspy.OutputField(desc="Detailed justification")
        )

        self.judge = dspy.ChainOfThought(self.evaluation_signature)

    def forward(self, reference, candidate, context=None):
        """Evaluate with human-aligned criteria."""
        # Format dimensions for prompt
        dim_text = "\n".join([
            f"- {dim}: {desc}"
            for dim, desc in self.dimensions.items()
        ])

        result = self.judge(
            dimensions=dim_text,
            reference=reference,
            candidate=candidate,
            context=context or "No additional context"
        )

        # Calculate weighted score
        scores = {
            "factual": getattr(result, 'factual_accuracy_score', 0),
            "completeness": getattr(result, 'completeness_score', 0),
            "conciseness": getattr(result, 'conciseness_score', 0)
        }

        weighted_score = sum(
            scores[dim] * self.weights.get(dim, 0.25)
            for dim in scores
        )

        return dspy.Prediction(
            scores=scores,
            weighted_score=weighted_score,
            justification=getattr(result, 'detailed_justification', ''),
            raw_result=result
        )
</code></pre>
<h2 id="case-study-clinical-summarization"><a class="header" href="#case-study-clinical-summarization">Case Study: Clinical Summarization</a></h2>
<h3 id="the-challenge"><a class="header" href="#the-challenge">The Challenge</a></h3>
<p>MultiClinSUM shared task: Multilingual clinical reports summarization where “quality” depends entirely on the use case:</p>
<ul>
<li>Clinician’s quick review: Needs key findings only</li>
<li>Patient understanding: Simplified language, no jargon</li>
<li>Billing system: Specific codes and procedures</li>
</ul>
<h3 id="solution-implementation"><a class="header" href="#solution-implementation">Solution Implementation</a></h3>
<pre><code class="language-python">class ClinicalSummarizationEvaluator:
    """Complete human-aligned evaluation for clinical summarization."""

    def __init__(self):
        self.human_collector = HumanFeedbackCollector([
            "Factual Accuracy",
            "Clinical Completeness",
            "Conciseness",
            "Clinical Relevance"
        ])

        self.llm_judge = HumanAlignedLLMJudge(
            quality_dimensions={
                "Factual Accuracy": "All medical information is correct",
                "Clinical Completeness": "Critical findings not omitted",
                "Conciseness": "Appropriate length for quick review",
                "Clinical Relevance": "Information is clinically significant"
            },
            weights={
                "Factual Accuracy": 0.5,
                "Clinical Completeness": 0.3,
                "Conciseness": 0.1,
                "Clinical Relevance": 0.1
            }
        )

    def evaluate_system(self, system, test_set):
        """Comprehensive evaluation with multiple metrics."""
        results = {
            "standard_metrics": {},
            "human_aligned": {},
            "correlation_analysis": {}
        }

        # Collect predictions
        predictions = []
        for example in test_set:
            pred = system(example.document)
            predictions.append(pred)

        # Calculate standard metrics
        results["standard_metrics"] = self._calculate_standard_metrics(
            test_set, predictions
        )

        # Human-aligned evaluation
        for example, pred in zip(test_set, predictions):
            # LLM judge evaluation
            judge_result = self.llm_judge(
                reference=example.summary,
                candidate=pred.summary,
                context=example.context
            )

            # Store results
            example.judge_score = judge_result.weighted_score
            example.judge_breakdown = judge_result.scores

        # Calculate human-aligned scores
        results["human_aligned"] = {
            "llm_judge_avg": np.mean([e.judge_score for e in test_set]),
            "dimension_averages": self._calculate_dim_averages(test_set)
        }

        return results

    def validate_alignment(self, human_feedback_data):
        """Check if LLM judge aligns with human judgment."""
        correlations = {}

        for example in human_feedback_data:
            # Compare human overall score with LLM judge
            human_score = example.overall_score / 5.0  # Normalize to [0,1]
            llm_score = example.llm_judge_score

            # Calculate correlation
            correlations.append((human_score, llm_score))

        spearman_rho = self._calculate_spearman(correlations)

        return {
            "spearman_correlation": spearman_rho,
            "alignment_quality": "good" if spearman_rho &gt; 0.5 else "needs_improvement",
            "recommendations": self._generate_alignment_recommendations(spearman_rho)
        }
</code></pre>
<h3 id="results-the-power-of-human-alignment"><a class="header" href="#results-the-power-of-human-alignment">Results: The Power of Human Alignment</a></h3>
<p>After implementing the human-aligned system:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Metric</th><th>Before Optimization</th><th>After Optimization</th><th>Improvement</th></tr>
</thead>
<tbody>
<tr><td>BERTScore</td><td>87.19</td><td>87.27</td><td>+0.08</td></tr>
<tr><td><strong>LLM Judge</strong></td><td><strong>53.90</strong></td><td><strong>68.07</strong></td><td><strong>+26.3%</strong></td></tr>
<tr><td>Human Alignment</td><td>ρ=0.14</td><td>ρ=0.28</td><td><strong>2x improvement</strong></td></tr>
</tbody>
</table>
</div>
<p>Key insights:</p>
<ol>
<li><strong>BERTScore barely changed</strong> - It wasn’t measuring what mattered</li>
<li><strong>Human-aligned metric improved 26%</strong> - Optimizing for the right target</li>
<li><strong>Correlation with humans doubled</strong> - Better alignment with actual needs</li>
</ol>
<h2 id="integration-with-dspy-optimization"><a class="header" href="#integration-with-dspy-optimization">Integration with DSPy Optimization</a></h2>
<h3 id="using-human-aligned-metrics-for-compilation"><a class="header" href="#using-human-aligned-metrics-for-compilation">Using Human-Aligned Metrics for Compilation</a></h3>
<pre><code class="language-python"># Configure DSPy optimizer with human-aligned metric
def human_aligned_metric(gold, pred, trace=None):
    """Metric that captures clinical quality."""
    judge = HumanAlignedLLMJudge()
    result = judge(
        reference=gold.summary,
        candidate=pred.summary,
        context=getattr(gold, 'context', None)
    )
    return result.weighted_score &gt; 0.7  # Threshold for acceptable quality

# Compile with human guidance
optimizer = dspy.BootstrapFewShot(
    metric=human_aligned_metric,
    max_bootstrapped_demos=5,
    max_labeled_demos=3
)

optimized_summarizer = optimizer.compile(
    ClinicalSummarizer(),
    trainset=training_examples_with_human_feedback
)
</code></pre>
<h3 id="continuous-improvement-loop"><a class="header" href="#continuous-improvement-loop">Continuous Improvement Loop</a></h3>
<pre><code class="language-python">class ContinuousImprovementSystem:
    """System for ongoing evaluation and improvement."""

    def __init__(self):
        self.evaluator = ClinicalSummarizationEvaluator()
        self.feedback_collector = HumanFeedbackCollector()
        self.performance_history = []

    def deployment_cycle(self, current_model, new_data):
        """Continuous evaluation and retraining cycle."""
        # 1. Evaluate current performance
        current_results = self.evaluator.evaluate_system(
            current_model, new_data
        )

        # 2. Collect human feedback on edge cases
        edge_cases = self._identify_edge_cases(new_data, current_results)
        for case in edge_cases:
            self.feedback_collector.collect_feedback(
                case.example, case.prediction, case.context
            )

        # 3. Analyze patterns
        patterns = self.feedback_collector.analyze_patterns()

        # 4. Update evaluation criteria if needed
        if self._need_criteria_update(patterns):
            self._update_evaluation_criteria(patterns)

        # 5. Retrain with new insights
        if current_results["human_aligned"]["llm_judge_avg"] &lt; 0.7:
            optimized_model = self._retrain_with_feedback(
                current_model,
                self.feedback_collector.feedback_data
            )
            return optimized_model

        return current_model

    def _identify_edge_cases(self, data, results):
        """Find cases where model performance is poor."""
        edge_cases = []

        for i, example in enumerate(data):
            if example.judge_score &lt; 0.5:  # Poor performance
                edge_cases.append({
                    "example": example,
                    "prediction": example.generated_summary,
                    "context": example.context,
                    "score": example.judge_score
                })

        return edge_cases[:20]  # Top 20 worst cases
</code></pre>
<h2 id="best-practices-for-human-aligned-evaluation"><a class="header" href="#best-practices-for-human-aligned-evaluation">Best Practices for Human-Aligned Evaluation</a></h2>
<h3 id="1-start-clear-stay-consistent"><a class="header" href="#1-start-clear-stay-consistent">1. Start Clear, Stay Consistent</a></h3>
<pre><code class="language-python"># Good: Clear, actionable quality criteria
EVALUATION_RUBRIC = """
Factual Accuracy (50% weight):
- 1.0: All information verifiably correct
- 0.5: Minor inaccuracies that don't affect clinical meaning
- 0.0: Major errors that could impact care

Clinical Completeness (30% weight):
- 1.0: All critical findings included
- 0.5: Some findings missing but not critical
- 0.0: Critical information omitted
"""

# Bad: Vague, subjective criteria
BAD_RUBRIC = """
Rate the summary quality:
- Good: Looks nice
- Bad: Looks wrong
"""
</code></pre>
<h3 id="2-separate-training-from-evaluation-data"><a class="header" href="#2-separate-training-from-evaluation-data">2. Separate Training from Evaluation Data</a></h3>
<pre><code class="language-python"># Prevent leakage between optimization and evaluation
def create_strict_splits(data, train_ratio=0.6, dev_ratio=0.2):
    """Create splits with no overlap in patients or documents."""
    # Group by patient/document to prevent leakage
    patient_groups = {}
    for item in data:
        patient_id = item.get("patient_id", item["doc_id"])
        if patient_id not in patient_groups:
            patient_groups[patient_id] = []
        patient_groups[patient_id].append(item)

    patients = list(patient_groups.keys())
    random.shuffle(patients)

    # Split by patient, not by example
    train_cutoff = int(len(patients) * train_ratio)
    dev_cutoff = int(len(patients) * (train_ratio + dev_ratio))

    train_patients = patients[:train_cutoff]
    dev_patients = patients[train_cutoff:dev_cutoff]
    test_patients = patients[dev_cutoff:]

    # Create datasets
    trainset = []
    for p in train_patients:
        trainset.extend(patient_groups[p])

    # ... similar for dev and test

    return trainset, devset, testset
</code></pre>
<h3 id="3-version-control-everything"><a class="header" href="#3-version-control-everything">3. Version Control Everything</a></h3>
<pre><code class="language-python">class EvaluationVersionControl:
    """Track all components of evaluation system."""

    def __init__(self):
        self.versions = {}

    def snapshot_evaluation(self, version_name, components):
        """Save complete evaluation configuration."""
        snapshot = {
            "version": version_name,
            "timestamp": datetime.now(),
            "components": {
                "metric_prompt": components["metric_prompt"],
                "quality_dimensions": components["quality_dimensions"],
                "weights": components["weights"],
                "thresholds": components["thresholds"],
                "test_set_hash": self._hash_dataset(components["test_set"])
            }
        }

        self.versions[version_name] = snapshot

        # Save to file for reproducibility
        with open(f"evaluations/{version_name}.json", "w") as f:
            json.dump(snapshot, f, indent=2, default=str)

    def compare_versions(self, v1, v2):
        """Compare two evaluation versions."""
        return {
            "prompt_changes": self._diff_prompts(v1, v2),
            "weight_changes": self._diff_weights(v1, v2),
            "dimension_changes": self._diff_dimensions(v1, v2)
        }
</code></pre>
<h2 id="exercises"><a class="header" href="#exercises">Exercises</a></h2>
<ol>
<li>
<p><strong>Identify Quality Dimensions</strong>: For your task, list 3-5 key quality dimensions that standard metrics miss. Assign weights based on importance.</p>
</li>
<li>
<p><strong>Create Human Feedback Protocol</strong>: Design a structured form for collecting human feedback on your task’s outputs.</p>
</li>
<li>
<p><strong>Build LLM Judge</strong>: Implement an LLM judge that evaluates outputs based on your quality dimensions.</p>
</li>
<li>
<p><strong>Validate Alignment</strong>: Collect human judgments on 20 examples and calculate correlation with your LLM judge.</p>
</li>
<li>
<p><strong>Iterate and Improve</strong>: Based on misalignments, refine your judge prompt and re-evaluate.</p>
</li>
</ol>
<h2 id="key-takeaways"><a class="header" href="#key-takeaways">Key Takeaways</a></h2>
<ol>
<li><strong>Standard metrics often fail</strong> to capture what matters for complex tasks</li>
<li><strong>Human alignment is crucial</strong> for building evaluation systems that reflect real needs</li>
<li><strong>LLM-as-a-judge bridges the gap</strong> between automated metrics and human judgment</li>
<li><strong>Continuous feedback</strong> drives ongoing improvement</li>
<li><strong>Context matters</strong> - quality definitions must adapt to specific use cases</li>
</ol>
<p>Remember: Good evaluation systems evolve with your understanding of the task and its real-world impact. Start simple, collect feedback, and iteratively refine what “quality” means for your specific context.</p>
<hr>
<p><strong>References:</strong></p>
<ul>
<li>Explosion AI. (2025). Engineering a human-aligned LLM evaluation workflow with Prodigy and DSPy.</li>
<li>Statsig. (2025). DSPy vs prompt engineering: Systematic vs manual tuning.</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../04-evaluation/08-llm-as-a-judge.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../04-evaluation/06-exercises.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../04-evaluation/08-llm-as-a-judge.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../04-evaluation/06-exercises.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>


        <script>
            window.playground_line_numbers = true;
        </script>

        <script>
            window.playground_copyable = true;
        </script>

        <script src="../ace-2a3cd908.js"></script>
        <script src="../mode-rust-2c9d5c9a.js"></script>
        <script src="../editor-16ca416c.js"></script>
        <script src="../theme-dawn-4493f9c8.js"></script>
        <script src="../theme-tomorrow_night-9dbe62a9.js"></script>

        <script src="../elasticlunr-ef4e11c1.min.js"></script>
        <script src="../mark-09e88c2c.min.js"></script>
        <script src="../searcher-c2a407aa.js"></script>

        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->
        <script src="../assets/analytics-08893ea1.html"></script>



    </div>
    </body>
</html>
