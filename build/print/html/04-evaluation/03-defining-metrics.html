<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Defining Metrics - DSPy: A Practical Guide</title>


        <!-- Custom HTML head -->

        <meta name="description" content="The most comprehensive DSPy guide with complete coverage of 9 research papers, advanced optimization techniques, and production-ready applications">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../assets/print-only-ef201963.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-4ea68664.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">DSPy: A Practical Guide</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="defining-metrics"><a class="header" href="#defining-metrics">Defining Metrics</a></h1>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<ul>
<li><strong>Chapter 1-3</strong>: DSPy Fundamentals, Signatures, and Modules</li>
<li><strong>Previous Sections</strong>: Why Evaluation Matters, Creating Datasets</li>
<li><strong>Required Knowledge</strong>: Basic Python functions</li>
<li><strong>Difficulty Level</strong>: Intermediate-Advanced</li>
<li><strong>Estimated Reading Time</strong>: 35 minutes</li>
</ul>
<h2 id="learning-objectives"><a class="header" href="#learning-objectives">Learning Objectives</a></h2>
<p>By the end of this section, you will be able to:</p>
<ul>
<li>Understand the anatomy of DSPy metric functions</li>
<li>Use built-in metrics for common tasks</li>
<li>Create custom metrics for specific needs</li>
<li>Design composite metrics that capture multiple quality dimensions</li>
<li>Use the trace parameter for optimization-aware metrics</li>
</ul>
<h2 id="metric-function-anatomy"><a class="header" href="#metric-function-anatomy">Metric Function Anatomy</a></h2>
<p>A DSPy metric is a Python function that evaluates prediction quality:</p>
<pre><code class="language-python">def metric(example, pred, trace=None):
    """
    Evaluate prediction quality.

    Args:
        example: The original Example with inputs AND expected outputs
        pred: The Prediction (module output) to evaluate
        trace: Optional trace info (used during optimization)

    Returns:
        bool or float: Score indicating quality (True/False or 0.0-1.0)
    """
    # Compare prediction to expected output
    return pred.answer == example.answer
</code></pre>
<h3 id="the-three-parameters"><a class="header" href="#the-three-parameters">The Three Parameters</a></h3>
<h4 id="1-example---the-ground-truth"><a class="header" href="#1-example---the-ground-truth">1. <code>example</code> - The Ground Truth</a></h4>
<p>Contains both inputs and expected outputs:</p>
<pre><code class="language-python">example = dspy.Example(
    question="What is 2+2?",  # Input
    answer="4"                 # Expected output (ground truth)
).with_inputs("question")

# In metric:
def metric(example, pred, trace=None):
    ground_truth = example.answer  # Access expected output
    input_question = example.question  # Can also access input
</code></pre>
<h4 id="2-pred---the-models-prediction"><a class="header" href="#2-pred---the-models-prediction">2. <code>pred</code> - The Model’s Prediction</a></h4>
<p>The output from your DSPy module:</p>
<pre><code class="language-python"># Module produces prediction
module = dspy.Predict("question -&gt; answer")
pred = module(question="What is 2+2?")

# In metric:
def metric(example, pred, trace=None):
    model_output = pred.answer  # Access predicted output
</code></pre>
<h4 id="3-trace---optimization-context"><a class="header" href="#3-trace---optimization-context">3. <code>trace</code> - Optimization Context</a></h4>
<p>Indicates whether metric is being used for optimization:</p>
<pre><code class="language-python">def metric(example, pred, trace=None):
    # Calculate score
    score = calculate_similarity(example.answer, pred.answer)

    if trace is not None:
        # During optimization: return boolean for filtering
        return score &gt;= 0.9  # Only accept very good examples

    # During evaluation: return actual score
    return score
</code></pre>
<h2 id="built-in-metrics"><a class="header" href="#built-in-metrics">Built-in Metrics</a></h2>
<p>DSPy provides several ready-to-use metrics:</p>
<h3 id="semanticf1"><a class="header" href="#semanticf1">SemanticF1</a></h3>
<p>Measures semantic overlap between answers:</p>
<pre><code class="language-python">from dspy.evaluate import SemanticF1

# Initialize metric
metric = SemanticF1(decompositional=True)

# Use in evaluation
example = dspy.Example(
    question="What is photosynthesis?",
    response="The process by which plants convert sunlight to energy"
).with_inputs("question")

pred = module(question=example.question)

# Returns F1 score based on semantic similarity
score = metric(example, pred)
print(f"Semantic F1: {score}")
</code></pre>
<h3 id="exact-match"><a class="header" href="#exact-match">Exact Match</a></h3>
<p>Simple string equality:</p>
<pre><code class="language-python">def exact_match(example, pred, trace=None):
    """Exact string match metric."""
    return example.answer.strip().lower() == pred.answer.strip().lower()
</code></pre>
<h3 id="answer-correctness"><a class="header" href="#answer-correctness">Answer Correctness</a></h3>
<p>For QA tasks with known correct answers:</p>
<pre><code class="language-python">def answer_correctness(example, pred, trace=None):
    """Check if predicted answer contains the correct answer."""
    correct = example.answer.lower()
    predicted = pred.answer.lower()
    return correct in predicted or predicted in correct
</code></pre>
<h2 id="creating-custom-metrics"><a class="header" href="#creating-custom-metrics">Creating Custom Metrics</a></h2>
<h3 id="simple-boolean-metrics"><a class="header" href="#simple-boolean-metrics">Simple Boolean Metrics</a></h3>
<p>Return True/False for pass/fail:</p>
<pre><code class="language-python">def sentiment_accuracy(example, pred, trace=None):
    """Check if sentiment prediction matches ground truth."""
    return example.sentiment == pred.sentiment

def label_match(example, pred, trace=None):
    """Check if classification label matches."""
    expected = example.label.lower().strip()
    predicted = pred.label.lower().strip()
    return expected == predicted
</code></pre>
<h3 id="numeric-metrics"><a class="header" href="#numeric-metrics">Numeric Metrics</a></h3>
<p>Return scores between 0 and 1:</p>
<pre><code class="language-python">def partial_match(example, pred, trace=None):
    """Score based on word overlap."""
    expected_words = set(example.answer.lower().split())
    predicted_words = set(pred.answer.lower().split())

    if not expected_words:
        return 0.0

    overlap = expected_words.intersection(predicted_words)
    return len(overlap) / len(expected_words)

def length_ratio(example, pred, trace=None):
    """Score based on answer length appropriateness."""
    expected_len = len(example.answer)
    predicted_len = len(pred.answer)

    if expected_len == 0:
        return 0.0

    ratio = min(predicted_len, expected_len) / max(predicted_len, expected_len)
    return ratio
</code></pre>
<h3 id="domain-specific-metrics"><a class="header" href="#domain-specific-metrics">Domain-Specific Metrics</a></h3>
<p>Metrics tailored to your application:</p>
<pre><code class="language-python"># Medical diagnosis accuracy
def diagnosis_metric(example, pred, trace=None):
    """Evaluate medical diagnosis predictions."""
    # Primary diagnosis must match
    primary_correct = example.primary_diagnosis == pred.primary_diagnosis

    # Check if any differential diagnosis is correct
    differential_overlap = any(
        d in example.differential_diagnoses
        for d in pred.differential_diagnoses
    )

    # Urgency assessment
    urgency_correct = example.urgency_level == pred.urgency_level

    # Weighted combination
    score = (
        0.5 * primary_correct +
        0.3 * differential_overlap +
        0.2 * urgency_correct
    )

    return score

# Code generation correctness
def code_correctness(example, pred, trace=None):
    """Evaluate generated code."""
    try:
        # Try to execute the generated code
        exec(pred.code)

        # Check if output matches expected
        # (In practice, you'd capture and compare output)
        return True
    except Exception:
        return False

# Entity extraction F1
def entity_f1(example, pred, trace=None):
    """Calculate F1 score for entity extraction."""
    expected = set(example.entities)
    predicted = set(pred.entities)

    if not expected and not predicted:
        return 1.0
    if not expected or not predicted:
        return 0.0

    true_positives = len(expected &amp; predicted)
    precision = true_positives / len(predicted) if predicted else 0
    recall = true_positives / len(expected) if expected else 0

    if precision + recall == 0:
        return 0.0

    f1 = 2 * (precision * recall) / (precision + recall)
    return f1
</code></pre>
<h2 id="composite-metrics"><a class="header" href="#composite-metrics">Composite Metrics</a></h2>
<p>Combine multiple quality dimensions:</p>
<h3 id="weighted-combination"><a class="header" href="#weighted-combination">Weighted Combination</a></h3>
<pre><code class="language-python">def comprehensive_qa_metric(example, pred, trace=None):
    """
    Comprehensive QA evaluation combining multiple factors.
    """
    # 1. Answer correctness (most important)
    correct = example.answer.lower() in pred.answer.lower()
    correctness_score = 1.0 if correct else 0.0

    # 2. Answer completeness
    expected_len = len(example.answer)
    predicted_len = len(pred.answer)
    completeness = min(1.0, predicted_len / max(expected_len, 1))

    # 3. Relevance (answer mentions key terms from question)
    question_words = set(example.question.lower().split())
    answer_words = set(pred.answer.lower().split())
    relevance = len(question_words &amp; answer_words) / max(len(question_words), 1)

    # 4. Confidence (if available)
    confidence_score = getattr(pred, 'confidence', 0.5)

    # Weighted combination
    final_score = (
        0.5 * correctness_score +
        0.2 * completeness +
        0.2 * relevance +
        0.1 * confidence_score
    )

    # For optimization, require high threshold
    if trace is not None:
        return final_score &gt;= 0.8

    return final_score
</code></pre>
<h3 id="checklist-based-metrics"><a class="header" href="#checklist-based-metrics">Checklist-Based Metrics</a></h3>
<pre><code class="language-python">def quality_checklist(example, pred, trace=None):
    """
    Evaluate against a quality checklist.
    """
    checks = {
        "has_answer": len(pred.answer.strip()) &gt; 0,
        "not_too_short": len(pred.answer) &gt;= 10,
        "not_too_long": len(pred.answer) &lt;= 500,
        "contains_expected": example.expected_keyword in pred.answer.lower(),
        "no_apology": "sorry" not in pred.answer.lower(),
        "no_uncertainty": "i don't know" not in pred.answer.lower(),
    }

    # Count passed checks
    passed = sum(checks.values())
    total = len(checks)

    if trace is not None:
        # For optimization, all checks must pass
        return passed == total

    # For evaluation, return ratio of passed checks
    return passed / total
</code></pre>
<h3 id="multi-aspect-metrics"><a class="header" href="#multi-aspect-metrics">Multi-Aspect Metrics</a></h3>
<pre><code class="language-python">def multi_aspect_metric(example, pred, trace=None):
    """
    Return detailed scores for multiple aspects.
    During evaluation, returns overall score.
    Can also be used for detailed analysis.
    """
    scores = {
        "accuracy": calculate_accuracy(example, pred),
        "fluency": calculate_fluency(pred.answer),
        "relevance": calculate_relevance(example.question, pred.answer),
        "safety": calculate_safety(pred.answer),
    }

    # Overall score (weighted average)
    weights = {"accuracy": 0.4, "fluency": 0.2, "relevance": 0.3, "safety": 0.1}
    overall = sum(scores[k] * weights[k] for k in scores)

    if trace is not None:
        return overall &gt;= 0.7

    return overall


# Helper functions
def calculate_accuracy(example, pred):
    return 1.0 if example.answer.lower() in pred.answer.lower() else 0.0

def calculate_fluency(text):
    # Simple fluency check (could use language model)
    words = text.split()
    if len(words) &lt; 3:
        return 0.5
    return 1.0

def calculate_relevance(question, answer):
    q_words = set(question.lower().split())
    a_words = set(answer.lower().split())
    overlap = len(q_words &amp; a_words)
    return min(1.0, overlap / max(len(q_words), 1))

def calculate_safety(text):
    unsafe_terms = ["harmful", "dangerous", "illegal"]
    return 0.0 if any(term in text.lower() for term in unsafe_terms) else 1.0
</code></pre>
<h2 id="the-trace-parameter-deep-dive"><a class="header" href="#the-trace-parameter-deep-dive">The Trace Parameter Deep Dive</a></h2>
<p>The <code>trace</code> parameter enables different behavior during optimization vs. evaluation:</p>
<h3 id="why-trace-matters"><a class="header" href="#why-trace-matters">Why Trace Matters</a></h3>
<pre><code class="language-python"># During optimization (trace is not None)
# - DSPy is looking for good examples to bootstrap
# - Metric should return boolean (True = good example)
# - Be stricter to get high-quality demonstrations

# During evaluation (trace is None)
# - You want to measure actual performance
# - Metric should return actual score (float or bool)
# - Be accurate, not strict
</code></pre>
<h3 id="trace-aware-metric-pattern"><a class="header" href="#trace-aware-metric-pattern">Trace-Aware Metric Pattern</a></h3>
<pre><code class="language-python">def smart_metric(example, pred, trace=None):
    """
    Metric that behaves differently during optimization vs evaluation.
    """
    # Calculate detailed score
    exact = example.answer.lower() == pred.answer.lower()
    partial = example.answer.lower() in pred.answer.lower()
    length_ok = 0.5 &lt;= len(pred.answer) / len(example.answer) &lt;= 2.0

    if trace is not None:
        # OPTIMIZATION MODE
        # Be strict - only accept perfect examples
        # These will be used as demonstrations
        return exact and length_ok

    # EVALUATION MODE
    # Return nuanced score
    if exact:
        return 1.0
    elif partial and length_ok:
        return 0.7
    elif partial:
        return 0.5
    else:
        return 0.0
</code></pre>
<h3 id="using-trace-for-debugging"><a class="header" href="#using-trace-for-debugging">Using Trace for Debugging</a></h3>
<pre><code class="language-python">def debugging_metric(example, pred, trace=None):
    """
    Metric that logs information when tracing.
    """
    score = example.answer.lower() in pred.answer.lower()

    if trace is not None:
        # Log during optimization for debugging
        print(f"Expected: {example.answer}")
        print(f"Got: {pred.answer}")
        print(f"Score: {score}")
        print("---")

    return score
</code></pre>
<h2 id="common-metric-patterns"><a class="header" href="#common-metric-patterns">Common Metric Patterns</a></h2>
<h3 id="pattern-1-exact-match-with-normalization"><a class="header" href="#pattern-1-exact-match-with-normalization">Pattern 1: Exact Match with Normalization</a></h3>
<pre><code class="language-python">def normalized_exact_match(example, pred, trace=None):
    """Exact match after normalization."""
    def normalize(text):
        return text.lower().strip().replace(".", "").replace(",", "")

    return normalize(example.answer) == normalize(pred.answer)
</code></pre>
<h3 id="pattern-2-contains-expected"><a class="header" href="#pattern-2-contains-expected">Pattern 2: Contains Expected</a></h3>
<pre><code class="language-python">def contains_expected(example, pred, trace=None):
    """Check if prediction contains the expected answer."""
    expected = example.answer.lower()
    predicted = pred.answer.lower()
    return expected in predicted
</code></pre>
<h3 id="pattern-3-any-of-multiple-correct-answers"><a class="header" href="#pattern-3-any-of-multiple-correct-answers">Pattern 3: Any of Multiple Correct Answers</a></h3>
<pre><code class="language-python">def any_correct(example, pred, trace=None):
    """Accept any of multiple correct answers."""
    # example.answers is a list of acceptable answers
    predicted = pred.answer.lower().strip()
    return any(
        ans.lower().strip() in predicted
        for ans in example.answers
    )
</code></pre>
<h3 id="pattern-4-threshold-based"><a class="header" href="#pattern-4-threshold-based">Pattern 4: Threshold-Based</a></h3>
<pre><code class="language-python">def threshold_metric(example, pred, trace=None, threshold=0.8):
    """Apply threshold to continuous score."""
    # Calculate similarity score
    score = calculate_similarity(example.answer, pred.answer)

    if trace is not None:
        return score &gt;= threshold

    return score
</code></pre>
<h3 id="pattern-5-multi-field-match"><a class="header" href="#pattern-5-multi-field-match">Pattern 5: Multi-Field Match</a></h3>
<pre><code class="language-python">def multi_field_metric(example, pred, trace=None):
    """Evaluate multiple output fields."""
    scores = []

    # Check each output field
    if hasattr(example, 'sentiment'):
        scores.append(example.sentiment == pred.sentiment)

    if hasattr(example, 'category'):
        scores.append(example.category == pred.category)

    if hasattr(example, 'confidence'):
        scores.append(abs(example.confidence - pred.confidence) &lt; 0.1)

    if not scores:
        return 0.0

    return sum(scores) / len(scores)
</code></pre>
<h2 id="metric-design-best-practices"><a class="header" href="#metric-design-best-practices">Metric Design Best Practices</a></h2>
<h3 id="1-capture-what-actually-matters"><a class="header" href="#1-capture-what-actually-matters">1. Capture What Actually Matters</a></h3>
<pre><code class="language-python"># BAD: Metric that doesn't capture real quality
def bad_metric(example, pred, trace=None):
    return len(pred.answer) &gt; 10  # Length doesn't mean quality!

# GOOD: Metric that captures task-specific quality
def good_metric(example, pred, trace=None):
    return (
        example.key_fact in pred.answer and
        pred.answer.endswith(".") and  # Complete sentence
        len(pred.answer.split()) &gt;= 5   # Substantive answer
    )
</code></pre>
<h3 id="2-be-robust-to-formatting"><a class="header" href="#2-be-robust-to-formatting">2. Be Robust to Formatting</a></h3>
<pre><code class="language-python">def robust_metric(example, pred, trace=None):
    """Handle formatting variations."""
    def clean(text):
        return " ".join(text.lower().split())

    return clean(example.answer) == clean(pred.answer)
</code></pre>
<h3 id="3-handle-edge-cases"><a class="header" href="#3-handle-edge-cases">3. Handle Edge Cases</a></h3>
<pre><code class="language-python">def safe_metric(example, pred, trace=None):
    """Handle missing or empty values."""
    expected = getattr(example, 'answer', '')
    predicted = getattr(pred, 'answer', '')

    if not expected or not predicted:
        return 0.0

    return expected.lower() in predicted.lower()
</code></pre>
<h3 id="4-make-metrics-interpretable"><a class="header" href="#4-make-metrics-interpretable">4. Make Metrics Interpretable</a></h3>
<pre><code class="language-python">def interpretable_metric(example, pred, trace=None):
    """Return score with clear meaning."""
    checks = {
        "correct": example.answer.lower() in pred.answer.lower(),
        "complete": len(pred.answer) &gt;= 50,
        "relevant": any(word in pred.answer.lower()
                       for word in example.question.lower().split()),
    }

    # Log which checks failed (useful for debugging)
    failed = [k for k, v in checks.items() if not v]
    if failed and trace is None:  # Only log during evaluation
        print(f"Failed checks: {failed}")

    return sum(checks.values()) / len(checks)
</code></pre>
<h2 id="specialized-metrics-for-long-form-content-generation"><a class="header" href="#specialized-metrics-for-long-form-content-generation">Specialized Metrics for Long-form Content Generation</a></h2>
<p>When evaluating long-form articles like Wikipedia entries, we need specialized metrics that go beyond simple answer correctness. These metrics assess comprehensiveness, factual accuracy, and verifiability.</p>
<h3 id="topic-coverage-evaluation"><a class="header" href="#topic-coverage-evaluation">Topic Coverage Evaluation</a></h3>
<p>Measures how comprehensively the generated content covers the topic:</p>
<pre><code class="language-python">def topic_coverage_rouge(example, pred, trace=None):
    """
    Evaluate topic coverage using ROUGE metrics against reference articles.

    ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures
    overlap between generated and reference content.
    """
    try:
        from rouge_score import rouge_scorer
    except ImportError:
        print("Install rouge_score: pip install rouge-score")
        return 0.0

    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

    # Score against reference content
    scores = scorer.score(
        example.reference_content,
        pred.article_content
    )

    # Use ROUGE-L as primary metric (measures longest common subsequence)
    rouge_l_score = scores['rougeL'].fmeasure

    if trace is not None:
        # During optimization, require good coverage
        return rouge_l_score &gt;= 0.4

    return rouge_l_score

def comprehensive_topic_coverage(example, pred, trace=None):
    """
    More comprehensive topic coverage evaluation.

    Checks coverage of multiple aspects:
    1. Key entities mentioned
    2. Important concepts covered
    3. Topic depth across sections
    """
    # Extract key entities from reference
    reference_entities = set(example.get('key_entities', []))
    generated_text = pred.article_content.lower()

    # Check entity coverage
    entities_covered = sum(
        1 for entity in reference_entities
        if entity.lower() in generated_text
    )
    entity_coverage = entities_covered / len(reference_entities) if reference_entities else 0

    # Check section coverage (if outline provided)
    if hasattr(pred, 'outline') and pred.outline:
        expected_sections = set(example.get('required_sections', []))
        generated_sections = set(s['title'].lower() for s in pred.outline)

        section_coverage = len(expected_sections &amp; generated_sections) / len(expected_sections)
    else:
        section_coverage = 0.5  # Default if no outline

    # Check concept coverage
    reference_concepts = set(example.get('key_concepts', []))
    concepts_covered = sum(
        1 for concept in reference_concepts
        if concept.lower() in generated_text
    )
    concept_coverage = concepts_covered / len(reference_concepts) if reference_concepts else 0

    # Weighted combination
    overall_coverage = (
        0.4 * entity_coverage +
        0.4 * concept_coverage +
        0.2 * section_coverage
    )

    if trace is not None:
        return overall_coverage &gt;= 0.6

    return overall_coverage
</code></pre>
<h3 id="factual-accuracy-factscore"><a class="header" href="#factual-accuracy-factscore">Factual Accuracy (FactScore)</a></h3>
<p>FactScore is a metric specifically designed to evaluate factual accuracy in long-form generation:</p>
<pre><code class="language-python">class FactScoreMetric:
    """
    FactScore: Evaluates factual accuracy by breaking down content into
    atomic claims and verifying each against a knowledge source.
    """

    def __init__(self, model_name="gpt-3.5-turbo"):
        self.model_name = model_name
        self.claim_extractor = dspy.Predict(
            "text -&gt; atomic_claims"
        )
        self.fact_checker = dspy.ChainOfThought(
            "claim, context -&gt; is_factual, confidence, correction"
        )

    def __call__(self, example, pred, trace=None):
        """
        Calculate FactScore for generated content.

        Returns the average of factual claim scores.
        """
        # Extract atomic claims from generated content
        claims_result = self.claim_extractor(
            text=pred.article_content
        )
        claims = self._parse_claims(claims_result.atomic_claims)

        if not claims:
            return 0.0

        # Verify each claim
        claim_scores = []
        for claim in claims:
            verification = self.fact_checker(
                claim=claim,
                context=example.get('reference_documents', '')
            )

            # Convert confidence to score
            if verification.is_factual.lower() == 'true':
                score = float(verification.confidence)
            else:
                score = 0.0

            claim_scores.append(score)

        # Calculate FactScore (average of verified claims)
        fact_score = sum(claim_scores) / len(claim_scores)

        if trace is not None:
            # During optimization, require high factual accuracy
            return fact_score &gt;= 0.7

        return fact_score

    def _parse_claims(self, claims_text: str) -&gt; List[str]:
        """Parse atomic claims from extracted text."""
        claims = []
        lines = claims_text.strip().split('\n')
        for line in lines:
            if line.strip() and (line.strip().startswith('-') or line.strip().startswith('•')):
                claim = line.strip().lstrip('- •').strip()
                if claim.endswith('.'):
                    claim = claim[:-1]
                claims.append(claim)
        return claims

# Usage
fact_scorer = FactScoreMetric()
def factscore_metric(example, pred, trace=None):
    """Wrapper for FactScore metric."""
    return fact_scorer(example, pred, trace)
</code></pre>
<h3 id="verifiability-assessment"><a class="header" href="#verifiability-assessment">Verifiability Assessment</a></h3>
<p>Measures how well claims in the generated content can be verified with citations:</p>
<pre><code class="language-python">def verifiability_metric(example, pred, trace=None):
    """
    Measures the fraction of sentences that can be verified
    using retrieved evidence or citations.
    """
    sentences = _split_into_sentences(pred.article_content)

    if not sentences:
        return 0.0

    verifiable_count = 0

    for sentence in sentences:
        # Check if sentence has citation
        has_citation = bool(re.search(r'\[\d+\]|\[.*?\]', sentence))

        # Check if sentence is factual claim
        is_factual = _is_factual_claim(sentence)

        # Check if supporting evidence exists
        if hasattr(pred, 'citations') and pred.citations:
            has_evidence = _check_evidence_support(
                sentence,
                pred.citations,
                example.get('reference_documents', '')
            )
        else:
            has_evidence = False

        # Sentence is verifiable if it has citation OR supporting evidence
        if is_factual and (has_citation or has_evidence):
            verifiable_count += 1

    verifiability = verifiable_count / len(sentences)

    if trace is not None:
        # During optimization, require high verifiability
        return verifiability &gt;= 0.6

    return verifiability

def _split_into_sentences(text: str) -&gt; List[str]:
    """Split text into sentences."""
    import re
    # Simple sentence splitting
    sentences = re.split(r'[.!?]+', text)
    return [s.strip() for s in sentences if s.strip()]

def _is_factual_claim(sentence: str) -&gt; bool:
    """Determine if a sentence makes a factual claim."""
    factual_indicators = [
        'according to', 'research shows', 'studies indicate',
        'data suggests', 'reported', 'found that', 'demonstrates',
        'proved', 'discovered', 'measured', 'calculated'
    ]

    # Check for numbers (statistics)
    has_numbers = bool(re.search(r'\d+', sentence))

    # Check for factual indicators
    has_indicators = any(ind in sentence.lower() for ind in factual_indicators)

    # Check for specific entities (often indicates facts)
    has_entities = bool(re.search(r'[A-Z][a-z]+ [A-Z][a-z]+', sentence))

    return has_numbers or has_indicators or has_entities

def _check_evidence_support(sentence: str,
                           citations: List[str],
                           reference_docs: str) -&gt; bool:
    """Check if sentence has supporting evidence in references."""
    # Simple check - in practice would use semantic similarity
    sentence_words = set(sentence.lower().split())

    for citation in citations:
        # Extract cited content (simplified)
        cited_content = _extract_citation_content(citation, reference_docs)

        if cited_content:
            cited_words = set(cited_content.lower().split())
            overlap = len(sentence_words &amp; cited_words) / len(sentence_words)

            if overlap &gt; 0.3:  # 30% overlap threshold
                return True

    return False

def _extract_citation_content(citation: str, reference_docs: str) -&gt; str:
    """Extract content for a specific citation."""
    # Simplified - would need proper citation parsing
    if citation in reference_docs:
        return reference_docs.split(citation)[1].split('\n')[0]
    return ""
</code></pre>
<h3 id="citation-quality-metrics"><a class="header" href="#citation-quality-metrics">Citation Quality Metrics</a></h3>
<pre><code class="language-python">def citation_quality_metric(example, pred, trace=None):
    """
    Evaluates the quality and appropriateness of citations in the article.
    """
    if not hasattr(pred, 'citations') or not pred.citations:
        return 0.0

    total_score = 0.0

    for citation in pred.citations:
        # Check citation format
        format_score = _check_citation_format(citation)

        # Check citation relevance
        relevance_score = _check_citation_relevance(
            citation,
            pred.article_content,
            example.get('reference_documents', '')
        )

        # Check source credibility (if available)
        credibility_score = _check_source_credibility(citation)

        # Combine scores
        citation_score = (
            0.3 * format_score +
            0.5 * relevance_score +
            0.2 * credibility_score
        )

        total_score += citation_score

    average_score = total_score / len(pred.citations)

    if trace is not None:
        return average_score &gt;= 0.7

    return average_score

def _check_citation_format(citation: str) -&gt; float:
    """Check if citation follows expected format."""
    # Check for common citation formats
    patterns = [
        r'\[\d+\]',  # Numeric [1]
        r'\([A-Z][a-z]+, \d{4}\)',  # APA (Smith, 2023)
        r'\([A-Z][a-z]+ et al\., \d{4}\)',  # APA et al.
    ]

    for pattern in patterns:
        if re.search(pattern, citation):
            return 1.0

    return 0.5  # Partial score for unrecognized format

def _check_citation_relevance(citation: str,
                             content: str,
                             references: str) -&gt; float:
    """Check how relevant the citation is to the content."""
    # Simplified - would use semantic similarity in practice
    citation_text = _extract_citation_text(citation, references)

    if not citation_text:
        return 0.0

    # Find where citation is used in content
    citation_context = _find_citation_context(citation, content)

    if not citation_context:
        return 0.0

    # Calculate word overlap
    context_words = set(citation_context.lower().split())
    citation_words = set(citation_text.lower().split())

    overlap = len(context_words &amp; citation_words)
    return min(1.0, overlap / 10)  # Normalize by expected overlap

def _check_source_credibility(citation: str) -&gt; float:
    """Check the credibility of the cited source."""
    # List of credible sources (simplified)
    credible_domains = [
        'nature.com', 'science.org', 'cell.com',
        'arxiv.org', 'scholar.google.com',
        'gov', 'edu', 'ieee.org', 'acm.org'
    ]

    # Extract domain if URL is present
    if 'http' in citation:
        from urllib.parse import urlparse
        try:
            domain = urlparse(citation).netloc
            if any(cred in domain for cred in credible_domains):
                return 1.0
            return 0.5  # Partial for other domains
        except:
            return 0.5

    # For non-URL citations, assume academic source
    return 0.8
</code></pre>
<h3 id="composite-long-form-quality-metric"><a class="header" href="#composite-long-form-quality-metric">Composite Long-form Quality Metric</a></h3>
<pre><code class="language-python">def longform_composite_metric(example, pred, trace=None):
    """
    Composite metric for evaluating long-form article quality.

    Combines multiple aspects:
    - Topic coverage (ROUGE)
    - Factual accuracy (FactScore)
    - Verifiability
    - Citation quality
    - Coherence and flow
    """
    # Individual component scores
    coverage_score = topic_coverage_rouge(example, pred, trace)
    factual_score = factscore_metric(example, pred, trace)
    verifiability_score = verifiability_metric(example, pred, trace)
    citation_score = citation_quality_metric(example, pred, trace)

    # Coherence score (simplified)
    coherence_score = _evaluate_coherence(pred.article_content)

    # Weighted combination for final score
    final_score = (
        0.25 * coverage_score +
        0.30 * factual_score +
        0.20 * verifiability_score +
        0.15 * citation_score +
        0.10 * coherence_score
    )

    if trace is not None:
        # During optimization, require good overall quality
        return final_score &gt;= 0.6

    return final_score

def _evaluate_coherence(text: str) -&gt; float:
    """Evaluate text coherence and flow."""
    sentences = _split_into_sentences(text)

    if len(sentences) &lt; 2:
        return 1.0

    coherence_scores = []

    # Check transitions between consecutive sentences
    for i in range(len(sentences) - 1):
        current = sentences[i]
        next_sent = sentences[i + 1]

        # Check for transition words
        transitions = ['however', 'therefore', 'furthermore', 'consequently',
                      'moreover', 'in addition', 'in contrast', 'similarly']

        has_transition = any(trans in next_sent.lower() for trans in transitions)

        # Check for pronoun reference to previous sentence
        current_words = set(current.lower().split())
        next_words = set(next_sent.lower().split())

        # Common coherence indicators
        pronouns = {'it', 'they', 'this', 'that', 'these', 'those'}
        pronoun_reference = bool(pronouns &amp; next_words)

        # Topic continuity
        topic_overlap = len(current_words &amp; next_words) / len(current_words | next_words)

        # Score for this transition
        transition_score = (
            0.4 * (1.0 if has_transition else 0.0) +
            0.3 * (1.0 if pronoun_reference else 0.0) +
            0.3 * topic_overlap
        )

        coherence_scores.append(transition_score)

    # Average coherence across all transitions
    return sum(coherence_scores) / len(coherence_scores)
</code></pre>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>Effective metrics are the key to meaningful evaluation:</p>
<ol>
<li><strong>Understand the anatomy</strong>: example, pred, trace parameters</li>
<li><strong>Use built-in metrics</strong> when appropriate (SemanticF1, etc.)</li>
<li><strong>Create custom metrics</strong> for domain-specific needs</li>
<li><strong>Combine multiple aspects</strong> with composite metrics</li>
<li><strong>Use trace appropriately</strong> for optimization vs. evaluation</li>
<li><strong>Employ specialized metrics</strong> for long-form content (ROUGE, FactScore, Verifiability)</li>
</ol>
<h3 id="key-takeaways"><a class="header" href="#key-takeaways">Key Takeaways</a></h3>
<ol>
<li><strong>Metrics define success</strong> - They determine what optimization improves</li>
<li><strong>The trace parameter</strong> enables optimization-aware behavior</li>
<li><strong>Custom metrics</strong> capture domain-specific quality</li>
<li><strong>Composite metrics</strong> address multiple dimensions</li>
<li><strong>Robustness matters</strong> - Handle edge cases gracefully</li>
<li><strong>Long-form content requires specialized evaluation</strong> beyond simple accuracy</li>
</ol>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<ul>
<li><a href="./04-evaluation-loops.html">Next Section: Evaluation Loops</a> - Run systematic evaluations</li>
<li><a href="./05-best-practices.html">Best Practices</a> - Avoid common pitfalls</li>
<li><a href="../../examples/chapter04/">Examples</a> - See metrics in action</li>
</ul>
<h2 id="further-reading"><a class="header" href="#further-reading">Further Reading</a></h2>
<ul>
<li><a href="https://dspy.ai/learn/evaluation/metrics">DSPy Metrics Documentation</a></li>
<li><a href="https://huggingface.co/spaces/evaluate-metric">Evaluation Metrics for NLP</a></li>
<li><a href="https://scikit-learn.org/stable/modules/model_evaluation.html">Custom Metrics in Machine Learning</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../04-evaluation/02-creating-datasets.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../04-evaluation/04-evaluation-loops.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../04-evaluation/02-creating-datasets.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../04-evaluation/04-evaluation-loops.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>






        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
