<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>LLM-as-a-Judge - DSPy: A Practical Guide</title>


        <!-- Custom HTML head -->

        <meta name="description" content="The most comprehensive DSPy guide with complete coverage of 9 research papers, advanced optimization techniques, and production-ready applications">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../assets/print-only-ef201963.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-4ea68664.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">DSPy: A Practical Guide</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="llm-as-a-judge-for-context-sensitive-evaluation"><a class="header" href="#llm-as-a-judge-for-context-sensitive-evaluation">LLM-as-a-Judge for Context-Sensitive Evaluation</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p><strong>LLM-as-a-Judge</strong> is a powerful evaluation paradigm that uses large language models to assess the quality and impact of model outputs. This approach is particularly valuable when traditional metrics fail to capture domain-specific nuances or real-world consequences.</p>
<p>This framework becomes essential in safety-critical domains like healthcare, where standard metrics such as Word Error Rate (WER) for Automatic Speech Recognition (ASR) correlate poorly with actual clinical risk. The approach demonstrates how LLMs can be trained to perform nuanced, context-aware evaluations that align with expert human judgment.</p>
<h2 id="when-to-use-llm-as-a-judge"><a class="header" href="#when-to-use-llm-as-a-judge">When to Use LLM-as-a-Judge</a></h2>
<h3 id="1-domain-specific-impact-assessment"><a class="header" href="#1-domain-specific-impact-assessment">1. Domain-Specific Impact Assessment</a></h3>
<pre><code class="language-python"># Standard metrics (WER, BLEU) fail to capture clinical meaning
standard_metrics = {
    "wer": 0.12,  # Low error rate
    "bleu": 0.85,  # High overlap
    # But missed critical negation: "no chest pain" → "chest pain"
}

# LLM-as-a-Judge captures actual impact
clinical_judge = ClinicalImpactJudge()
assessment = clinical_judge.evaluate(
    ground_truth="Patient reports no chest pain or shortness of breath",
    hypothesis="Patient reports chest pain or shortness of breath"
)
# Result: SIGNIFICANT_CLINICAL_IMPACT (2/2)
</code></pre>
<h3 id="2-nuanced-semantic-evaluation"><a class="header" href="#2-nuanced-semantic-evaluation">2. Nuanced Semantic Evaluation</a></h3>
<p>Traditional metrics struggle with:</p>
<ul>
<li>Context-dependent meaning</li>
<li>Domain-specific terminology</li>
<li>Weighted importance of different errors</li>
<li>Complex relationships between concepts</li>
</ul>
<h3 id="3-multi-dimensional-quality-assessment"><a class="header" href="#3-multi-dimensional-quality-assessment">3. Multi-Dimensional Quality Assessment</a></h3>
<pre><code class="language-python">class MultiDimensionalJudge(dspy.Module):
    """Evaluates outputs across multiple quality dimensions."""

    def __init__(self, dimensions: List[str]):
        super().__init__()
        self.dimensions = dimensions
        self.judge = dspy.ChainOfThought(
            """Evaluate the {output} against {ground_truth}.

            Consider these dimensions:
            {dimensions}

            For each dimension, provide:
            - Score (1-5)
            - Justification
            - Impact severity"""
        )

    def forward(self, output: str, ground_truth: str):
        evaluation = self.judge(
            output=output,
            ground_truth=ground_truth,
            dimensions=", ".join(self.dimensions)
        )
        return evaluation
</code></pre>
<h2 id="implementation-framework"><a class="header" href="#implementation-framework">Implementation Framework</a></h2>
<h3 id="1-core-judge-architecture"><a class="header" href="#1-core-judge-architecture">1. Core Judge Architecture</a></h3>
<pre><code class="language-python">import dspy
from typing import Dict, List, Tuple, Optional

class LLMJudge(dspy.Module):
    """Base class for LLM-as-a-Judge implementations."""

    def __init__(self,
                 prompt_template: str,
                 output_schema: type,
                 max_tokens: int = 1000):
        super().__init__()
        self.prompt_template = prompt_template
        self.output_schema = output_schema
        self.max_tokens = max_tokens

        # Initialize the judge with Chain of Thought for reasoning
        self.judge = dspy.ChainOfThought(
            self.prompt_template,
            max_tokens=self.max_tokens
        )

    def evaluate(self, ground_truth: str, hypothesis: str, **context) -&gt; Dict:
        """Evaluate hypothesis against ground truth."""
        # Format the prompt with inputs
        prompt = self.prompt_template.format(
            ground_truth=ground_truth,
            hypothesis=hypothesis,
            **context
        )

        # Get LLM evaluation
        result = self.judge(
            ground_truth=ground_truth,
            hypothesis=hypothesis,
            **context
        )

        # Parse and validate output
        try:
            return self.parse_output(result)
        except Exception as e:
            return {
                "error": str(e),
                "raw_output": str(result),
                "evaluation": "PARSING_ERROR"
            }

    def parse_output(self, raw_output) -&gt; Dict:
        """Parse LLM output into structured format."""
        # Implementation depends on output_schema
        # This is a generic implementation
        if hasattr(raw_output, 'reasoning'):
            return {
                "reasoning": raw_output.reasoning,
                "evaluation": getattr(raw_output, 'evaluation', None),
                "confidence": getattr(raw_output, 'confidence', 0.0)
            }
        return {"raw_output": str(raw_output)}
</code></pre>
<h3 id="2-clinical-impact-judge"><a class="header" href="#2-clinical-impact-judge">2. Clinical Impact Judge</a></h3>
<pre><code class="language-python">class ClinicalImpactJudge(LLMJudge):
    """Judge for assessing clinical impact of ASR errors."""

    # Define impact levels
    IMPACT_LEVELS = {
        0: "No Clinical Impact",
        1: "Minimal Clinical Impact",
        2: "Significant Clinical Impact"
    }

    def __init__(self):
        prompt_template = """
        You are an expert medical analyst. Your task is to assess the clinical impact
        of errors in an AI-generated transcription of a medical conversation.

        You will be given:
        1. ground_truth_conversation: The accurate, human-verified transcript
        2. transcription_conversation: The machine-generated transcript with errors

        Core Principle: Determine if a clinician reading the transcription would
        make different medical decisions than if they read the ground truth.

        Provide:
        1. reasoning: Step-by-step analysis of differences
        2. clinical_impact: Single integer (0, 1, or 2)

        Impact Levels:
        - 0: No Clinical Impact (cosmetic errors only)
        - 1: Minimal Clinical Impact (non-critical ambiguities)
        - 2: Significant Clinical Impact (could affect diagnosis/treatment)

        Ground Truth: {ground_truth}
        Transcription: {hypothesis}
        Context: {context}
        """

        super().__init__(
            prompt_template=prompt_template,
            output_schema=dict
        )

    def evaluate(self, ground_truth: str, hypothesis: str,
                  context: Optional[str] = None) -&gt; Dict:
        """Evaluate clinical impact with structured output."""
        result = super().evaluate(
            ground_truth=ground_truth,
            hypothesis=hypothesis,
            context=context or "No additional context"
        )

        # Normalize the impact level
        if 'clinical_impact' in result:
            try:
                impact = int(result['clinical_impact'])
                result['clinical_impact'] = min(max(impact, 0), 2)
                result['impact_label'] = self.IMPACT_LEVELS[result['clinical_impact']]
            except:
                result['clinical_impact'] = -1
                result['impact_label'] = "UNKNOWN"

        return result
</code></pre>
<h3 id="3-specialized-judges-for-different-domains"><a class="header" href="#3-specialized-judges-for-different-domains">3. Specialized Judges for Different Domains</a></h3>
<pre><code class="language-python">class CodeQualityJudge(LLMJudge):
    """Judge for evaluating code quality and correctness."""

    def __init__(self):
        prompt_template = """
        Evaluate the generated code against the reference implementation.

        Consider:
        - Correctness: Does it produce the right output?
        - Efficiency: Is it optimal in time/space complexity?
        - Readability: Is it clean and maintainable?
        - Edge Cases: Does it handle unusual inputs?

        Provide scores (1-5) for each dimension and overall assessment.

        Reference: {ground_truth}
        Generated: {hypothesis}
        """

        super().__init__(prompt_template, dict)

class CreativeWritingJudge(LLMJudge):
    """Judge for evaluating creative writing quality."""

    def __init__(self):
        prompt_template = """
        Evaluate the creative writing piece against criteria:

        - Creativity and originality
        - Engagement and flow
        - Character development (if applicable)
        - Plot coherence
        - Language quality

        Reference Piece: {ground_truth}
        Generated Piece: {hypothesis}
        Writing Style: {style}
        """

        super().__init__(prompt_template, dict)

class FactualAccuracyJudge(LLMJudge):
    """Judge for checking factual accuracy in generated text."""

    def __init__(self):
        prompt_template = """
        Fact-check the generated text against verified information.

        For each factual claim:
        - Is it accurate?
        - Is it properly attributed?
        - Is any important context missing?

        Flag any hallucinations or misstatements.

        Verified Information: {ground_truth}
        Generated Text: {hypothesis}
        """

        super().__init__(prompt_template, dict)
</code></pre>
<h2 id="training-and-optimization"><a class="header" href="#training-and-optimization">Training and Optimization</a></h2>
<h3 id="1-using-gepa-for-prompt-optimization"><a class="header" href="#1-using-gepa-for-prompt-optimization">1. Using GEPA for Prompt Optimization</a></h3>
<pre><code class="language-python">from gepa import GEPAOptimizer

class OptimizedJudge:
    """Train LLM judge using GEPA for prompt optimization."""

    def __init__(self, base_judge_class, training_data: List[Dict]):
        self.base_judge_class = base_judge_class
        self.training_data = training_data
        self.optimized_prompt = None
        self.trained_judge = None

    def optimize_prompt(self, num_iterations: int = 10):
        """Optimize the judge's prompt using GEPA."""

        # Initialize GEPA optimizer
        optimizer = GEPAOptimizer(
            population_size=10,
            generations=num_iterations,
            objectives=["accuracy", "robustness"],
            reflection_model="gpt-4"
        )

        # Define initial prompt
        initial_prompt = self.base_judge_class.__init__.__doc__

        # Create evaluation function
        def evaluate_prompt(prompt: str) -&gt; Dict:
            # Create temporary judge with new prompt
            temp_judge = self.base_judge_class()
            temp_judge.prompt_template = prompt

            # Evaluate on training data
            correct = 0
            total = len(self.training_data)

            for example in self.training_data:
                result = temp_judge.evaluate(**example)
                if result.get('evaluation') == example.get('expected'):
                    correct += 1

            return {
                "accuracy": correct / total,
                "robustness": self._calculate_robustness(temp_judge)
            }

        # Run optimization
        best_prompt = optimizer.compile(
            program=initial_prompt,
            trainset=self.training_data,
            evalset=self.training_data
        )

        self.optimized_prompt = best_prompt
        self.trained_judge = self.base_judge_class()
        self.trained_judge.prompt_template = self.optimized_prompt

    def _calculate_robustness(self, judge) -&gt; float:
        """Calculate robustness across diverse examples."""
        # Test on edge cases and variations
        edge_cases = self._generate_edge_cases()
        consistent_results = 0

        for case in edge_cases:
            result1 = judge.evaluate(**case)
            # Slight variation
            case_variant = self._add_noise(case)
            result2 = judge.evaluate(**case_variant)

            if self._results_consistent(result1, result2):
                consistent_results += 1

        return consistent_results / len(edge_cases)
</code></pre>
<h3 id="2-cost-sensitive-training"><a class="header" href="#2-cost-sensitive-training">2. Cost-Sensitive Training</a></h3>
<pre><code class="language-python">class CostSensitiveTraining:
    """Train judge with cost-sensitive loss function."""

    def __init__(self, cost_matrix: Dict[Tuple[int, int], float]):
        self.cost_matrix = cost_matrix
        # Example: cost_matrix[(true, pred)] = penalty

    def calculate_loss(self, predictions: List[int],
                       labels: List[int]) -&gt; float:
        """Calculate weighted loss based on cost matrix."""
        total_cost = 0.0

        for pred, true in zip(predictions, labels):
            cost = self.cost_matrix.get((true, pred), 0.0)
            total_cost += cost

        return total_cost / len(predictions)

# Example cost matrix for clinical impact
clinical_cost_matrix = {
    (0, 0): 1.2,   # Correctly identify no impact
    (0, 1): 0.3,   # Over-predict minimal impact
    (0, 2): -1.0,  # Over-predict significant impact
    (1, 0): 0.3,   # Under-predict minimal impact
    (1, 1): 1.5,   # Correctly identify minimal impact
    (1, 2): 0.5,   # Over-predict significant impact
    (2, 0): -1.2,  # Miss significant impact (worst)
    (2, 1): 0.4,   # Under-predict significance
    (2, 2): 1.5    # Correctly identify significant impact
}
</code></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="1-prompt-design"><a class="header" href="#1-prompt-design">1. Prompt Design</a></h3>
<pre><code class="language-python"># Good: Clear, structured prompts with explicit criteria
GOOD_PROMPT_TEMPLATE = """
You are evaluating [task_type] outputs.

Evaluation Criteria:
1. [Criterion 1]: [Clear definition]
2. [Criterion 2]: [Clear definition]
3. [Criterion 3]: [Clear definition]

For each criterion:
- Provide a score (1-5)
- Give specific justification
- Note any concerns

Output Format:
{
  "scores": {{
    "criterion_1": score,
    "criterion_2": score,
    "criterion_3": score
  }},
  "justifications": {{
    "criterion_1": "explanation",
    "criterion_2": "explanation",
    "criterion_3": "explanation"
  }},
  "overall_assessment": "summary",
  "confidence": 0.0-1.0
}
"""

# Bad: Vague, unstructured evaluation
BAD_PROMPT_TEMPLATE = """
Is this output good?
Output: {hypothesis}
Reference: {ground_truth}
"""
</code></pre>
<h3 id="2-handling-bias"><a class="header" href="#2-handling-bias">2. Handling Bias</a></h3>
<pre><code class="language-python">class UnbiasedJudge:
    """Judge with bias mitigation strategies."""

    def __init__(self, base_judge, bias_detectors: List[callable]):
        self.base_judge = base_judge
        self.bias_detectors = bias_detectors

    def evaluate(self, *args, **kwargs):
        # Get initial evaluation
        result = self.base_judge.evaluate(*args, **kwargs)

        # Check for various biases
        for detector in self.bias_detectors:
            bias_score = detector(result, *args, **kwargs)
            if bias_score &gt; 0.7:  # High bias detected
                result["bias_warning"] = f"High {detector.__name__} detected"
                result["bias_score"] = bias_score

        return result

def length_bias_detector(result, hypothesis, **kwargs):
    """Detect bias towards longer/shorter outputs."""
    if len(hypothesis) &gt; 500:
        return 0.8  # Likely favoring longer outputs
    return 0.1

def positivity_bias_detector(result, hypothesis, **kwargs):
    """Detect bias towards overly positive evaluations."""
    positive_words = ["excellent", "perfect", "outstanding"]
    count = sum(1 for word in positive_words if word in str(result).lower())
    if count &gt; 2:
        return 0.7
    return 0.1
</code></pre>
<h3 id="3-ensemble-of-judges"><a class="header" href="#3-ensemble-of-judges">3. Ensemble of Judges</a></h3>
<pre><code class="language-python">class JudgeEnsemble:
    """Combine multiple judges for more robust evaluation."""

    def __init__(self, judges: List[LLMJudge], weights: Optional[List[float]] = None):
        self.judges = judges
        self.weights = weights or [1.0] * len(judges)

    def evaluate(self, *args, **kwargs):
        """Get evaluations from all judges and combine."""
        evaluations = []

        for judge in self.judges:
            eval_result = judge.evaluate(*args, **kwargs)
            evaluations.append(eval_result)

        # Combine results
        combined = self._combine_evaluations(evaluations)

        # Calculate confidence based on agreement
        combined["agreement_score"] = self._calculate_agreement(evaluations)
        combined["individual_evaluations"] = evaluations

        return combined

    def _combine_evaluations(self, evaluations: List[Dict]) -&gt; Dict:
        """Combine multiple evaluation results."""
        # Simple averaging for numeric scores
        combined = {}

        if all('evaluation' in e for e in evaluations):
            # For classification tasks
            scores = [e['evaluation'] for e in evaluations]
            combined['evaluation'] = round(sum(scores) / len(scores))
            combined['vote_distribution'] = {
                score: scores.count(score) for score in set(scores)
            }

        return combined

    def _calculate_agreement(self, evaluations: List[Dict]) -&gt; float:
        """Calculate how much judges agree with each other."""
        if len(evaluations) &lt; 2:
            return 1.0

        agreements = 0
        total_comparisons = 0

        for i in range(len(evaluations)):
            for j in range(i + 1, len(evaluations)):
                if evaluations[i].get('evaluation') == evaluations[j].get('evaluation'):
                    agreements += 1
                total_comparisons += 1

        return agreements / total_comparisons if total_comparisons &gt; 0 else 0.0
</code></pre>
<h2 id="integration-with-dspy-evaluation"><a class="header" href="#integration-with-dspy-evaluation">Integration with DSPy Evaluation</a></h2>
<h3 id="1-custom-metrics"><a class="header" href="#1-custom-metrics">1. Custom Metrics</a></h3>
<pre><code class="language-python">class LLMJudgeMetric(dspy.Metric):
    """DSPy metric that uses LLM judge for evaluation."""

    def __init__(self, judge: LLMJudge):
        self.judge = judge

    def __call__(self, example, prediction, trace=None):
        """Evaluate prediction using LLM judge."""
        # Extract relevant fields from example and prediction
        ground_truth = example.outputs()
        hypothesis = prediction.get('output', str(prediction))

        # Add context if available
        context = example.get('context', None)

        # Get judge evaluation
        result = self.judge.evaluate(
            ground_truth=ground_truth,
            hypothesis=hypothesis,
            context=context
        )

        # Convert to numeric score
        if 'evaluation' in result:
            return result['evaluation'] / 2.0  # Normalize to [0, 1]

        # Fallback to confidence score
        return result.get('confidence', 0.0)

# Usage in DSPy evaluation
clinical_metric = LLMJudgeMetric(ClinicalImpactJudge())

evaluate = dspy.Evaluate(
    devset=test_set,
    metric=clinical_metric,
    num_threads=1  # LLM judges may be expensive
)
</code></pre>
<h3 id="2-progressive-evaluation"><a class="header" href="#2-progressive-evaluation">2. Progressive Evaluation</a></h3>
<pre><code class="language-python">class ProgressiveEvaluator:
    """Multi-stage evaluation using different judges."""

    def __init__(self):
        self.stages = [
            ("quick_filter", QuickFilterJudge()),  # Fast, cheap
            ("detailed", DetailedJudge()),        # Slower, thorough
            ("expert", ExpertJudge())              # Slowest, most accurate
        ]

    def evaluate(self, examples, predictions):
        """Progressively evaluate with increasing detail."""
        results = {}

        for stage_name, judge in self.stages:
            stage_results = []

            for example, pred in zip(examples, predictions):
                # Skip if already filtered out
                if stage_name != "quick_filter" and \
                   results.get("quick_filter", {}).get(pred.id, True) == False:
                    stage_results.append(False)
                    continue

                result = judge.evaluate(
                    ground_truth=example.outputs(),
                    hypothesis=pred.get('output', str(pred))
                )

                passed = result.get('evaluation', True)
                stage_results.append(passed)

            results[stage_name] = dict(zip([p.id for p in predictions],
                                           stage_results))

        return results
</code></pre>
<h2 id="exercises"><a class="header" href="#exercises">Exercises</a></h2>
<ol>
<li>
<p><strong>Implement Domain-Specific Judge</strong>: Create an LLM judge for evaluating responses in your specific domain (e.g., legal documents, scientific papers, customer service).</p>
</li>
<li>
<p><strong>Compare with Traditional Metrics</strong>: Evaluate a dataset using both traditional metrics (WER, BLEU) and an LLM judge. Compare the correlation with human judgments.</p>
</li>
<li>
<p><strong>Optimize with GEPA</strong>: Take a basic judge prompt and optimize it using GEPA on a small labeled dataset.</p>
</li>
<li>
<p><strong>Create Ensemble</strong>: Build an ensemble of judges with different specializations and evaluate their combined performance.</p>
</li>
<li>
<p><strong>Bias Analysis</strong>: Implement bias detection for your judge and analyze potential biases in evaluations.</p>
</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../04-evaluation/07-structured-prompting.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../04-evaluation/09-human-aligned-evaluation.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../04-evaluation/07-structured-prompting.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../04-evaluation/09-human-aligned-evaluation.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>






        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
