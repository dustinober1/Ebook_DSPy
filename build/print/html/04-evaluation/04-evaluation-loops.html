<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Evaluation Loops - DSPy: A Practical Guide</title>


        <!-- Custom HTML head -->

        <meta name="description" content="The most comprehensive DSPy guide with complete coverage of 9 research papers, advanced optimization techniques, and production-ready applications">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../assets/print-only-ef201963.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-4ea68664.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">DSPy: A Practical Guide</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="evaluation-loops"><a class="header" href="#evaluation-loops">Evaluation Loops</a></h1>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<ul>
<li><strong>Chapter 1-3</strong>: DSPy Fundamentals, Signatures, and Modules</li>
<li><strong>Previous Sections</strong>: Creating Datasets, Defining Metrics</li>
<li><strong>Required Knowledge</strong>: Basic Python iteration concepts</li>
<li><strong>Difficulty Level</strong>: Intermediate</li>
<li><strong>Estimated Reading Time</strong>: 30 minutes</li>
</ul>
<h2 id="learning-objectives"><a class="header" href="#learning-objectives">Learning Objectives</a></h2>
<p>By the end of this section, you will be able to:</p>
<ul>
<li>Use the DSPy Evaluate class for systematic evaluation</li>
<li>Run parallel evaluations for better performance</li>
<li>Track and analyze evaluation results</li>
<li>Integrate evaluations with MLflow for experiment tracking</li>
<li>Build evaluation workflows into your development process</li>
</ul>
<h2 id="the-evaluate-class"><a class="header" href="#the-evaluate-class">The Evaluate Class</a></h2>
<p>DSPy’s <code>Evaluate</code> class provides a powerful, systematic way to assess module performance.</p>
<h3 id="basic-usage"><a class="header" href="#basic-usage">Basic Usage</a></h3>
<pre><code class="language-python">import dspy

# Setup: module and data
module = dspy.Predict("question -&gt; answer")
devset = [
    dspy.Example(question="What is 2+2?", answer="4").with_inputs("question"),
    dspy.Example(question="What is 3*3?", answer="9").with_inputs("question"),
    # ... more examples
]

# Define metric
def accuracy(example, pred, trace=None):
    return example.answer.lower() == pred.answer.lower()

# Create evaluator
evaluate = dspy.Evaluate(
    devset=devset,
    metric=accuracy
)

# Run evaluation
score = evaluate(module)
print(f"Accuracy: {score}%")
</code></pre>
<h3 id="evaluate-parameters"><a class="header" href="#evaluate-parameters">Evaluate Parameters</a></h3>
<pre><code class="language-python">evaluate = dspy.Evaluate(
    devset=devset,           # Dataset to evaluate on
    metric=metric,           # Metric function
    num_threads=8,           # Parallel threads (default: 1)
    display_progress=True,   # Show progress bar
    display_table=5,         # Show N example results
    return_all_scores=False, # Return individual scores
    return_outputs=False,    # Return predictions
    provide_traceback=False, # Show errors
)
</code></pre>
<h3 id="understanding-the-output"><a class="header" href="#understanding-the-output">Understanding the Output</a></h3>
<pre><code class="language-python"># Basic usage - returns aggregate score
score = evaluate(module)
print(f"Score: {score}%")  # e.g., "Score: 87.5%"

# With return_all_scores - returns Result object
result = dspy.Evaluate(
    devset=devset,
    metric=metric,
    return_all_scores=True
)(module)

print(f"Aggregate: {result.score}%")
print(f"Individual scores: {result.scores}")  # List of per-example scores

# With return_outputs - includes predictions
result = dspy.Evaluate(
    devset=devset,
    metric=metric,
    return_outputs=True
)(module)

# Access detailed results
for example, prediction, score in result.results:
    print(f"Q: {example.question}")
    print(f"Expected: {example.answer}")
    print(f"Got: {prediction.answer}")
    print(f"Score: {score}")
    print("---")
</code></pre>
<h2 id="parallel-evaluation"><a class="header" href="#parallel-evaluation">Parallel Evaluation</a></h2>
<p>Speed up evaluation with multi-threading:</p>
<h3 id="setting-thread-count"><a class="header" href="#setting-thread-count">Setting Thread Count</a></h3>
<pre><code class="language-python"># Single-threaded (slow but deterministic)
evaluate_slow = dspy.Evaluate(
    devset=devset,
    metric=metric,
    num_threads=1
)

# Multi-threaded (faster)
evaluate_fast = dspy.Evaluate(
    devset=devset,
    metric=metric,
    num_threads=16  # Adjust based on API rate limits
)

# Compare times
import time

start = time.time()
score_slow = evaluate_slow(module)
slow_time = time.time() - start

start = time.time()
score_fast = evaluate_fast(module)
fast_time = time.time() - start

print(f"Single-threaded: {slow_time:.2f}s")
print(f"Multi-threaded: {fast_time:.2f}s")
print(f"Speedup: {slow_time/fast_time:.1f}x")
</code></pre>
<h3 id="thread-count-guidelines"><a class="header" href="#thread-count-guidelines">Thread Count Guidelines</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>API Provider</th><th>Recommended Threads</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>OpenAI Free Tier</td><td>2-4</td><td>Conservative rate limits</td></tr>
<tr><td>OpenAI Paid</td><td>8-16</td><td>Higher limits</td></tr>
<tr><td>Anthropic</td><td>4-8</td><td>Check your tier</td></tr>
<tr><td>Local LLM</td><td>CPU cores</td><td>Limited by hardware</td></tr>
<tr><td>Azure OpenAI</td><td>8-20</td><td>Depends on deployment</td></tr>
</tbody>
</table>
</div>
<pre><code class="language-python"># Detect optimal thread count
import os

# Conservative default
num_threads = min(8, os.cpu_count() or 4)

evaluate = dspy.Evaluate(
    devset=devset,
    metric=metric,
    num_threads=num_threads
)
</code></pre>
<h2 id="progress-tracking"><a class="header" href="#progress-tracking">Progress Tracking</a></h2>
<p>Monitor evaluation progress in real-time:</p>
<h3 id="progress-bar"><a class="header" href="#progress-bar">Progress Bar</a></h3>
<pre><code class="language-python"># Enable progress bar
evaluate = dspy.Evaluate(
    devset=devset,
    metric=metric,
    num_threads=8,
    display_progress=True  # Shows progress bar
)

score = evaluate(module)
# Output: Progress bar with ETA and current score
</code></pre>
<h3 id="display-table"><a class="header" href="#display-table">Display Table</a></h3>
<pre><code class="language-python"># Show example results table
evaluate = dspy.Evaluate(
    devset=devset,
    metric=metric,
    display_progress=True,
    display_table=5  # Show first 5 results
)

score = evaluate(module)
# Output: Table showing questions, expected answers, predictions, scores
</code></pre>
<h2 id="manual-evaluation-loops"><a class="header" href="#manual-evaluation-loops">Manual Evaluation Loops</a></h2>
<p>For more control, write manual evaluation loops:</p>
<h3 id="basic-loop"><a class="header" href="#basic-loop">Basic Loop</a></h3>
<pre><code class="language-python">import dspy

def manual_evaluate(module, devset, metric):
    """Simple manual evaluation loop."""
    scores = []

    for example in devset:
        # Get prediction
        pred = module(**example.inputs())

        # Calculate score
        score = metric(example, pred)
        scores.append(score)

    # Aggregate
    avg_score = sum(scores) / len(scores) if scores else 0
    return avg_score * 100  # Return as percentage

# Usage
score = manual_evaluate(qa_module, devset, accuracy_metric)
print(f"Accuracy: {score:.1f}%")
</code></pre>
<h3 id="loop-with-detailed-tracking"><a class="header" href="#loop-with-detailed-tracking">Loop with Detailed Tracking</a></h3>
<pre><code class="language-python">import dspy
from collections import defaultdict

def detailed_evaluate(module, devset, metric):
    """Evaluation with detailed tracking."""
    results = {
        'scores': [],
        'predictions': [],
        'errors': [],
        'by_category': defaultdict(list)
    }

    for i, example in enumerate(devset):
        try:
            # Get prediction
            pred = module(**example.inputs())

            # Calculate score
            score = metric(example, pred)

            # Store results
            results['scores'].append(score)
            results['predictions'].append({
                'example': example.toDict(),
                'prediction': pred.toDict() if hasattr(pred, 'toDict') else str(pred),
                'score': score
            })

            # Track by category if available
            if hasattr(example, 'category'):
                results['by_category'][example.category].append(score)

        except Exception as e:
            results['errors'].append({
                'index': i,
                'example': example.toDict(),
                'error': str(e)
            })
            results['scores'].append(0)

    # Calculate statistics
    results['stats'] = {
        'total': len(devset),
        'errors': len(results['errors']),
        'avg_score': sum(results['scores']) / len(results['scores']) if results['scores'] else 0,
        'min_score': min(results['scores']) if results['scores'] else 0,
        'max_score': max(results['scores']) if results['scores'] else 0,
    }

    # Category breakdown
    for category, scores in results['by_category'].items():
        results['stats'][f'avg_{category}'] = sum(scores) / len(scores)

    return results

# Usage
results = detailed_evaluate(qa_module, devset, metric)
print(f"Overall accuracy: {results['stats']['avg_score']*100:.1f}%")
print(f"Errors: {results['stats']['errors']}")
</code></pre>
<h3 id="async-evaluation-loop"><a class="header" href="#async-evaluation-loop">Async Evaluation Loop</a></h3>
<p>For I/O-bound operations:</p>
<pre><code class="language-python">import asyncio
import dspy

async def async_evaluate(module, devset, metric, max_concurrent=10):
    """Async evaluation for I/O-bound modules."""
    semaphore = asyncio.Semaphore(max_concurrent)
    scores = []

    async def evaluate_one(example):
        async with semaphore:
            # Note: Requires async-compatible module
            pred = await module.aforward(**example.inputs())
            return metric(example, pred)

    tasks = [evaluate_one(ex) for ex in devset]
    scores = await asyncio.gather(*tasks, return_exceptions=True)

    # Handle exceptions
    valid_scores = [s for s in scores if isinstance(s, (int, float, bool))]
    avg = sum(valid_scores) / len(valid_scores) if valid_scores else 0

    return avg * 100

# Usage (in async context)
# score = await async_evaluate(module, devset, metric)
</code></pre>
<h2 id="mlflow-integration"><a class="header" href="#mlflow-integration">MLflow Integration</a></h2>
<p>Track experiments with MLflow:</p>
<h3 id="basic-mlflow-logging"><a class="header" href="#basic-mlflow-logging">Basic MLflow Logging</a></h3>
<pre><code class="language-python">import dspy
import mlflow

# Configure MLflow
mlflow.set_experiment("dspy-qa-evaluation")

# Run evaluation with logging
with mlflow.start_run(run_name="qa_module_v1"):
    # Log parameters
    mlflow.log_param("module_type", "Predict")
    mlflow.log_param("model", "gpt-4")
    mlflow.log_param("dataset_size", len(devset))

    # Run evaluation
    evaluate = dspy.Evaluate(
        devset=devset,
        metric=metric,
        num_threads=8,
        display_progress=True
    )
    score = evaluate(qa_module)

    # Log metrics
    mlflow.log_metric("accuracy", score)

    print(f"Run logged with accuracy: {score}%")
</code></pre>
<h3 id="comprehensive-mlflow-tracking"><a class="header" href="#comprehensive-mlflow-tracking">Comprehensive MLflow Tracking</a></h3>
<pre><code class="language-python">import dspy
import mlflow
import json

def evaluate_with_mlflow(module, devset, metric, run_name, tags=None):
    """Full evaluation with MLflow tracking."""

    with mlflow.start_run(run_name=run_name):
        # Log tags
        if tags:
            mlflow.set_tags(tags)

        # Log dataset info
        mlflow.log_param("dataset_size", len(devset))

        # Run evaluation
        evaluate = dspy.Evaluate(
            devset=devset,
            metric=metric,
            num_threads=16,
            display_progress=True,
            return_outputs=True
        )
        result = evaluate(module)

        # Log aggregate metrics
        mlflow.log_metric("accuracy", result.score)

        # Log detailed results
        detailed_results = []
        for example, pred, score in result.results:
            detailed_results.append({
                "input": example.inputs(),
                "expected": example.toDict(),
                "predicted": pred.toDict() if hasattr(pred, 'toDict') else str(pred),
                "score": score
            })

        mlflow.log_table(
            data={
                "Question": [r["input"].get("question", "") for r in detailed_results],
                "Expected": [r["expected"].get("answer", "") for r in detailed_results],
                "Predicted": [r["predicted"].get("answer", "") if isinstance(r["predicted"], dict) else r["predicted"] for r in detailed_results],
                "Score": [r["score"] for r in detailed_results],
            },
            artifact_file="evaluation_results.json"
        )

        # Log error analysis
        failures = [r for r in detailed_results if not r["score"]]
        if failures:
            mlflow.log_metric("failure_count", len(failures))

        return result.score

# Usage
score = evaluate_with_mlflow(
    module=qa_module,
    devset=devset,
    metric=metric,
    run_name="qa_v1_gpt4",
    tags={"version": "1.0", "model": "gpt-4"}
)
</code></pre>
<h2 id="evaluation-workflows"><a class="header" href="#evaluation-workflows">Evaluation Workflows</a></h2>
<h3 id="development-workflow"><a class="header" href="#development-workflow">Development Workflow</a></h3>
<pre><code class="language-python">import dspy

def development_evaluation(module, devset, metric):
    """Quick evaluation during development."""
    # Use small subset for speed
    mini_devset = devset[:20]

    evaluate = dspy.Evaluate(
        devset=mini_devset,
        metric=metric,
        num_threads=4,
        display_progress=True,
        display_table=5  # See examples
    )

    score = evaluate(module)
    print(f"\n[Dev] Quick check: {score:.1f}%")
    return score

# Fast iteration loop
for iteration in range(5):
    # Make changes to module...
    score = development_evaluation(module, devset, metric)
    if score &gt; 90:
        print("Target reached!")
        break
</code></pre>
<h3 id="pre-commit-evaluation"><a class="header" href="#pre-commit-evaluation">Pre-Commit Evaluation</a></h3>
<pre><code class="language-python">import dspy

def pre_commit_evaluation(module, devset, metric, threshold=80):
    """Run before committing changes."""
    evaluate = dspy.Evaluate(
        devset=devset,
        metric=metric,
        num_threads=8,
        display_progress=True
    )

    score = evaluate(module)

    if score &lt; threshold:
        raise ValueError(
            f"Evaluation score {score:.1f}% below threshold {threshold}%"
        )

    print(f"[Pre-commit] PASSED with {score:.1f}%")
    return score

# Use in CI/CD or pre-commit hook
pre_commit_evaluation(module, devset, metric, threshold=85)
</code></pre>
<h3 id="ab-testing-workflow"><a class="header" href="#ab-testing-workflow">A/B Testing Workflow</a></h3>
<pre><code class="language-python">import dspy

def compare_modules(module_a, module_b, devset, metric, names=("A", "B")):
    """Compare two module versions."""
    evaluate = dspy.Evaluate(
        devset=devset,
        metric=metric,
        num_threads=8,
        display_progress=True
    )

    print(f"Evaluating {names[0]}...")
    score_a = evaluate(module_a)

    print(f"\nEvaluating {names[1]}...")
    score_b = evaluate(module_b)

    # Report
    print("\n" + "="*50)
    print("COMPARISON RESULTS")
    print("="*50)
    print(f"{names[0]}: {score_a:.1f}%")
    print(f"{names[1]}: {score_b:.1f}%")
    print(f"Difference: {score_b - score_a:+.1f}%")

    if score_b &gt; score_a:
        print(f"\n{names[1]} is better by {score_b - score_a:.1f} points")
    elif score_a &gt; score_b:
        print(f"\n{names[0]} is better by {score_a - score_b:.1f} points")
    else:
        print("\nBoth modules perform equally")

    return score_a, score_b

# Compare baseline vs optimized
baseline = dspy.Predict("question -&gt; answer")
optimized = optimizer.compile(baseline, trainset=trainset)

compare_modules(baseline, optimized, testset, metric, ("Baseline", "Optimized"))
</code></pre>
<h2 id="error-analysis"><a class="header" href="#error-analysis">Error Analysis</a></h2>
<p>Understanding failures is as important as measuring success:</p>
<h3 id="categorizing-errors"><a class="header" href="#categorizing-errors">Categorizing Errors</a></h3>
<pre><code class="language-python">import dspy
from collections import defaultdict

def error_analysis(module, devset, metric):
    """Analyze evaluation errors."""
    errors = defaultdict(list)
    successes = []

    evaluate = dspy.Evaluate(
        devset=devset,
        metric=metric,
        return_outputs=True
    )
    result = evaluate(module)

    for example, pred, score in result.results:
        if not score:  # Failed
            # Categorize the error
            if len(pred.answer) == 0:
                errors['empty_response'].append((example, pred))
            elif len(pred.answer) &lt; 10:
                errors['too_short'].append((example, pred))
            elif example.answer.lower() not in pred.answer.lower():
                errors['wrong_answer'].append((example, pred))
            else:
                errors['other'].append((example, pred))
        else:
            successes.append((example, pred))

    # Report
    print("ERROR ANALYSIS")
    print("="*50)
    print(f"Total: {len(devset)}")
    print(f"Success: {len(successes)} ({100*len(successes)/len(devset):.1f}%)")
    print(f"Failures: {len(devset) - len(successes)}")
    print("\nError breakdown:")
    for error_type, examples in errors.items():
        print(f"  {error_type}: {len(examples)} ({100*len(examples)/len(devset):.1f}%)")

    return errors

# Run analysis
errors = error_analysis(qa_module, devset, metric)

# Examine specific error types
print("\nExamples of wrong answers:")
for example, pred in errors['wrong_answer'][:3]:
    print(f"  Q: {example.question}")
    print(f"  Expected: {example.answer}")
    print(f"  Got: {pred.answer}")
    print()
</code></pre>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>Evaluation loops are your systematic approach to measuring quality:</p>
<ol>
<li><strong>Use dspy.Evaluate</strong> for standard evaluation needs</li>
<li><strong>Enable parallel execution</strong> for faster evaluation</li>
<li><strong>Track results with MLflow</strong> for experiment management</li>
<li><strong>Build evaluation into workflows</strong> (development, pre-commit, A/B testing)</li>
<li><strong>Analyze errors</strong> to understand failure patterns</li>
</ol>
<h3 id="key-takeaways"><a class="header" href="#key-takeaways">Key Takeaways</a></h3>
<ol>
<li><strong>dspy.Evaluate</strong> provides comprehensive evaluation capabilities</li>
<li><strong>Parallel execution</strong> speeds up evaluation significantly</li>
<li><strong>Progress tracking</strong> keeps you informed during long evaluations</li>
<li><strong>MLflow integration</strong> enables experiment tracking</li>
<li><strong>Error analysis</strong> reveals improvement opportunities</li>
</ol>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<ul>
<li><a href="./05-best-practices.html">Next Section: Best Practices</a> - Evaluation best practices</li>
<li><a href="./06-exercises.html">Exercises</a> - Practice evaluation skills</li>
<li><a href="../../examples/chapter04/">Examples</a> - See evaluation code</li>
</ul>
<h2 id="further-reading"><a class="header" href="#further-reading">Further Reading</a></h2>
<ul>
<li><a href="https://dspy.ai/api/evaluation/evaluate">DSPy Evaluate Documentation</a></li>
<li><a href="https://mlflow.org/docs/latest/tracking.html">MLflow Tracking</a></li>
<li><a href="https://www.optimizely.com/optimization-glossary/ab-testing/">A/B Testing Best Practices</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../04-evaluation/03-defining-metrics.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../04-evaluation/05-best-practices.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../04-evaluation/03-defining-metrics.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../04-evaluation/05-best-practices.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>






        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
