<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Bayesian Optimization - DSPy: A Practical Guide</title>


        <!-- Custom HTML head -->

        <meta name="description" content="The most comprehensive DSPy guide with complete coverage of 9 research papers, advanced optimization techniques, and production-ready applications">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../assets/print-only-ef201963.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-4ea68664.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">DSPy: A Practical Guide</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="bayesian-optimization-for-prompt-tuning"><a class="header" href="#bayesian-optimization-for-prompt-tuning">Bayesian Optimization for Prompt Tuning</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>Bayesian Optimization (BO) is a powerful global optimization technique that excels at optimizing expensive black-box functions with few evaluations. In the context of DSPy and prompt tuning, BO provides a principled approach to navigating the vast space of possible prompt configurations by building a probabilistic model of the performance landscape. This model allows BO to make intelligent decisions about which configurations to evaluate next, effectively balancing exploration (trying uncertain regions) and exploitation (refining promising areas).</p>
<h3 id="learning-objectives"><a class="header" href="#learning-objectives">Learning Objectives</a></h3>
<p>By the end of this section, you will:</p>
<ul>
<li>Understand Bayesian optimization principles and their application to prompt tuning</li>
<li>Implement Gaussian Process-based optimization for prompts</li>
<li>Master acquisition functions for intelligent exploration</li>
<li>Apply BO to various prompt optimization scenarios</li>
<li>Evaluate and tune BO hyperparameters for optimal performance</li>
</ul>
<h2 id="bayesian-optimization-fundamentals"><a class="header" href="#bayesian-optimization-fundamentals">Bayesian Optimization Fundamentals</a></h2>
<h3 id="core-components"><a class="header" href="#core-components">Core Components</a></h3>
<p>Bayesian Optimization consists of four main components:</p>
<ol>
<li><strong>Search Space</strong>: The domain of possible configurations</li>
<li><strong>Surrogate Model</strong>: A probabilistic model approximating the objective function</li>
<li><strong>Acquisition Function</strong>: Guides the selection of next evaluation points</li>
<li><strong>Optimization Loop</strong>: Iteratively selects and evaluates configurations</li>
</ol>
<pre><code class="language-python">import numpy as np
from typing import Dict, List, Tuple, Optional, Callable
import dspy
from scipy.stats import norm
from scipy.optimize import minimize

class BayesianPromptOptimizer:
    """
    Bayesian Optimization framework for prompt tuning in DSPy.
    """

    def __init__(
        self,
        task_signature,
        trainset,
        valset,
        metric_fn,
        search_space=None,
        surrogate_model="gp",  # Gaussian Process
        acquisition="ei",  # Expected Improvement
        max_iterations=100,
        n_initial_points=10,
        random_state=None
    ):
        self.task_signature = task_signature
        self.trainset = trainset
        self.valset = valset
        self.metric_fn = metric_fn
        self.search_space = search_space or self._define_search_space()
        self.max_iterations = max_iterations
        self.n_initial_points = n_initial_points
        self.random_state = random_state

        # Initialize components
        self.surrogate_model = self._create_surrogate_model(surrogate_model)
        self.acquisition_fn = self._create_acquisition_function(acquisition)

        # Storage for observations
        self.X_observed = []  # Evaluated configurations
        self.y_observed = []  # Corresponding scores

        # Track best solution
        self.best_config = None
        self.best_score = float("-inf")

    def _define_search_space(self):
        """Define the search space for prompt optimization."""
        return {
            "instruction_length": {"type": "discrete", "values": [10, 20, 30, 40, 50]},
            "instruction_style": {
                "type": "categorical",
                "values": ["direct", "polite", "detailed", "concise"]
            },
            "n_examples": {"type": "discrete", "values": [0, 1, 2, 3, 4, 5]},
            "example_complexity": {
                "type": "categorical",
                "values": ["simple", "medium", "complex"]
            },
            "temperature": {"type": "continuous", "bounds": [0.0, 1.0]},
            "top_p": {"type": "continuous", "bounds": [0.8, 1.0]},
            "max_tokens": {"type": "discrete", "values": [50, 100, 150, 200, 250]},
            "format_style": {
                "type": "categorical",
                "values": ["qa", "instruction", "conversation", "template"]
            }
        }

    def optimize(self):
        """Execute Bayesian optimization."""
        print(f"Starting Bayesian optimization for prompt tuning...")
        print(f"Max iterations: {self.max_iterations}")
        print(f"Initial random points: {self.n_initial_points}")

        # Phase 1: Initial random exploration
        print("\n=== Phase 1: Initial Exploration ===")
        for i in range(self.n_initial_points):
            config = self._sample_random_configuration()
            score = self._evaluate_configuration(config)
            self._add_observation(config, score)

        # Phase 2: Bayesian optimization loop
        print("\n=== Phase 2: Bayesian Optimization ===")
        for iteration in range(self.max_iterations - self.n_initial_points):
            print(f"\nIteration {iteration + 1}")

            # Fit surrogate model
            self.surrogate_model.fit(self.X_observed, self.y_observed)

            # Find next point to evaluate
            next_config = self._select_next_configuration()

            # Evaluate selected configuration
            score = self._evaluate_configuration(next_config)
            self._add_observation(next_config, score)

            # Report progress
            print(f"Score: {score:.4f} (Best: {self.best_score:.4f})")

        return self.best_config, self.best_score
</code></pre>
<h3 id="gaussian-process-surrogate-model"><a class="header" href="#gaussian-process-surrogate-model">Gaussian Process Surrogate Model</a></h3>
<pre><code class="language-python">class GaussianProcessSurrogate:
    """
    Gaussian Process surrogate model for Bayesian optimization.
    """

    def __init__(
        self,
        kernel="rbf",  # Radial Basis Function
        alpha=1e-6,  # Noise parameter
        length_scale=1.0,
        length_scale_bounds=(1e-1, 10.0)
    ):
        self.kernel = kernel
        self.alpha = alpha
        self.length_scale = length_scale
        self.length_scale_bounds = length_scale_bounds

        self.X_train = None
        self.y_train = None
        self.L = None  # Cholesky decomposition
        self.alpha_vec = None

    def fit(self, X, y):
        """Fit the Gaussian Process to observed data."""
        # Convert configurations to feature vectors
        X_encoded = self._encode_configurations(X)
        y = np.array(y)

        # Center the target values
        self.y_mean = np.mean(y)
        y_centered = y - self.y_mean

        # Compute kernel matrix
        K = self._compute_kernel_matrix(X_encoded)

        # Add noise for numerical stability
        K += self.alpha * np.eye(K.shape[0])

        # Cholesky decomposition
        self.L = np.linalg.cholesky(K)

        # Solve for alpha vector
        self.alpha_vec = np.linalg.solve(self.L.T, np.linalg.solve(self.L, y_centered))

        # Store training data
        self.X_train = X_encoded

    def predict(self, X, return_std=False):
        """Predict mean and uncertainty for new configurations."""
        X_new = self._encode_configurations(X)

        # Compute kernel between new points and training points
        K_star = self._compute_cross_kernel(X_new, self.X_train)

        # Predict mean
        y_mean = K_star.dot(self.alpha_vec) + self.y_mean

        if return_std:
            # Compute variance
            v = np.linalg.solve(self.L, K_star.T)
            y_var = self._compute_kernel_diagonal(X_new) - np.sum(v ** 2, axis=0)
            y_std = np.sqrt(np.maximum(y_var, 1e-10))

            return y_mean, y_std

        return y_mean

    def _encode_configurations(self, configs):
        """Encode configurations as feature vectors."""
        if not configs:
            return np.array([[]])

        encoded = []
        for config in configs:
            vector = []
            for param_name, param_value in config.items():
                # One-hot encode categorical variables
                if isinstance(param_value, str):
                    # Get all possible values for this parameter
                    all_values = self._get_param_values(param_name)
                    one_hot = [1.0 if v == param_value else 0.0 for v in all_values]
                    vector.extend(one_hot)
                else:
                    # Normalize continuous and discrete values
                    normalized = self._normalize_parameter(param_name, param_value)
                    vector.append(normalized)
            encoded.append(vector)

        return np.array(encoded)

    def _compute_kernel_matrix(self, X):
        """Compute the kernel matrix."""
        if self.kernel == "rbf":
            # RBF kernel
            sq_dist = np.sum(X ** 2, axis=1).reshape(-1, 1) + \
                      np.sum(X ** 2, axis=1) - 2 * np.dot(X, X.T)
            K = np.exp(-0.5 / self.length_scale ** 2 * sq_dist)
        elif self.kernel == "matern":
            # Matérn kernel (ν = 3/2)
            dist = np.sqrt(np.sum(X ** 2, axis=1).reshape(-1, 1) + \
                          np.sum(X ** 2, axis=1) - 2 * np.dot(X, X.T))
            K = (1 + np.sqrt(3) * dist / self.length_scale) * \
                np.exp(-np.sqrt(3) * dist / self.length_scale)
        else:
            raise ValueError(f"Unknown kernel: {self.kernel}")

        return K

    def _compute_cross_kernel(self, X1, X2):
        """Compute kernel between two sets of points."""
        if self.kernel == "rbf":
            sq_dist = np.sum(X1 ** 2, axis=1).reshape(-1, 1) + \
                      np.sum(X2 ** 2, axis=1) - 2 * np.dot(X1, X2.T)
            K = np.exp(-0.5 / self.length_scale ** 2 * sq_dist)
        elif self.kernel == "matern":
            dist = np.sqrt(np.sum(X1 ** 2, axis=1).reshape(-1, 1) + \
                          np.sum(X2 ** 2, axis=1) - 2 * np.dot(X1, X2.T))
            K = (1 + np.sqrt(3) * dist / self.length_scale) * \
                np.exp(-np.sqrt(3) * dist / self.length_scale)
        else:
            raise ValueError(f"Unknown kernel: {self.kernel}")

        return K
</code></pre>
<h3 id="acquisition-functions"><a class="header" href="#acquisition-functions">Acquisition Functions</a></h3>
<p>Acquisition functions guide the optimization by balancing exploration and exploitation:</p>
<pre><code class="language-python">class AcquisitionFunctions:
    """Collection of acquisition functions for Bayesian optimization."""

    @staticmethod
    def expected_improvement(mean, std, best_y, xi=0.01):
        """
        Expected Improvement acquisition function.

        Args:
            mean: Predicted mean values
            std: Predicted standard deviations
            best_y: Best observed value so far
            xi: Exploration-exploitation trade-off
        """
        with np.errstate(divide='warn'):
            imp = mean - best_y - xi
            Z = imp / std
            ei = imp * norm.cdf(Z) + std * norm.pdf(Z)
            ei[std == 0.0] = 0.0

        return ei

    @staticmethod
    def probability_of_improvement(mean, std, best_y, xi=0.01):
        """
        Probability of Improvement acquisition function.
        """
        with np.errstate(divide='warn'):
            Z = (mean - best_y - xi) / std
            pi = norm.cdf(Z)
            pi[std == 0.0] = 0.0

        return pi

    @staticmethod
    def upper_confidence_bound(mean, std, kappa=2.576):
        """
        Upper Confidence Bound acquisition function.

        kappa determines the confidence level (2.576 for 99% confidence)
        """
        return mean + kappa * std

    @staticmethod
    def thompson_sampling(mean, std, n_samples=1000):
        """
        Thompson Sampling acquisition function.
        """
        samples = np.random.normal(mean, std, size=(n_samples, len(mean)))
        return np.mean(samples, axis=0)
</code></pre>
<h2 id="advanced-bayesian-optimization-techniques"><a class="header" href="#advanced-bayesian-optimization-techniques">Advanced Bayesian Optimization Techniques</a></h2>
<h3 id="multi-objective-bayesian-optimization"><a class="header" href="#multi-objective-bayesian-optimization">Multi-Objective Bayesian Optimization</a></h3>
<pre><code class="language-python">class MultiObjectiveBayesianOptimizer:
    """
    Bayesian optimizer for multiple objectives (e.g., accuracy and latency).
    """

    def __init__(
        self,
        objectives,
        task_signature,
        trainset,
        valset,
        metric_fns,
        search_space=None,
        preference_weights=None
    ):
        self.objectives = objectives
        self.task_signature = task_signature
        self.trainset = trainset
        self.valset = valset
        self.metric_fns = metric_fns
        self.search_space = search_space or self._define_search_space()
        self.preference_weights = preference_weights or {obj: 1.0 for obj in objectives}

        # Initialize separate surrogate models for each objective
        self.surrogates = {
            obj: GaussianProcessSurrogate() for obj in objectives
        }

        # Storage
        self.X_observed = []
        self.y_observed = {obj: [] for obj in objectives}
        self.pareto_front = []

    def optimize(self, max_iterations=100):
        """Execute multi-objective optimization."""
        print(f"Starting multi-objective Bayesian optimization...")
        print(f"Objectives: {list(self.objectives)}")

        # Initial random exploration
        for i in range(self.n_initial_points):
            config = self._sample_random_configuration()
            scores = self._evaluate_multi_objective(config)
            self._add_observation(config, scores)

        # Optimization loop
        for iteration in range(max_iterations - self.n_initial_points):
            # Fit all surrogates
            for obj in self.objectives:
                self.surrogates[obj].fit(self.X_observed, self.y_observed[obj])

            # Select next configuration using hypervolume improvement
            next_config = self._select_next_hvi_configuration()

            # Evaluate
            scores = self._evaluate_multi_objective(next_config)
            self._add_observation(next_config, scores)

            # Update Pareto front
            self._update_pareto_front()

        return self.pareto_front

    def _select_next_hvi_configuration(self):
        """Select next configuration using Expected Hypervolume Improvement."""
        # Generate candidate configurations
        candidates = self._generate_candidates(1000)

        # Predict performance for all objectives
        predictions = {}
        uncertainties = {}
        for obj in self.objectives:
            mean, std = self.surrogates[obj].predict(candidates, return_std=True)
            predictions[obj] = mean
            uncertainties[obj] = std

        # Compute hypervolume improvement for each candidate
        hvi_scores = []
        reference_point = self._compute_reference_point()

        for i, candidate in enumerate(candidates):
            # Sample possible outcomes
            n_samples = 100
            samples = []
            for _ in range(n_samples):
                sample_scores = {}
                for obj in self.objectives:
                    sample = np.random.normal(
                        predictions[obj][i],
                        uncertainties[obj][i]
                    )
                    sample_scores[obj] = sample
                samples.append(sample_scores)

            # Compute expected hypervolume improvement
            hvi = self._expected_hypervolume_improvement(
                samples, reference_point
            )
            hvi_scores.append(hvi)

        # Select candidate with highest hypervolume improvement
        best_idx = np.argmax(hvi_scores)
        return candidates[best_idx]

    def _expected_hypervolume_improvement(self, samples, reference_point):
        """Compute expected hypervolume improvement."""
        # Compute hypervolume of current Pareto front
        current_hv = self._compute_hypervolume(self.pareto_front, reference_point)

        # Add samples to Pareto front and compute new hypervolumes
        hvs = []
        for sample in samples:
            temp_front = self.pareto_front + [sample]
            temp_front = self._filter_dominated(temp_front)
            hv = self._compute_hypervolume(temp_front, reference_point)
            hvs.append(hv)

        # Expected improvement
        return np.mean(hvs) - current_hv
</code></pre>
<h3 id="contextual-bayesian-optimization"><a class="header" href="#contextual-bayesian-optimization">Contextual Bayesian Optimization</a></h3>
<pre><code class="language-python">class ContextualBayesianOptimizer:
    """
    Bayesian optimizer that considers context (e.g., task difficulty, domain).
    """

    def __init__(
        self,
        contexts,
        base_optimizer,
        context_features=None
    ):
        self.contexts = contexts
        self.base_optimizer = base_optimizer
        self.context_features = context_features or self._extract_context_features()

        # Learn context-dependent search spaces
        self.contextual_search_spaces = self._learn_contextual_spaces()

    def optimize_with_context(self, context, max_iterations=50):
        """Optimize for a specific context."""
        print(f"Optimizing for context: {context}")

        # Get context-specific search space
        search_space = self.contextual_search_spaces.get(context, self.base_optimizer.search_space)

        # Create context-aware optimizer
        context_optimizer = BayesianPromptOptimizer(
            task_signature=self.base_optimizer.task_signature,
            trainset=self.base_optimizer.trainset,
            valset=self.base_optimizer.valset,
            metric_fn=self.base_optimizer.metric_fn,
            search_space=search_space,
            max_iterations=max_iterations
        )

        # Warm-start with knowledge from similar contexts
        similar_contexts = self._find_similar_contexts(context)
        if similar_contexts:
            self._warm_start_optimizer(context_optimizer, similar_contexts)

        # Run optimization
        return context_optimizer.optimize()

    def _learn_contextual_spaces(self):
        """Learn context-specific search spaces."""
        contextual_spaces = {}

        for context in self.contexts:
            # Analyze successful configurations for this context
            successful_configs = self._get_successful_configs(context)

            # Infer promising ranges and values
            inferred_space = self._infer_search_space(successful_configs)
            contextual_spaces[context] = inferred_space

        return contextual_spaces
</code></pre>
<h2 id="practical-implementation"><a class="header" href="#practical-implementation">Practical Implementation</a></h2>
<h3 id="complete-bayesian-optimization-pipeline"><a class="header" href="#complete-bayesian-optimization-pipeline">Complete Bayesian Optimization Pipeline</a></h3>
<pre><code class="language-python">def optimize_dspy_prompts_with_bo(
    task_type="qa",
    trainset_size=100,
    valset_size=50,
    optimization_budget=200
):
    """Complete Bayesian optimization pipeline for DSPy prompts."""

    # 1. Load and prepare data
    print("=== Loading and Preparing Data ===")
    trainset, valset = load_and_prepare_data(
        task_type=task_type,
        train_size=trainset_size,
        val_size=valset_size
    )

    # 2. Define task signature
    if task_type == "qa":
        class QASignature(dspy.Signature):
            """Answer questions based on provided context."""
            context = dspy.InputField(desc="Relevant context")
            question = dspy.InputField(desc="Question to answer")
            answer = dspy.OutputField(desc="Answer")

        task_signature = QASignature

    # 3. Define evaluation metric
    def evaluation_metric(example, pred, trace=None):
        if task_type == "qa":
            return evaluate_qa_performance(example, pred)
        # Add other task types as needed

    # 4. Create Bayesian optimizer
    print("\n=== Initializing Bayesian Optimizer ===")
    optimizer = BayesianPromptOptimizer(
        task_signature=task_signature,
        trainset=trainset,
        valset=valset,
        metric_fn=evaluation_metric,
        surrogate_model="gp",
        acquisition="ei",
        max_iterations=optimization_budget,
        n_initial_points=20
    )

    # 5. Run optimization
    print("\n=== Running Bayesian Optimization ===")
    best_config, best_score = optimizer.optimize()

    # 6. Create optimized prompt module
    print("\n=== Creating Optimized Module ===")
    optimized_module = create_module_from_config(
        task_signature,
        best_config
    )

    # 7. Evaluate on test set
    print("\n=== Final Evaluation ===")
    testset = load_test_data(task_type)
    final_score = evaluate_module(optimized_module, testset, evaluation_metric)

    # 8. Report results
    print(f"\n=== Optimization Results ===")
    print(f"Best validation score: {best_score:.4f}")
    print(f"Test score: {final_score:.4f}")
    print(f"Best configuration:")
    for key, value in best_config.items():
        print(f"  {key}: {value}")

    return {
        "module": optimized_module,
        "config": best_config,
        "val_score": best_score,
        "test_score": final_score,
        "history": optimizer.X_observed,
        "scores": optimizer.y_observed
    }

def create_module_from_config(signature, config):
    """Create a DSPy module from optimized configuration."""
    # Build instruction
    instruction = build_instruction_from_config(config)

    # Create enhanced signature
    class OptimizedSignature(signature):
        instructions = instruction

    # Create module
    if config.get("chain_of_thought", False):
        module = dspy.ChainOfThought(OptimizedSignature)
    else:
        module = dspy.Predict(OptimizedSignature)

    # Configure LM parameters
    module.lm = module.lm.copy(
        temperature=config.get("temperature", 0.7),
        top_p=config.get("top_p", 0.9),
        max_tokens=config.get("max_tokens", 150)
    )

    # Add examples if configured
    if config.get("n_examples", 0) &gt; 0:
        examples = select_examples(config["n_examples"])
        module = module.with_demos(examples)

    return module
</code></pre>
<h3 id="example-optimizing-chain-of-thought-prompts"><a class="header" href="#example-optimizing-chain-of-thought-prompts">Example: Optimizing Chain-of-Thought Prompts</a></h3>
<pre><code class="language-python">class CoTBayesianOptimizer:
    """
    Specialized Bayesian optimizer for Chain-of-Thought prompts.
    """

    def __init__(self, task_signature, trainset, valset, metric_fn):
        self.task_signature = task_signature
        self.trainset = trainset
        self.valset = valset
        self.metric_fn = metric_fn

        # CoT-specific search space
        self.cot_search_space = {
            "reasoning_instruction": {
                "type": "categorical",
                "values": [
                    "Think step by step.",
                    "Break down the problem.",
                    "Reason through this carefully.",
                    "Work through this methodically."
                ]
            },
            "show_reasoning": {
                "type": "categorical",
                "values": ["before", "after", "integrated"]
            },
            "n_demonstrations": {"type": "discrete", "values": [0, 1, 2, 3]},
            "demo_complexity": {
                "type": "categorical",
                "values": ["minimal", "detailed", "verbose"]
            },
            "final_instruction": {
                "type": "categorical",
                "values": [
                    "Finally, provide the answer.",
                    "Now give the final answer.",
                    "The answer is:",
                    "Therefore:"
                ]
            }
        }

        self.optimizer = BayesianPromptOptimizer(
            task_signature=task_signature,
            trainset=trainset,
            valset=valset,
            metric_fn=metric_fn,
            search_space=self.cot_search_space
        )

    def optimize_cot(self, max_iterations=100):
        """Optimize Chain-of-Thought prompt configuration."""
        print("=== Optimizing Chain-of-Thought Configuration ===")

        # Special evaluation for CoT
        def cot_metric(example, pred, trace=None):
            # Base task performance
            base_score = self.metric_fn(example, pred)

            # Reasoning quality
            reasoning_score = evaluate_reasoning_quality(
                pred.get("rationale", ""),
                example.get("reasoning_steps", [])
            )

            # Combined score
            return 0.7 * base_score + 0.3 * reasoning_score

        # Update optimizer metric
        self.optimizer.metric_fn = cot_metric

        # Run optimization
        return self.optimizer.optimize(max_iterations=max_iterations)

    def create_cot_module(self, config):
        """Create optimized CoT module."""
        # Build CoT instruction
        cot_instruction = build_cot_instruction(config)

        # Create enhanced signature
        class OptimizedCoTSignature(self.task_signature):
            instructions = cot_instruction

        # Create CoT module
        cot_module = dspy.ChainOfThought(OptimizedCoTSignature)

        # Add demonstrations
        if config["n_demonstrations"] &gt; 0:
            demos = create_cot_demonstrations(
                self.trainset[:config["n_demonstrations"]],
                config["demo_complexity"]
            )
            cot_module = cot_module.with_demos(demos)

        return cot_module
</code></pre>
<h2 id="evaluation-and-analysis"><a class="header" href="#evaluation-and-analysis">Evaluation and Analysis</a></h2>
<h3 id="convergence-analysis"><a class="header" href="#convergence-analysis">Convergence Analysis</a></h3>
<pre><code class="language-python">def analyze_bo_convergence(optimizer_history, true_optimum=None):
    """Analyze the convergence of Bayesian optimization."""
    iterations = range(len(optimizer_history["scores"]))
    scores = optimizer_history["scores"]
    best_so_far = np.maximum.accumulate(scores)

    # Plot convergence
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(iterations, scores, 'o-', alpha=0.5, label='All evaluations')
    plt.plot(iterations, best_so_far, 'r-', linewidth=2, label='Best so far')
    if true_optimum:
        plt.axhline(y=true_optimum, color='g', linestyle='--', label='True optimum')
    plt.xlabel('Iteration')
    plt.ylabel('Score')
    plt.title('BO Convergence')
    plt.legend()

    # Plot exploration vs exploitation
    plt.subplot(1, 2, 2)
    uncertainty = optimizer_history.get("uncertainty", [])
    if uncertainty:
        plt.scatter(iterations, uncertainty, c=scores, cmap='viridis', alpha=0.6)
        plt.colorbar(label='Score')
        plt.xlabel('Iteration')
        plt.ylabel('Uncertainty')
        plt.title('Exploration Pattern')

    plt.tight_layout()
    plt.show()

    # Compute convergence metrics
    convergence_metrics = {
        "initial_improvement": best_so_far[10] - scores[0] if len(scores) &gt; 10 else 0,
        "final_improvement": best_so_far[-1] - best_so_far[10] if len(scores) &gt; 10 else best_so_far[-1] - scores[0],
        "iterations_to_90_percent": np.where(best_so_far &gt;= 0.9 * best_so_far[-1])[0][0] if any(best_so_far &gt;= 0.9 * best_so_far[-1]) else len(scores) - 1,
        "regret": (true_optimum - best_so_far[-1]) if true_optimum else None
    }

    print("\n=== Convergence Metrics ===")
    for metric, value in convergence_metrics.items():
        if value is not None:
            print(f"{metric}: {value:.4f}")

    return convergence_metrics
</code></pre>
<h3 id="comparison-with-other-methods"><a class="header" href="#comparison-with-other-methods">Comparison with Other Methods</a></h3>
<pre><code class="language-python">def compare_optimization_methods(task_data, budget=200):
    """Compare Bayesian optimization with other optimization methods."""

    methods = {
        "Bayesian Optimization": lambda: run_bayesian_optimization(task_data, budget),
        "Random Search": lambda: run_random_search(task_data, budget),
        "Grid Search": lambda: run_grid_search(task_data, budget//10),  # Grid search is expensive
        "Genetic Algorithm": lambda: run_genetic_algorithm(task_data, budget)
    }

    results = {}

    for method_name, method_fn in methods.items():
        print(f"\n=== Running {method_name} ===")
        start_time = time.time()
        best_config, best_score, history = method_fn()
        end_time = time.time()

        results[method_name] = {
            "best_score": best_score,
            "best_config": best_config,
            "time": end_time - start_time,
            "history": history
        }

        print(f"Best score: {best_score:.4f}")
        print(f"Time: {end_time - start_time:.2f}s")

    # Analysis
    print("\n=== Comparison Summary ===")
    for method, result in results.items():
        efficiency = result["best_score"] / result["time"]
        print(f"{method}:")
        print(f"  Score: {result['best_score']:.4f}")
        print(f"  Time: {result['time']:.2f}s")
        print(f"  Efficiency: {efficiency:.6f}")

    return results
</code></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="bayesian-optimization-configuration"><a class="header" href="#bayesian-optimization-configuration">Bayesian Optimization Configuration</a></h3>
<pre><code class="language-python"># For high-dimensional spaces
high_dim_config = {
    "surrogate_model": "gp",
    "kernel": "matern",  # Better for high dimensions
    "acquisition": "ei",
    "max_iterations": 500,
    "n_initial_points": 50  # More initial points
}

# For noisy evaluations
noisy_config = {
    "surrogate_model": "gp",
    "alpha": 1e-3,  # Higher noise parameter
    "acquisition": "ucb",  # More exploration
    "kappa": 2.0,
    "max_iterations": 300
}

# For expensive evaluations
expensive_config = {
    "max_iterations": 50,  # Fewer evaluations
    "n_initial_points": 10,
    "acquisition": "ei",  # Good exploitation
    "xi": 0.1  # More exploitation
}
</code></pre>
<h3 id="common-pitfalls-and-solutions"><a class="header" href="#common-pitfalls-and-solutions">Common Pitfalls and Solutions</a></h3>
<ol>
<li>
<p><strong>Poor Search Space Definition</strong>:</p>
<ul>
<li>Problem: Too large or inappropriate search space</li>
<li>Solution: Start with a focused search space and expand if needed</li>
</ul>
</li>
<li>
<p><strong>Insufficient Initial Points</strong>:</p>
<ul>
<li>Problem: Poor initial model fitting</li>
<li>Solution: Use at least 10-20 initial random points</li>
</ul>
</li>
<li>
<p><strong>Local Optima</strong>:</p>
<ul>
<li>Problem: Getting stuck in suboptimal regions</li>
<li>Solution: Use exploration-focused acquisition functions</li>
</ul>
</li>
<li>
<p><strong>Noisy Evaluations</strong>:</p>
<ul>
<li>Problem: Inconsistent evaluation scores</li>
<li>Solution: Increase noise parameter and use multiple evaluations</li>
</ul>
</li>
</ol>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>Bayesian optimization provides a principled and efficient approach to prompt tuning in DSPy. By building a probabilistic model of the performance landscape, BO can make intelligent decisions about which configurations to evaluate next, achieving superior performance with fewer evaluations compared to traditional optimization methods.</p>
<h3 id="key-takeaways"><a class="header" href="#key-takeaways">Key Takeaways</a></h3>
<ol>
<li>BO balances exploration and exploitation through acquisition functions</li>
<li>Gaussian Processes provide effective surrogate models for prompt optimization</li>
<li>Multi-objective optimization can handle multiple metrics simultaneously</li>
<li>Contextual BO adapts optimization to different task characteristics</li>
<li>Proper configuration is crucial for success</li>
</ol>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<p>In the next section, we’ll explore advanced optimization strategies that combine multiple techniques and discuss how to choose the right optimizer for specific use cases.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../05-optimizers/11-monte-carlo-optimization.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../05-optimizers/13-comprehensive-examples.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../05-optimizers/11-monte-carlo-optimization.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../05-optimizers/13-comprehensive-examples.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>






        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
