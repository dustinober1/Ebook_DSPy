<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Joint Optimization - DSPy: A Practical Guide</title>


        <!-- Custom HTML head -->

        <meta name="description" content="The most comprehensive DSPy guide with complete coverage of 9 research papers, advanced optimization techniques, and production-ready applications">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../assets/print-only-ef201963.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-4ea68664.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">DSPy: A Practical Guide</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="joint-optimization-fine-tuning-and-prompt-synergy"><a class="header" href="#joint-optimization-fine-tuning-and-prompt-synergy">Joint Optimization: Fine-Tuning and Prompt Synergy</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>Joint optimization in DSPy represents a paradigm shift from treating fine-tuning and prompt optimization as separate processes. Instead, it recognizes that these two optimization dimensions are deeply interconnected and can be optimized together to achieve superior performance. This approach simultaneously adjusts model parameters and prompt structures, creating a cohesive optimization strategy that leverages the strengths of both approaches.</p>
<h3 id="learning-objectives"><a class="header" href="#learning-objectives">Learning Objectives</a></h3>
<p>By the end of this section, you will:</p>
<ul>
<li>Understand the theoretical foundation of joint optimization</li>
<li>Implement joint optimization strategies in DSPy</li>
<li>Master techniques for coordinating parameter and prompt updates</li>
<li>Apply joint optimization to various task types</li>
<li>Evaluate the benefits of joint vs. sequential optimization</li>
</ul>
<h2 id="theoretical-foundations"><a class="header" href="#theoretical-foundations">Theoretical Foundations</a></h2>
<h3 id="why-joint-optimization-matters"><a class="header" href="#why-joint-optimization-matters">Why Joint Optimization Matters</a></h3>
<p>Traditional approaches often follow a sequential pattern:</p>
<ol>
<li>Fine-tune the model on task-specific data</li>
<li>Optimize prompts for the fine-tuned model</li>
</ol>
<p>However, this approach has limitations:</p>
<ul>
<li><strong>Suboptimal Local Minima</strong>: Each optimization phase gets stuck in its own local optimum</li>
<li><strong>Mismatched Representations</strong>: The fine-tuned model and optimized prompts may not be perfectly aligned</li>
<li><strong>Inefficient Exploration</strong>: Sequential optimization doesn’t explore the full parameter-prompt space</li>
</ul>
<p>Joint optimization addresses these issues by:</p>
<ul>
<li><strong>Simultaneous Exploration</strong>: Exploring the combined space of parameters and prompts</li>
<li><strong>Coordinated Updates</strong>: Ensuring parameter and prompt updates complement each other</li>
<li><strong>Global Optimum Seeking</strong>: Working toward a true global optimum across both dimensions</li>
</ul>
<h3 id="mathematical-framework"><a class="header" href="#mathematical-framework">Mathematical Framework</a></h3>
<p>Let θ represent model parameters and p represent prompts. The objective is to maximize:</p>
<pre><code>L(θ, p) = Σ_i log P(y_i | x_i; θ, p) + λ1 * R1(θ) + λ2 * R2(p)
</code></pre>
<p>Where:</p>
<ul>
<li>R1(θ) is a regularization term for parameters</li>
<li>R2(p) is a regularization term for prompts</li>
<li>λ1, λ2 are weighting factors</li>
</ul>
<p>The joint optimization problem can be solved using various strategies:</p>
<pre><code class="language-python">class JointOptimizationFramework:
    """
    Framework for joint optimization of model parameters and prompts.
    """

    def __init__(
        self,
        model,
        prompt_templates,
        learning_rates={"params": 1e-5, "prompts": 0.1},
        regularization={"params": 0.01, "prompts": 0.1},
        optimization_strategy="alternating"
    ):
        self.model = model
        self.prompt_templates = prompt_templates
        self.learning_rates = learning_rates
        self.regularization = regularization
        self.optimization_strategy = optimization_strategy

        # Initialize optimizers
        self.param_optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=learning_rates["params"],
            weight_decay=regularization["params"]
        )

        # Prompt optimizer (could be gradient-based or discrete)
        self.prompt_optimizer = self._create_prompt_optimizer()

    def _create_prompt_optimizer(self):
        """Create appropriate optimizer for prompts."""
        if self.optimization_strategy == "gradient_based":
            return torch.optim.Adam(
                self.prompt_templates.parameters(),
                lr=self.learning_rates["prompts"],
                weight_decay=self.regularization["prompts"]
            )
        elif self.optimization_strategy == "discrete":
            return DiscretePromptOptimizer(self.prompt_templates)
        else:
            return EvolutionaryPromptOptimizer(self.prompt_templates)
</code></pre>
<h2 id="joint-optimization-strategies"><a class="header" href="#joint-optimization-strategies">Joint Optimization Strategies</a></h2>
<h3 id="1-alternating-optimization"><a class="header" href="#1-alternating-optimization">1. Alternating Optimization</a></h3>
<p>The most common approach where parameters and prompts are optimized in alternating phases:</p>
<pre><code class="language-python">class AlternatingJointOptimizer(JointOptimizationFramework):
    """
    Alternating optimization between parameters and prompts.
    """

    def optimize(self, train_data, val_data, num_epochs=10):
        """Execute alternating joint optimization."""

        best_metric = 0
        best_state = None

        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch + 1}/{num_epochs}")

            # Phase 1: Parameter optimization (k steps)
            param_metrics = self._optimize_parameters(
                train_data, val_data, steps=5
            )

            # Phase 2: Prompt optimization (1 step)
            prompt_metrics = self._optimize_prompts(
                train_data, val_data, steps=1
            )

            # Evaluate combined performance
            combined_metric = self._evaluate(val_data)

            print(f"Param improvement: {param_metrics:.4f}")
            print(f"Prompt improvement: {prompt_metrics:.4f}")
            print(f"Combined metric: {combined_metric:.4f}")

            # Track best performance
            if combined_metric &gt; best_metric:
                best_metric = combined_metric
                best_state = self._save_state()

        # Restore best state
        self._restore_state(best_state)

        return best_metric

    def _optimize_parameters(self, train_data, val_data, steps=5):
        """Optimize model parameters with fixed prompts."""
        self.model.train()
        self.prompt_templates.eval()

        initial_metric = self._evaluate(val_data)
        total_loss = 0

        for step in range(steps):
            for batch in train_data:
                # Forward pass
                outputs = self.forward_with_fixed_prompts(batch)
                loss = self.compute_loss(outputs, batch)

                # Backward pass
                self.param_optimizer.zero_grad()
                loss.backward()
                self.param_optimizer.step()

                total_loss += loss.item()

        final_metric = self._evaluate(val_data)
        return final_metric - initial_metric

    def _optimize_prompts(self, train_data, val_data, steps=1):
        """Optimize prompts with fixed parameters."""
        self.model.eval()
        self.prompt_templates.train()

        initial_metric = self._evaluate(val_data)

        # Use DSPy's prompt optimizers
        for step in range(steps):
            # Extract current prompt templates
            current_templates = self.prompt_templates.get_templates()

            # Optimize using DSPy optimizer
            optimized_templates = self._dspy_prompt_optimize(
                current_templates, train_data
            )

            # Update prompts
            self.prompt_templates.update_templates(optimized_templates)

        final_metric = self._evaluate(val_data)
        return final_metric - initial_metric
</code></pre>
<h3 id="2-simultaneous-gradient-based-optimization"><a class="header" href="#2-simultaneous-gradient-based-optimization">2. Simultaneous Gradient-Based Optimization</a></h3>
<p>For soft prompts that can be optimized with gradients:</p>
<pre><code class="language-python">class SimultaneousJointOptimizer(JointOptimizationFramework):
    """
    Simultaneous optimization using gradient-based methods.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs, optimization_strategy="gradient_based")

    def optimize(self, train_data, val_data, num_epochs=10):
        """Execute simultaneous gradient-based optimization."""

        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch + 1}/{num_epochs}")

            self.model.train()
            self.prompt_templates.train()

            epoch_loss = 0
            num_batches = 0

            for batch in train_data:
                # Forward pass with both parameter and prompt gradients
                outputs = self.forward(batch)
                loss = self.compute_joint_loss(outputs, batch)

                # Backward pass
                self.param_optimizer.zero_grad()
                self.prompt_optimizer.zero_grad()
                loss.backward()

                # Apply different learning rates
                self.param_optimizer.step()
                self.prompt_optimizer.step()

                epoch_loss += loss.item()
                num_batches += 1

            # Evaluate on validation set
            val_metric = self._evaluate(val_data)
            avg_loss = epoch_loss / num_batches

            print(f"Average loss: {avg_loss:.4f}")
            print(f"Validation metric: {val_metric:.4f}")

    def compute_joint_loss(self, outputs, batch):
        """Compute joint loss considering both parameters and prompts."""
        # Task-specific loss
        task_loss = self.compute_task_loss(outputs, batch)

        # Parameter regularization
        param_reg = self.compute_parameter_regularization()

        # Prompt regularization (encourage diversity, etc.)
        prompt_reg = self.compute_prompt_regularization()

        # Alignment loss (ensure parameters and prompts are aligned)
        alignment_loss = self.compute_alignment_loss(outputs, batch)

        # Combined loss
        total_loss = (
            task_loss +
            self.regularization["params"] * param_reg +
            self.regularization["prompts"] * prompt_reg +
            0.1 * alignment_loss
        )

        return total_loss
</code></pre>
<h3 id="3-multi-objective-optimization"><a class="header" href="#3-multi-objective-optimization">3. Multi-Objective Optimization</a></h3>
<p>Treating parameter and prompt optimization as multiple objectives:</p>
<pre><code class="language-python">class MultiObjectiveJointOptimizer:
    """
    Multi-objective optimization for parameters and prompts.
    """

    def __init__(self, model, prompt_templates):
        self.model = model
        self.prompt_templates = prompt_templates
        self.pareto_front = []

    def optimize(self, train_data, val_data, generations=50):
        """Execute multi-objective optimization."""

        # Initialize population
        population = self._initialize_population()

        for gen in range(generations):
            print(f"\nGeneration {gen + 1}/{generations}")

            # Evaluate all individuals
            evaluated = []
            for individual in population:
                param_score, prompt_score = self._evaluate_individual(
                    individual, train_data, val_data
                )
                evaluated.append({
                    "individual": individual,
                    "param_score": param_score,
                    "prompt_score": prompt_score
                })

            # Update Pareto front
            self._update_pareto_front(evaluated)

            # Create next generation
            population = self._create_next_generation(evaluated)

        return self.pareto_front

    def _evaluate_individual(self, individual, train_data, val_data):
        """Evaluate an individual's performance on both objectives."""
        # Apply individual's parameters and prompts
        self._apply_individual(individual)

        # Parameter optimization score
        param_score = self._evaluate_parameter_performance(val_data)

        # Prompt optimization score
        prompt_score = self._evaluate_prompt_performance(val_data)

        return param_score, prompt_score

    def _update_pareto_front(self, evaluated):
        """Update the Pareto front with non-dominated solutions."""
        for eval_item in evaluated:
            dominated = False

            # Check if this solution is dominated by any in Pareto front
            for pareto_item in self.pareto_front:
                if (pareto_item["param_score"] &gt;= eval_item["param_score"] and
                    pareto_item["prompt_score"] &gt;= eval_item["prompt_score"] and
                    (pareto_item["param_score"] &gt; eval_item["param_score"] or
                     pareto_item["prompt_score"] &gt; eval_item["prompt_score"])):
                    dominated = True
                    break

            # If not dominated, add to Pareto front and remove dominated solutions
            if not dominated:
                self.pareto_front = [
                    item for item in self.pareto_front
                    if not (eval_item["param_score"] &gt;= item["param_score"] and
                           eval_item["prompt_score"] &gt;= item["prompt_score"] and
                           (eval_item["param_score"] &gt; item["param_score"] or
                            eval_item["prompt_score"] &gt; item["prompt_score"]))
                ]
                self.pareto_front.append(eval_item)
</code></pre>
<h2 id="practical-implementation-in-dspy"><a class="header" href="#practical-implementation-in-dspy">Practical Implementation in DSPy</a></h2>
<h3 id="joint-optimization-module"><a class="header" href="#joint-optimization-module">Joint Optimization Module</a></h3>
<pre><code class="language-python">class DSPyJointOptimizer(dspy.Module):
    """
    DSPy module for joint optimization of fine-tuning and prompts.
    """

    def __init__(
        self,
        base_model,
        task_signature,
        optimization_config=None
    ):
        super().__init__()
        self.base_model = base_model
        self.task_signature = task_signature
        self.config = optimization_config or self._default_config()

        # Initialize components
        self.prompt_optimizer = self._create_prompt_optimizer()
        self.fine_tuner = self._create_fine_tuner()
        self.coordinator = OptimizationCoordinator(self.config)

    def _default_config(self):
        """Default configuration for joint optimization."""
        return {
            "alternating_schedule": {
                "param_steps": 5,
                "prompt_steps": 2,
                "warmup_iterations": 3
            },
            "learning_rates": {
                "model": 2e-5,
                "prompts": 0.1
            },
            "regularization": {
                "model": 0.01,
                "prompts": 0.05
            },
            "evaluation": {
                "frequency": 10,
                "early_stopping": True,
                "patience": 5
            }
        }

    def optimize(self, trainset, valset, metric=None):
        """Execute joint optimization."""

        # Initialize optimization state
        state = OptimizationState(
            model=self.base_model,
            prompts=self._initialize_prompts(),
            trainset=trainset,
            valset=valset,
            metric=metric
        )

        # Run optimization
        best_state = self.coordinator.optimize(state)

        return best_state.model, best_state.prompts

    def _initialize_prompts(self):
        """Initialize learnable prompts."""
        if self.config["prompt_type"] == "soft":
            return SoftPromptTemplates(self.task_signature)
        elif self.config["prompt_type"] == "hard":
            return HardPromptTemplates(self.task_signature)
        else:
            return HybridPromptTemplates(self.task_signature)

class OptimizationCoordinator:
    """Coordinates the joint optimization process."""

    def __init__(self, config):
        self.config = config
        self.history = []

    def optimize(self, state):
        """Execute the optimization coordination."""
        best_metric = 0
        best_state = state.copy()
        patience_counter = 0

        for iteration in range(self.config["max_iterations"]):
            print(f"\nIteration {iteration + 1}")

            # Determine optimization phase
            if iteration &lt; self.config["alternating_schedule"]["warmup_iterations"]:
                # Warmup: alternate frequently
                if iteration % 2 == 0:
                    self._parameter_optimization_step(state)
                else:
                    self._prompt_optimization_step(state)
            else:
                # Regular schedule
                for _ in range(self.config["alternating_schedule"]["param_steps"]):
                    self._parameter_optimization_step(state)
                for _ in range(self.config["alternating_schedule"]["prompt_steps"]):
                    self._prompt_optimization_step(state)

            # Evaluate
            if iteration % self.config["evaluation"]["frequency"] == 0:
                metric_value = self._evaluate(state)
                self.history.append({
                    "iteration": iteration,
                    "metric": metric_value
                })

                print(f"Evaluation metric: {metric_value:.4f}")

                # Early stopping
                if self.config["evaluation"]["early_stopping"]:
                    if metric_value &gt; best_metric:
                        best_metric = metric_value
                        best_state = state.copy()
                        patience_counter = 0
                    else:
                        patience_counter += 1
                        if patience_counter &gt;= self.config["evaluation"]["patience"]:
                            print("Early stopping triggered")
                            break

        return best_state

    def _parameter_optimization_step(self, state):
        """Execute one parameter optimization step."""
        # Sample batch from trainset
        batch = state.trainset.sample_batch(
            self.config["batch_size"]
        )

        # Forward pass
        outputs = state.model.forward_with_prompts(
            batch, state.prompts
        )

        # Compute loss
        loss = self._compute_parameter_loss(outputs, batch)

        # Backward pass
        state.param_optimizer.zero_grad()
        loss.backward()
        state.param_optimizer.step()

    def _prompt_optimization_step(self, state):
        """Execute one prompt optimization step."""
        # Use DSPy's prompt optimizers
        current_prompt = state.prompts.get_current_template()

        # Optimize prompt
        optimized_prompt = state.prompt_optimizer.optimize(
            current_prompt,
            state.trainset,
            state.model
        )

        # Update prompts
        state.prompts.update_template(optimized_prompt)
</code></pre>
<h3 id="example-joint-optimization-for-rag"><a class="header" href="#example-joint-optimization-for-rag">Example: Joint Optimization for RAG</a></h3>
<pre><code class="language-python">class JointOptimizedRAG(dspy.Module):
    """
    RAG system with joint optimization of retriever and generator.
    """

    def __init__(self, num_passages=5):
        super().__init__()
        self.num_passages = num_passages

        # Initialize retriever (learnable)
        self.retriever = dspy.Retrieve(k=num_passages)

        # Initialize generator with learnable prompts
        self.generator = dspy.ChainOfThought(
            GenerateAnswerSignature()
        )

        # Learnable components
        self.query_translator = LearnableQueryTranslator()
        self.passage_reranker = LearnableReranker()

    def forward(self, question):
        # Translate and optimize query
        optimized_query = self.query_translator(question)

        # Retrieve passages
        passages = self.retriever(optimized_query).passages

        # Rerank passages
        ranked_passages = self.passage_reranker(passages, question)

        # Generate answer with context
        context = "\n".join(ranked_passages[:self.num_passages])
        answer = self.generator(question=question, context=context)

        return dspy.Prediction(
            answer=answer.answer,
            context=ranked_passages,
            reasoning=answer.rationale
        )

def joint_optimize_rag(trainset, valset):
    """Jointly optimize RAG system."""

    # Initialize RAG system
    rag = JointOptimizedRAG()

    # Create joint optimizer
    optimizer = DSPyJointOptimizer(
        base_model=rag,
        task_signature=GenerateAnswerSignature(),
        optimization_config={
            "max_iterations": 50,
            "batch_size": 8,
            "prompt_type": "hybrid",
            "alternating_schedule": {
                "param_steps": 3,
                "prompt_steps": 1,
                "warmup_iterations": 5
            }
        }
    )

    # Define evaluation metric
    def rag_metric(example, pred, trace=None):
        # Answer correctness
        answer_score = evaluate_answer_faithfulness(
            pred.answer, example.answer, pred.context
        )

        # Retrieval quality
        retrieval_score = evaluate_retrieval_quality(
            pred.context, example.relevant_passages
        )

        # Faithfulness to context
        faithfulness_score = evaluate_faithfulness(
            pred.answer, pred.context
        )

        return (
            0.4 * answer_score +
            0.3 * retrieval_score +
            0.3 * faithfulness_score
        )

    # Run joint optimization
    optimized_rag, optimized_prompts = optimizer.optimize(
        trainset, valset, metric=rag_metric
    )

    return optimized_rag
</code></pre>
<h2 id="advanced-techniques"><a class="header" href="#advanced-techniques">Advanced Techniques</a></h2>
<h3 id="curriculum-joint-optimization"><a class="header" href="#curriculum-joint-optimization">Curriculum Joint Optimization</a></h3>
<pre><code class="language-python">class CurriculumJointOptimizer:
    """
    Joint optimization with curriculum learning.
    """

    def __init__(self, base_optimizer, curriculum_strategy):
        self.base_optimizer = base_optimizer
        self.curriculum_strategy = curriculum_strategy

    def optimize(self, full_trainset, valset):
        """Optimize with curriculum learning."""

        # Initialize curriculum
        curriculum = self.curriculum_strategy.create_curriculum(full_trainset)

        # Iterate through curriculum stages
        for stage_idx, stage_data in enumerate(curriculum):
            print(f"\n=== Curriculum Stage {stage_idx + 1} ===")
            print(f"Stage examples: {len(stage_data)}")

            # Adjust optimization parameters based on stage
            stage_config = self._get_stage_config(stage_idx)
            self.base_optimizer.update_config(stage_config)

            # Optimize on current stage data
            self.base_optimizer.optimize(stage_data, valset)

        # Final optimization on full dataset
        print("\n=== Final Optimization on Full Dataset ===")
        final_config = self._get_final_config()
        self.base_optimizer.update_config(final_config)
        self.base_optimizer.optimize(full_trainset, valset)

    def _get_stage_config(self, stage_idx):
        """Get configuration for specific curriculum stage."""
        # Gradually increase complexity
        base_lr = 1e-5
        stage_lr = base_lr * (2 ** stage_idx)

        return {
            "learning_rate": stage_lr,
            "optimization_intensity": 0.3 + 0.1 * stage_idx,
            "prompt_complexity": "simple" if stage_idx &lt; 2 else "complex"
        }
</code></pre>
<h3 id="meta-learning-for-joint-optimization"><a class="header" href="#meta-learning-for-joint-optimization">Meta-Learning for Joint Optimization</a></h3>
<pre><code class="language-python">class MetaJointOptimizer:
    """
    Meta-learning approach for joint optimization.
    """

    def __init__(self, base_tasks):
        self.base_tasks = base_tasks
        self.meta_knowledge = {}

    def meta_train(self):
        """Train meta-learner on multiple tasks."""

        for task_name, task_data in self.base_tasks.items():
            print(f"\nMeta-training on task: {task_name}")

            # Run joint optimization
            optimizer = DSPyJointOptimizer(
                base_model=task_data["model"],
                task_signature=task_data["signature"]
            )

            optimized = optimizer.optimize(
                task_data["trainset"],
                task_data["valset"]
            )

            # Extract meta-knowledge
            self._extract_meta_knowledge(task_name, optimized)

        # Consolidate meta-knowledge
        self._consolidate_meta_knowledge()

    def adapt_to_new_task(self, new_task_data):
        """Adapt to new task using meta-knowledge."""

        # Initialize with meta-knowledge
        init_config = self._get_init_config_from_meta(new_task_data)

        # Create optimizer with meta-knowledge
        optimizer = DSPyJointOptimizer(
            base_model=new_task_data["model"],
            task_signature=new_task_data["signature"],
            optimization_config=init_config
        )

        # Fast adaptation
        return optimizer.optimize(
            new_task_data["trainset"],
            new_task_data["valset"],
            num_iterations=10  # Fewer iterations for fast adaptation
        )
</code></pre>
<h2 id="evaluation-and-analysis"><a class="header" href="#evaluation-and-analysis">Evaluation and Analysis</a></h2>
<h3 id="comparative-evaluation"><a class="header" href="#comparative-evaluation">Comparative Evaluation</a></h3>
<pre><code class="language-python">def compare_optimization_strategies(task_data):
    """Compare different optimization strategies."""

    results = {}

    # 1. Sequential optimization
    print("\n=== Sequential Optimization ===")
    sequential_result = run_sequential_optimization(task_data)
    results["sequential"] = sequential_result

    # 2. Joint optimization
    print("\n=== Joint Optimization ===")
    joint_result = run_joint_optimization(task_data)
    results["joint"] = joint_result

    # 3. Multi-objective optimization
    print("\n=== Multi-Objective Optimization ===")
    mo_result = run_multi_objective_optimization(task_data)
    results["multi_objective"] = mo_result

    # Analyze results
    print("\n=== Results Analysis ===")
    for strategy, result in results.items():
        print(f"\n{strategy}:")
        print(f"  Final metric: {result['final_metric']:.4f}")
        print(f"  Training time: {result['training_time']:.2f}s")
        print(f"  Convergence iteration: {result['convergence_iter']}")

        # Compute efficiency
        efficiency = result['final_metric'] / result['training_time']
        print(f"  Efficiency: {efficiency:.6f}")

    return results

def analyze_joint_optimization_effects():
    """Analyze the effects of joint optimization."""

    # Load multiple tasks
    tasks = load_benchmark_tasks()

    effects = {
        "improvement_over_sequential": [],
        "convergence_speed": [],
        "final_performance": [],
        "stability": []
    }

    for task_name, task_data in tasks.items():
        # Run both approaches
        sequential = run_sequential_optimization(task_data)
        joint = run_joint_optimization(task_data)

        # Calculate effects
        improvement = (joint["final_metric"] - sequential["final_metric"]) / sequential["final_metric"]
        convergence_speed = sequential["convergence_iter"] / joint["convergence_iter"]

        effects["improvement_over_sequential"].append(improvement)
        effects["convergence_speed"].append(convergence_speed)
        effects["final_performance"].append(joint["final_metric"])

        # Stability: measure variance across multiple runs
        joint_stability = measure_stability(task_data, "joint")
        effects["stability"].append(joint_stability)

    # Report aggregate statistics
    print("\n=== Joint Optimization Effects ===")
    for metric, values in effects.items():
        print(f"\n{metric}:")
        print(f"  Mean: {np.mean(values):.4f}")
        print(f"  Std: {np.std(values):.4f}")
        print(f"  Min: {np.min(values):.4f}")
        print(f"  Max: {np.max(values):.4f}")

    return effects
</code></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="when-to-use-joint-optimization"><a class="header" href="#when-to-use-joint-optimization">When to Use Joint Optimization</a></h3>
<ol>
<li><strong>Complex Tasks</strong>: Multi-step reasoning or multi-component systems</li>
<li><strong>Limited Compute</strong>: When you need maximum efficiency</li>
<li><strong>Performance Critical</strong>: Applications requiring highest possible accuracy</li>
<li><strong>Domain Adaptation</strong>: Adapting to new domains with limited data</li>
</ol>
<h3 id="configuration-guidelines"><a class="header" href="#configuration-guidelines">Configuration Guidelines</a></h3>
<pre><code class="language-python"># For small models (&lt; 1B parameters)
small_model_config = {
    "optimization_strategy": "alternating",
    "param_steps": 3,
    "prompt_steps": 2,
    "learning_rates": {"model": 5e-5, "prompts": 0.2}
}

# For medium models (1-7B parameters)
medium_model_config = {
    "optimization_strategy": "simultaneous",
    "learning_rates": {"model": 2e-5, "prompts": 0.1}
}

# For large models (&gt; 7B parameters)
large_model_config = {
    "optimization_strategy": "alternating",
    "param_steps": 1,
    "prompt_steps": 5,
    "learning_rates": {"model": 1e-5, "prompts": 0.05}
}
</code></pre>
<h3 id="common-challenges"><a class="header" href="#common-challenges">Common Challenges</a></h3>
<ol>
<li><strong>Gradient Magnitude Mismatch</strong>: Parameters and prompts may have different gradient scales</li>
<li><strong>Optimization Instability</strong>: Joint optimization can be less stable</li>
<li><strong>Memory Constraints</strong>: Storing both parameter and prompt states requires more memory</li>
<li><strong>Evaluation Complexity</strong>: Need to evaluate both dimensions separately and jointly</li>
</ol>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>Joint optimization represents a powerful approach for maximizing performance in language model systems. By optimizing parameters and prompts together, we can achieve synergistic effects that outperform traditional sequential approaches. The flexibility of the framework allows it to adapt to different model sizes, task complexities, and computational constraints.</p>
<h3 id="key-takeaways"><a class="header" href="#key-takeaways">Key Takeaways</a></h3>
<ol>
<li>Joint optimization simultaneously optimizes model parameters and prompts</li>
<li>Multiple strategies exist: alternating, simultaneous, and multi-objective</li>
<li>The approach achieves superior performance on complex tasks</li>
<li>Proper configuration is crucial for stability and efficiency</li>
<li>Meta-learning can accelerate optimization on new tasks</li>
</ol>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<p>In the next section, we’ll explore Monte Carlo optimization methods, which provide stochastic approaches for navigating complex optimization spaces in DSPy.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../05-optimizers/09-copa-method.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../05-optimizers/11-monte-carlo-optimization.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../05-optimizers/09-copa-method.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../05-optimizers/11-monte-carlo-optimization.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>






        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
