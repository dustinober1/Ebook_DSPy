<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Fine-tuning - DSPy: A Practical Guide</title>


        <!-- Custom HTML head -->

        <meta name="description" content="The most comprehensive DSPy guide with complete coverage of 9 research papers, advanced optimization techniques, and production-ready applications">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon-de23e50b.svg">
        <link rel="shortcut icon" href="../favicon-8114d1fc.png">
        <link rel="stylesheet" href="../css/variables-8adf115d.css">
        <link rel="stylesheet" href="../css/general-2459343d.css">
        <link rel="stylesheet" href="../css/chrome-ae938929.css">
        <link rel="stylesheet" href="../css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="../highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="../tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="../ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../assets/print-only-ef201963.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc-4ea68664.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">DSPy: A Practical Guide</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="fine-tuning-small-language-models-in-dspy"><a class="header" href="#fine-tuning-small-language-models-in-dspy">Fine-Tuning Small Language Models in DSPy</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>While prompt optimization and few-shot learning work well with large language models (LLMs), sometimes you need better performance from smaller models. Fine-tuning adapts small language models to your specific task, achieving competitive performance with lower computational costs.</p>
<h2 id="when-to-use-fine-tuning"><a class="header" href="#when-to-use-fine-tuning">When to Use Fine-Tuning</a></h2>
<h3 id="ideal-scenarios"><a class="header" href="#ideal-scenarios">Ideal Scenarios</a></h3>
<ul>
<li><strong>Domain-Specific Tasks</strong>: Medical, legal, or technical domains</li>
<li><strong>High Volume</strong>: Large-scale applications where inference cost matters</li>
<li><strong>Latency Critical</strong>: Real-time applications requiring fast responses</li>
<li><strong>Privacy Concerns</strong>: On-premises deployment without external APIs</li>
<li><strong>Consistent Performance</strong>: Need for stable, reproducible outputs</li>
</ul>
<h3 id="model-size-trade-offs"><a class="header" href="#model-size-trade-offs">Model Size Trade-offs</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model Size</th><th>Parameters</th><th>Use Case</th><th>Pros</th><th>Cons</th></tr>
</thead>
<tbody>
<tr><td>&lt; 1B</td><td>&lt; 1B</td><td>Simple classification, basic QA</td><td>Fast, cheap</td><td>Limited capabilities</td></tr>
<tr><td>1-7B</td><td>1-7B</td><td>Most tasks, good balance</td><td>Capable, efficient</td><td>Still needs optimization</td></tr>
<tr><td>7-13B</td><td>7-13B</td><td>Complex reasoning</td><td>Powerful, smaller</td><td>More resources needed</td></tr>
<tr><td>&gt; 13B</td><td>&gt; 13B</td><td>Specialized tasks</td><td>High quality</td><td>Expensive to fine-tune</td></tr>
</tbody>
</table>
</div>
<h2 id="setting-up-fine-tuning"><a class="header" href="#setting-up-fine-tuning">Setting Up Fine-Tuning</a></h2>
<h3 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h3>
<pre><code class="language-python"># Install required packages
!pip install torch transformers datasets accelerate peft bitsandbytes

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from datasets import Dataset
import dspy
</code></pre>
<h3 id="model-selection"><a class="header" href="#model-selection">Model Selection</a></h3>
<pre><code class="language-python"># Popular small models for fine-tuning
MODELS = {
    "mistral-7b": "mistralai/Mistral-7B-v0.1",
    "llama2-7b": "meta-llama/Llama-2-7b-hf",
    "phi-2": "microsoft/phi-2",
    "qwen-7b": "Qwen/Qwen-7B",
    "gemma-7b": "google/gemma-7b"
}

def load_model(model_name, use_4bit=True):
    """Load a model for fine-tuning."""
    model_id = MODELS[model_name]

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Load model
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto",
        load_in_4bit=use_4bit,  # QLoRA support
        trust_remote_code=True
    )

    return model, tokenizer
</code></pre>
<h2 id="qlora-parameter-efficient-fine-tuning"><a class="header" href="#qlora-parameter-efficient-fine-tuning">QLoRA: Parameter-Efficient Fine-Tuning</a></h2>
<p>QLoRA (Quantized Low-Rank Adaptation) is a memory-efficient fine-tuning method that works with 4-bit quantized models.</p>
<h3 id="qlora-configuration"><a class="header" href="#qlora-configuration">QLoRA Configuration</a></h3>
<pre><code class="language-python">from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

def setup_qlora(model, target_modules=None):
    """Set up QLoRA for parameter-efficient fine-tuning."""
    # Default target modules for common architectures
    if target_modules is None:
        target_modules = [
            "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
            "gate_proj", "up_proj", "down_proj",     # MLP
            "lm_head"                                 # Output
        ]

    # LoRA configuration
    lora_config = LoraConfig(
        r=16,                 # Rank
        lora_alpha=32,        # Alpha
        target_modules=target_modules,
        lora_dropout=0.05,    # Dropout
        bias="none",          # No bias adaptation
        task_type="CAUSAL_LM" # Causal language modeling
    )

    # Prepare model for 4-bit training
    model = prepare_model_for_kbit_training(model)

    # Add LoRA adapters
    peft_model = get_peft_model(model, lora_config)

    # Print trainable parameters
    peft_model.print_trainable_parameters()

    return peft_model
</code></pre>
<h3 id="data-preparation"><a class="header" href="#data-preparation">Data Preparation</a></h3>
<pre><code class="language-python">def prepare_training_data(examples, tokenizer, max_length=512):
    """Prepare DSPy examples for fine-tuning."""
    training_data = []

    for example in examples:
        # Format as chat or instruction-following
        if hasattr(example, 'question') and hasattr(example, 'answer'):
            # QA format
            prompt = f"Question: {example.question}\nAnswer: {example.answer}"
        elif hasattr(example, 'context') and hasattr(example, 'response'):
            # Instruction format
            prompt = f"Context: {example.context}\n\nResponse: {example.response}"
        else:
            # Generic format
            prompt = str(example)

        # Tokenize
        tokenized = tokenizer(
            prompt,
            truncation=True,
            padding="max_length",
            max_length=max_length,
            return_tensors="pt"
        )

        training_data.append({
            "input_ids": tokenized["input_ids"].squeeze(),
            "attention_mask": tokenized["attention_mask"].squeeze(),
            "labels": tokenized["input_ids"].squeeze().clone()  # Labels = input_ids
        })

    return Dataset.from_list(training_data)

# Example: Prepare QA data
qa_examples = [
    dspy.Example(
        question="What is machine learning?",
        answer="Machine learning is a field of AI where computers learn from data."
    ),
    # ... more examples
]

model, tokenizer = load_model("mistral-7b")
training_data = prepare_training_data(qa_examples, tokenizer)
</code></pre>
<h2 id="fine-tuning-process"><a class="header" href="#fine-tuning-process">Fine-Tuning Process</a></h2>
<h3 id="training-configuration"><a class="header" href="#training-configuration">Training Configuration</a></h3>
<pre><code class="language-python">from transformers import Trainer

def fine_tune_model(model, training_data, val_data=None):
    """Fine-tune the model with QLoRA."""
    # Training arguments
    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=4,
        warmup_steps=100,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=10,
        optim="paged_adamw_32bit",  # Memory efficient optimizer
        save_steps=100,
        eval_steps=100,
        evaluation_strategy="steps" if val_data else "no",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to="none"  # Disable wandb/tensorboard
    )

    # Create trainer
    trainer = Trainer(
        model=model,
        train_dataset=training_data,
        eval_dataset=val_data,
        args=training_args,
        data_collator=lambda data: {
            'input_ids': torch.stack([item['input_ids'] for item in data]),
            'attention_mask': torch.stack([item['attention_mask'] for item in data]),
            'labels': torch.stack([item['labels'] for item in data])
        }
    )

    # Start training
    trainer.train()

    return trainer.model
</code></pre>
<h3 id="integration-with-dspy"><a class="header" href="#integration-with-dspy">Integration with DSPy</a></h3>
<pre><code class="language-python">class FineTunedLLM(dspy.LM):
    """Wrapper for fine-tuned models in DSPy."""

    def __init__(self, model, tokenizer, temperature=0.7):
        self.model = model
        self.tokenizer = tokenizer
        self.temperature = temperature
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def generate(self, prompt, **kwargs):
        """Generate text using the fine-tuned model."""
        # Tokenize input
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(self.device)

        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=self.temperature,
                do_sample=self.temperature &gt; 0,
                pad_token_id=self.tokenizer.eos_token_id,
                **kwargs
            )

        # Decode
        generated_text = self.tokenizer.decode(
            outputs[0],
            skip_special_tokens=True
        )

        # Remove input prompt from output
        if prompt in generated_text:
            generated_text = generated_text.replace(prompt, "").strip()

        return generated_text

    def __call__(self, prompt, **kwargs):
        return [self.generate(prompt, **kwargs)]

# Use in DSPy
fine_tuned_model = FineTunedLLM(model, tokenizer)
dspy.settings.configure(lm=fine_tuned_model)
</code></pre>
<h2 id="task-specific-fine-tuning"><a class="header" href="#task-specific-fine-tuning">Task-Specific Fine-Tuning</a></h2>
<h3 id="classification-fine-tuning"><a class="header" href="#classification-fine-tuning">Classification Fine-Tuning</a></h3>
<pre><code class="language-python">def prepare_classification_data(examples, tokenizer, labels):
    """Prepare data for classification tasks."""
    training_data = []

    for example in examples:
        # Format as classification prompt
        prompt = f"""Classify the following text into one of: {', '.join(labels)}

Text: {example.text}

Classification:"""

        # Tokenize
        tokenized = tokenizer(
            prompt + " " + example.label,
            truncation=True,
            max_length=256,
            return_tensors="pt"
        )

        # Create labels
        labels_text = tokenizer.decode(
            tokenized["input_ids"].squeeze(),
            skip_special_tokens=True
        )

        training_data.append({
            "input_ids": tokenized["input_ids"].squeeze(),
            "attention_mask": tokenized["attention_mask"].squeeze(),
            "labels": tokenized["input_ids"].squeeze().clone()
        })

    return Dataset.from_list(training_data)

# Example usage
sentiment_examples = [
    dspy.Example(text="I love this!", label="positive"),
    dspy.Example(text="This is bad.", label="negative"),
    # ... more examples
]

sentiment_labels = ["positive", "negative", "neutral"]
sentiment_data = prepare_classification_data(
    sentiment_examples,
    tokenizer,
    sentiment_labels
)
</code></pre>
<h3 id="rag-fine-tuning"><a class="header" href="#rag-fine-tuning">RAG Fine-Tuning</a></h3>
<pre><code class="language-python">def prepare_rag_data(examples, tokenizer):
    """Prepare data for Retrieval-Augmented Generation."""
    training_data = []

    for example in examples:
        # Format as RAG prompt
        prompt = f"""Context: {example.context}

Question: {example.question}

Answer:"""

        # Tokenize
        tokenized = tokenizer(
            prompt + " " + example.answer,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        )

        training_data.append({
            "input_ids": tokenized["input_ids"].squeeze(),
            "attention_mask": tokenized["attention_mask"].squeeze(),
            "labels": tokenized["input_ids"].squeeze().clone()
        })

    return Dataset.from_list(training_data)

class RAGFineTuner:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def fine_tune_rag(self, examples):
        """Fine-tune model for RAG tasks."""
        # Prepare data
        training_data = prepare_rag_data(examples, tokenizer)

        # Fine-tune with specific settings for RAG
        training_args = TrainingArguments(
            output_dir="./rag_results",
            num_train_epochs=2,
            per_device_train_batch_size=2,
            gradient_accumulation_steps=8,
            learning_rate=1e-4,  # Lower learning rate for RAG
            warmup_ratio=0.1,
            fp16=True,
            logging_steps=10,
            save_steps=50
        )

        trainer = Trainer(
            model=self.model,
            train_dataset=training_data,
            args=training_args
        )

        trainer.train()
        return trainer.model
</code></pre>
<h2 id="evaluation-and-testing"><a class="header" href="#evaluation-and-testing">Evaluation and Testing</a></h2>
<h3 id="fine-tuned-model-evaluation"><a class="header" href="#fine-tuned-model-evaluation">Fine-Tuned Model Evaluation</a></h3>
<pre><code class="language-python">def evaluate_fine_tuned_model(model, tokenizer, test_examples):
    """Evaluate fine-tuned model performance."""
    correct = 0
    total = 0
    predictions = []

    model.eval()
    fine_tuned_lm = FineTunedLLM(model, tokenizer, temperature=0)

    for example in test_examples:
        # Generate prediction
        if hasattr(example, 'question'):
            prompt = f"Question: {example.question}\nAnswer:"
        elif hasattr(example, 'text'):
            prompt = f"Text: {example.text}\nClassification:"
        else:
            prompt = str(example)

        with torch.no_grad():
            prediction = fine_tuned_lm.generate(prompt)
            predictions.append((example, prediction))

        # Evaluate (adjust based on task)
        if hasattr(example, 'answer'):
            # QA evaluation
            if example.answer.lower() in prediction.lower():
                correct += 1
        total += 1

    accuracy = correct / total if total &gt; 0 else 0
    return accuracy, predictions

# Evaluate
accuracy, predictions = evaluate_fine_tuned_model(
    model,
    tokenizer,
    test_examples
)
print(f"Fine-tuned model accuracy: {accuracy:.2%}")
</code></pre>
<h3 id="comparison-with-baseline"><a class="header" href="#comparison-with-baseline">Comparison with Baseline</a></h3>
<pre><code class="language-python">def compare_models(fine_tuned_model, baseline_lm, test_examples):
    """Compare fine-tuned model with baseline."""
    fine_tuned_lm = FineTunedLLM(fine_tuned_model, tokenizer, temperature=0)

    results = {
        "fine_tuned": [],
        "baseline": []
    }

    for example in test_examples:
        prompt = f"Question: {example.question}\nAnswer:"

        # Fine-tuned prediction
        ft_pred = fine_tuned_lm.generate(prompt)
        results["fine_tuned"].append((example, ft_pred))

        # Baseline prediction
        base_pred = baseline_lm.generate(prompt)[0]
        results["baseline"].append((example, base_pred))

    # Calculate metrics
    ft_correct = sum(1 for ex, pred in results["fine_tuned"]
                    if ex.answer.lower() in pred.lower())
    base_correct = sum(1 for ex, pred in results["baseline"]
                      if ex.answer.lower() in pred.lower())

    ft_acc = ft_correct / len(test_examples)
    base_acc = base_correct / len(test_examples)

    print(f"Fine-tuned accuracy: {ft_acc:.2%}")
    print(f"Baseline accuracy: {base_acc:.2%}")
    print(f"Improvement: {ft_acc - base_acc:.2%}")

    return results
</code></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="1-data-quality-over-quantity"><a class="header" href="#1-data-quality-over-quantity">1. Data Quality Over Quantity</a></h3>
<pre><code class="language-python">def filter_high_quality_examples(examples, min_length=10, max_length=500):
    """Filter for high-quality training examples."""
    filtered = []

    for example in examples:
        text = str(example)
        if min_length &lt;= len(text) &lt;= max_length:
            # Additional quality checks
            if not has_repetitions(text) and not has_issues(text):
                filtered.append(example)

    return filtered

def has_repetitions(text):
    """Check for excessive repetitions."""
    words = text.lower().split()
    for i in range(len(words) - 2):
        if words[i] == words[i+1] == words[i+2]:
            return True
    return False
</code></pre>
<h3 id="2-balanced-training-set"><a class="header" href="#2-balanced-training-set">2. Balanced Training Set</a></h3>
<pre><code class="language-python">def create_balanced_dataset(examples, field_name):
    """Create a balanced dataset by field."""
    from collections import defaultdict

    # Group by field
    groups = defaultdict(list)
    for example in examples:
        value = getattr(example, field_name, 'unknown')
        groups[value].append(example)

    # Find minimum group size
    min_size = min(len(group) for group in groups.values())

    # Sample from each group
    balanced = []
    for group in groups.values():
        import random
        balanced.extend(random.sample(group, min(min_size, len(group))))

    return balanced
</code></pre>
<h3 id="3-learning-rate-scheduling"><a class="header" href="#3-learning-rate-scheduling">3. Learning Rate Scheduling</a></h3>
<pre><code class="language-python">from transformers import get_cosine_schedule_with_warmup

def create_lr_scheduler(optimizer, num_training_steps, warmup_ratio=0.1):
    """Create a learning rate scheduler."""
    warmup_steps = int(num_training_steps * warmup_ratio)
    return get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=num_training_steps
    )
</code></pre>
<h3 id="4-gradient-clipping"><a class="header" href="#4-gradient-clipping">4. Gradient Clipping</a></h3>
<pre><code class="language-python">training_args = TrainingArguments(
    # ... other args
    max_grad_norm=1.0,  # Prevent gradient explosion
    # ...
)
</code></pre>
<h2 id="common-pitfalls-and-solutions"><a class="header" href="#common-pitfalls-and-solutions">Common Pitfalls and Solutions</a></h2>
<h3 id="pitfall-1-catastrophic-forgetting"><a class="header" href="#pitfall-1-catastrophic-forgetting">Pitfall 1: Catastrophic Forgetting</a></h3>
<pre><code class="language-python"># Problem: Model forgets original capabilities
# Solution: Include diverse examples
def create_mixed_dataset(domain_examples, general_examples, ratio=0.8):
    """Mix domain-specific with general examples."""
    domain_size = int(len(domain_examples) * ratio)
    mixed = domain_examples[:domain_size]
    mixed.extend(general_examples[:len(mixed) - domain_size])
    return mixed
</code></pre>
<h3 id="pitfall-2-overfitting"><a class="header" href="#pitfall-2-overfitting">Pitfall 2: Overfitting</a></h3>
<pre><code class="language-python"># Problem: Model memorizes training data
# Solution: Early stopping and regularization
training_args = TrainingArguments(
    # ... other args
    evaluation_strategy="steps",
    eval_steps=50,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    weight_decay=0.01,  # L2 regularization
    # ...
)
</code></pre>
<h3 id="pitfall-3-memory-issues"><a class="header" href="#pitfall-3-memory-issues">Pitfall 3: Memory Issues</a></h3>
<pre><code class="language-python"># Problem: GPU memory overflow
# Solution: Gradient accumulation and mixed precision
training_args = TrainingArguments(
    # ... other args
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,  # Effective batch size = 16
    fp16=True,  # Mixed precision
    dataloader_pin_memory=False,
    # ...
)
</code></pre>
<h2 id="combined-optimization-fine-tuning--prompt-optimization"><a class="header" href="#combined-optimization-fine-tuning--prompt-optimization">Combined Optimization: Fine-Tuning + Prompt Optimization</a></h2>
<p>One of the most powerful techniques in DSPy is combining fine-tuning with prompt optimization. Research shows that these approaches are complementary, with combined optimization achieving 2-26x improvements over baseline performance.</p>
<h3 id="why-fine-tuning-and-prompt-optimization-are-complementary"><a class="header" href="#why-fine-tuning-and-prompt-optimization-are-complementary">Why Fine-Tuning and Prompt Optimization Are Complementary</a></h3>
<p>Fine-tuning and prompt optimization target different aspects of model behavior:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th>Fine-Tuning</th><th>Prompt Optimization</th></tr>
</thead>
<tbody>
<tr><td><strong>Target</strong></td><td>Model weights</td><td>Instructions and demonstrations</td></tr>
<tr><td><strong>Effect</strong></td><td>Deep task adaptation</td><td>Surface-level guidance</td></tr>
<tr><td><strong>Persistence</strong></td><td>Permanent (model changes)</td><td>Runtime (prompt changes)</td></tr>
<tr><td><strong>Flexibility</strong></td><td>Fixed after training</td><td>Dynamic per query</td></tr>
</tbody>
</table>
</div>
<p>When combined, fine-tuning creates a stronger foundation that prompt optimization can build upon:</p>
<pre><code class="language-python"># The synergistic effect of combined optimization
# Fine-tuning improvement: +15%
# Prompt optimization improvement: +10%
# Combined improvement: +35% (not just 25%!)

# This synergy occurs because:
# 1. Fine-tuned models follow complex instructions better
# 2. Better instruction following enables more sophisticated prompts
# 3. Optimized prompts unlock capabilities learned during fine-tuning
</code></pre>
<h3 id="optimization-order-effects"><a class="header" href="#optimization-order-effects">Optimization Order Effects</a></h3>
<p><strong>Critical insight</strong>: The order of optimization matters significantly.</p>
<pre><code class="language-python"># RECOMMENDED: Fine-tuning FIRST, then prompt optimization
def optimal_order_optimization(program, trainset, base_model):
    """
    Fine-tune first, then apply prompt optimization.
    This order consistently outperforms the reverse.
    """
    # Step 1: Fine-tune the base model
    finetuned_model = finetune_model(
        base_model,
        trainset,
        epochs=3
    )

    # Step 2: Configure DSPy with fine-tuned model
    dspy.settings.configure(lm=finetuned_model)

    # Step 3: Apply prompt optimization
    optimizer = BootstrapFewShot(
        metric=accuracy_metric,
        max_bootstrapped_demos=8
    )
    compiled_program = optimizer.compile(program, trainset=trainset)

    return compiled_program

# NOT RECOMMENDED: Prompt optimization first
def suboptimal_order(program, trainset, base_model):
    """
    This order yields lower performance.
    Prompt optimizations don't transfer well to fine-tuned models.
    """
    # Prompts optimized for base model
    dspy.settings.configure(lm=base_model)
    optimizer = BootstrapFewShot(metric=accuracy_metric)
    compiled_program = optimizer.compile(program, trainset=trainset)

    # Fine-tuning doesn't preserve prompt-specific behaviors
    finetuned_model = finetune_model(base_model, trainset)

    return compiled_program  # Prompts may no longer be optimal
</code></pre>
<h3 id="performance-improvements-with-combined-optimization"><a class="header" href="#performance-improvements-with-combined-optimization">Performance Improvements with Combined Optimization</a></h3>
<p>Real-world benchmarks demonstrate the power of combined optimization:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Task</th><th>Baseline</th><th>Fine-Tune Only</th><th>Prompt Only</th><th>Combined</th><th>Synergy</th></tr>
</thead>
<tbody>
<tr><td>MultiHopQA</td><td>12%</td><td>28%</td><td>20%</td><td>45%</td><td>+9%</td></tr>
<tr><td>GSM8K</td><td>11%</td><td>32%</td><td>22%</td><td>55%</td><td>+12%</td></tr>
<tr><td>Classification</td><td>65%</td><td>82%</td><td>78%</td><td>91%</td><td>+4%</td></tr>
</tbody>
</table>
</div>
<p>The “synergy” column shows improvement beyond simple addition.</p>
<h3 id="code-example-full-combined-optimization-pipeline"><a class="header" href="#code-example-full-combined-optimization-pipeline">Code Example: Full Combined Optimization Pipeline</a></h3>
<pre><code class="language-python">import dspy
from dspy.teleprompter import BootstrapFewShot, MIPRO

def combined_optimization_pipeline(
    program,
    trainset,
    valset,
    base_model_name,
    metric
):
    """
    Complete pipeline for combined fine-tuning and prompt optimization.
    """
    # Phase 1: Prepare fine-tuning data
    print("Phase 1: Preparing fine-tuning data...")
    ft_data = prepare_training_data(trainset, tokenizer)

    # Phase 2: Fine-tune the model
    print("Phase 2: Fine-tuning model...")
    model, tokenizer = load_model(base_model_name)
    peft_model = setup_qlora(model)
    finetuned = fine_tune_model(peft_model, ft_data)

    # Phase 3: Create DSPy LM wrapper
    print("Phase 3: Creating DSPy language model...")
    finetuned_lm = FineTunedLLM(finetuned, tokenizer)
    dspy.settings.configure(lm=finetuned_lm)

    # Phase 4: Apply prompt optimization
    print("Phase 4: Optimizing prompts...")
    # Use BootstrapFewShot for quick optimization
    optimizer = BootstrapFewShot(
        metric=metric,
        max_bootstrapped_demos=8,
        max_labeled_demos=4
    )

    # Or use MIPRO for maximum performance
    # optimizer = MIPRO(metric=metric, auto="medium")

    compiled_program = optimizer.compile(
        program,
        trainset=trainset,
        valset=valset
    )

    # Phase 5: Evaluate
    print("Phase 5: Evaluating...")
    score = evaluate(compiled_program, valset)
    print(f"Final performance: {score:.2%}")

    return compiled_program, finetuned

# Usage
optimized_program, optimized_model = combined_optimization_pipeline(
    program=MyQASystem(),
    trainset=train_examples,
    valset=val_examples,
    base_model_name="mistralai/Mistral-7B-v0.1",
    metric=exact_match_metric
)
</code></pre>
<h3 id="instruction-complexity-scaling"><a class="header" href="#instruction-complexity-scaling">Instruction Complexity Scaling</a></h3>
<p>Fine-tuned models can follow significantly more complex instructions than base models:</p>
<pre><code class="language-python"># Base model: Limited instruction complexity
simple_instruction = "Answer the question."

# Fine-tuned model: Handles complex multi-step instructions
complex_instruction = """
Analyze the question following this process:
1. Identify the core question and any sub-questions
2. Determine what knowledge domains are relevant
3. Consider potential ambiguities or edge cases
4. Synthesize information from multiple sources
5. Provide a clear, well-structured answer
6. Note any assumptions or limitations
"""

def test_instruction_complexity(model, instructions, test_set):
    """Test model's ability to follow complex instructions."""
    results = {}
    for name, instruction in instructions.items():
        # Configure signature with instruction
        signature = dspy.Signature(
            "question -&gt; answer",
            instruction
        )
        predictor = dspy.Predict(signature)

        scores = []
        for example in test_set:
            try:
                pred = predictor(question=example.question)
                scores.append(accuracy_metric(example, pred))
            except:
                scores.append(0)

        results[name] = np.mean(scores)

    return results

# Fine-tuned models show larger gains with complex instructions
</code></pre>
<h3 id="demonstration-efficiency-fewer-shots-required"><a class="header" href="#demonstration-efficiency-fewer-shots-required">Demonstration Efficiency: Fewer Shots Required</a></h3>
<p>Fine-tuned models achieve equivalent performance with fewer demonstrations:</p>
<pre><code class="language-python">def compare_demonstration_efficiency(base_lm, finetuned_lm, trainset, testset):
    """
    Compare how many demonstrations each model needs.
    Fine-tuned models typically need 3 demos where base needs 8.
    """
    results = {"base": {}, "finetuned": {}}

    for num_demos in [1, 2, 3, 4, 5, 6, 7, 8]:
        # Test base model
        dspy.settings.configure(lm=base_lm)
        optimizer = BootstrapFewShot(
            metric=accuracy_metric,
            max_bootstrapped_demos=num_demos
        )
        compiled_base = optimizer.compile(program, trainset=trainset)
        results["base"][num_demos] = evaluate(compiled_base, testset)

        # Test fine-tuned model
        dspy.settings.configure(lm=finetuned_lm)
        compiled_ft = optimizer.compile(program, trainset=trainset)
        results["finetuned"][num_demos] = evaluate(compiled_ft, testset)

    # Find efficiency ratio
    base_8shot = results["base"][8]
    for num_demos in [1, 2, 3, 4, 5]:
        if results["finetuned"][num_demos] &gt;= base_8shot:
            print(f"Fine-tuned {num_demos}-shot &gt;= Base 8-shot")
            print(f"Demonstration efficiency: {8/num_demos:.1f}x")
            break

    return results
</code></pre>
<h3 id="integration-with-copa"><a class="header" href="#integration-with-copa">Integration with COPA</a></h3>
<p>For maximum performance, use the COPA optimizer which systematically combines fine-tuning and prompt optimization:</p>
<pre><code class="language-python">from copa_optimizer import COPAOptimizer  # See 09-copa-optimizer.md

# COPA handles the optimization order automatically
copa = COPAOptimizer(
    base_model_name="mistralai/Mistral-7B-v0.1",
    metric=accuracy_metric,
    finetune_epochs=3,
    prompt_optimizer="mipro"
)

optimized_program, optimized_model = copa.optimize(
    program=MyQASystem(),
    trainset=train_examples,
    valset=val_examples
)

# COPA achieves 2-26x improvements on complex tasks
</code></pre>
<p>For more details on COPA and advanced joint optimization techniques, see <a href="09-copa-optimizer.html">COPA: Combined Fine-Tuning and Prompt Optimization</a>.</p>
<h2 id="key-takeaways"><a class="header" href="#key-takeaways">Key Takeaways</a></h2>
<ol>
<li>Fine-tuning adapts small models for specific tasks efficiently</li>
<li>QLoRA enables memory-efficient fine-tuning with 4-bit models</li>
<li>Proper data preparation is crucial for success</li>
<li>Balance domain-specific and general examples</li>
<li>Monitor for overfitting and catastrophic forgetting</li>
<li>Use gradient accumulation for larger effective batch sizes</li>
<li><strong>Combined optimization (fine-tuning + prompts) achieves synergistic improvements</strong></li>
<li><strong>Always fine-tune first, then apply prompt optimization</strong></li>
<li><strong>Fine-tuned models require fewer demonstrations (3-shot vs 8-shot)</strong></li>
</ol>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<p>In the next section, we’ll explore how to choose the right optimizer for your specific needs and compare different approaches. For advanced joint optimization, see <a href="09-copa-optimizer.html">COPA: Combined Fine-Tuning and Prompt Optimization</a>.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../05-optimizers/04-knnfewshot.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                            </a>

                            <a rel="next prefetch" href="../05-optimizers/07-constraint-driven-optimization.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../05-optimizers/04-knnfewshot.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M41.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"/></svg></span>
                    </a>

                    <a rel="next prefetch" href="../05-optimizers/07-constraint-driven-optimization.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M278.6 233.4c12.5 12.5 12.5 32.8 0 45.3l-160 160c-12.5 12.5-32.8 12.5-45.3 0s-12.5-32.8 0-45.3L210.7 256 73.4 118.6c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0l160 160z"/></svg></span>
                    </a>
            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>






        <script src="../clipboard-1626706a.min.js"></script>
        <script src="../highlight-abc7f01d.js"></script>
        <script src="../book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
